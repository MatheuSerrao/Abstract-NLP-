{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "with open('../tokens', 'rb') as pickle_file:\n",
    "    tokenized_sent = pickle.load(pickle_file)\n",
    "\n",
    "random.shuffle(tokenized_sent)\n",
    "\n",
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "test_tokens = tokenized_sent[:10000]\n",
    "train_tokens = tokenized_sent[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_train_data = [TaggedDocument(token[2], [i]) for i, token in enumerate(train_tokens)]\n",
    "tagged_test_data = [i[2] for i in test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvector_size = Dimensionality of the feature vectors.\\nwindow = The maximum distance between the current and predicted word within a sentence.\\nmin_count = Ignores all words with total frequency lower than this.\\nalpha = The initial learning rate.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train doc2vec model\n",
    "model = Doc2Vec(tagged_train_data, vector_size = 50, min_count = 0, epochs = 10)\n",
    "\n",
    "'''\n",
    "vector_size = Dimensionality of the feature vectors.\n",
    "window = The maximum distance between the current and predicted word within a sentence.\n",
    "min_count = Ignores all words with total frequency lower than this.\n",
    "alpha = The initial learning rate.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(26903, 0.6516794562339783),\n",
       " (22822, 0.6434744596481323),\n",
       " (47227, 0.6004378795623779),\n",
       " (10948, 0.5948018431663513),\n",
       " (41307, 0.5944525003433228),\n",
       " (30907, 0.5944363474845886),\n",
       " (36334, 0.5713683366775513),\n",
       " (34430, 0.566096305847168),\n",
       " (31470, 0.5648831725120544),\n",
       " (39729, 0.5602819919586182)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc = tagged_test_data[0]\n",
    "test_doc_vector = model.infer_vector(test_doc)\n",
    "model.dv.most_similar(positive = [test_doc_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (6056): «this research study is to analyse the role of big data for enhancing the decision making for is one of the it organisation in oman which provides key to it solutions includes application infrastructure in oman big data has become a forthcoming part of all trades and business segment oman in electronic portal is for the citizens which make easy use of a transactions enhancement of big data in company supports in providing perfect public services and also searching of big term in process the range which is used in this topic is exemplified in a crucial panel conversation a recent big data conference we present a cooperative big data analytics stage for big data as a service it takes longer time to achieve wrinkle data progress events and investigative services the outdate technologies do not become an appropriate solution to process a big data platform has begun to appear the quality of big data is of great significance is more significant because the quality of material is affected by size speed and format in which the data is generated the main benefit of the is by implementing the it makes easy communication between the government and the citizens the quality of big data is great pertinent and it is more significant quality of information is affected by the size speed and the data in which the format is generated»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,s0.001,t3):\n",
      "\n",
      "MOST (28325, 0.6744415163993835): «currently undergoing the fourth industrial revolution the modernization of public security governance requires the support of public security big data which is also an important strategy for the modernization of police services this article describes the functional value of public security big data in the field of public security governance preliminary analysis of the mechanism of public security big data application and public security governance discusses the problems of public security big data in practice and proposes the optimization of public security big data applications path at the same time the risks in the application of public security big data should be prevented and controlled actively respond to the new legal ethical and social impact brought by public security big data technology and continue to promote the deep integration of modern police mechanism reform and public security big data application and meet the security needs of people life»\n",
      "\n",
      "MEDIAN (29276, 0.24039749801158905): «the vehicular networks vanet has attracted the attention of both the industry and the academia researcher over the last decade the concept of connecting vehicles to the internet using the already deployed cellular network architecture has opened many avenues for research and development that are contributing significantly towards the intelligent transportation systems its almost every vehicle requires a seamless connectivity to the internet without interruption however with the emergence of the 5g network and the v2i concept the design of efficient mobility management techniques that can handle the mobility constraints in vanet becomes a critical task in this paper we propose a new handover algorithm that uses a markov chain predictor to determine when and where a handover will be needed the aim of the proposed solution is to reduce the number of unnecessary handover by maintaining the vehicle connectivity to the 5g base station as long as possible without degrading the network performance simulation studies were conducted to evaluate the performance of the proposed scheme our results show that the proposed handover algorithm greatly outperforms the conventional 3gpp handover algorithms»\n",
      "\n",
      "LEAST (40741, -0.4151585102081299): «»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(tagged_test_data) - 1)\n",
    "inferred_vector = model.infer_vector(tagged_test_data[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(tagged_test_data[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(tagged_train_data[sims[index][0]].words)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
