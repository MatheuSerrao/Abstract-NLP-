id,title,abstract
9613914,Human-Inspired Camera: A Novel Camera System for Computer Vision,"Computer vision models aim to emulate biological design so that systems can perform meaningful tasks. We believe that the underlying processes of the human visual system hold the keys to further improving the performance of such computer vision solutions. This exploratory paper investigates the swaying motion of human vision when walking to develop a novel camera system. We successfully demonstrate that this design is able to improve performance in computer vision tasks, such as monocular depth estimation."
9499877,Development of a Computer Vision Interactive Game for Supporting Dental Care Education in Taiwan,"Dental care is important to pupils in elementary school since it affects their physical and mental development significantly. Therefore, dental care education is an important issue for governments around the world. Based on this, this study proposed a computer vision interactive game for supporting dental care education in Taiwan. Different from traditional lecture-based instruction, this study developed an interactive educational game that applied artificial intelligence technology to develop a computer vision service for recognizing pupils' dental photos and thus engaged pupils in playing an interactive game to prevent caries. To evaluate the proposed approach, an experiment was conducted in an elementary school and twenty-one fifth grade students was asked to participant in the experiment. The results indicated that the proposed approach can improve students' learning motivation, learning attitude, and learning achievement."
9270498,Research on Image Processing Technology of Computer Vision Algorithm,"With the gradual improvement of artificial intelligence technology, image processing has become a common technology and is widely used in various fields to provide people with high-quality services. Starting from computer vision algorithms and image processing technologies, the computer vision display system is designed, and image distortion correction algorithms are explored for reference."
9400438,Adaptable Architecture for the Development of Computer Vision Systems in FPGA,"Computer vision systems are increasingly used in industry for inspection or process control. The more demanding requirements observed today make the implementation of this type of systems a technological challenge. Many of the computational architectures available allow us to meet the main functional requirements related to the use case and also the non-functional ones, such as processing time restrictions and connectivity. However, the requirement for adaptability so that such systems can be easily modified to meet different use cases or even accommodate environmental changes remains a challenge. This work proposes a flexible architecture for computer vision systems using FPGA. This architecture combines components of the processing flow of the vision system implemented in hardware and software to obtain advantages associated with both approaches. The proposed solution is validated against different real use cases existing in the industry. The results obtained allow us to affirm that such architecture brings an interesting advantages since it meets the operational requirements present in industrial applications, demands less development effort and can be easily adapted to new usage scenarios."
9121137,Controlling Mouse Motions Using Eye Tracking Using Computer Vision,"This paper introduces an algorithm to carry out the functions of a mouse by providing a hands-free interaction between humans and computers. It provi des an alternative to the traditional mouse computer. By using different expressions of a face using computer vision and matching it with already stored expression and execute actions as per the move. This algorithm will help physically disabled people to perform the functions of a mouse using their face and eye movement. It allows them to left-click, right-click, scroll up and down, move the cursor up, down, left, right. The system has a very basic need like webcam, NumPy, dlib, and a few other basic libraries."
8932817,Computer Vision Based Distance Measurement System using Stereo Camera View,"In recent years, especially in industrial automation systems, in order for robots to understand their distances and positions according to the target, close to human vision, computer vision systems are needed. Computer vision close to human vision can only be created using stereo cameras. In this study, a computer vision system is developed using the stereo camera system for measuring object distances. In the study, first of all, for the distance measurement, the distance of the face images obtained from the stereo camera system to the screen is calculated. In measuring the distances of the face images to screen, the disparity maps are first extracted and the face region is detected. Afterwards, the distance measurements are performed on the obtained images in the stereo camera system on account of calculating the shifts between the frames. In the experimental studies, the actual distance values such as 71, 74, 75, 79, 110, 125, and 115 of the face to the screen are measured as 70, 72, 73, 77, 97, 120, 132 cm by proposed system, respectively. When the experimental results are examined, we can say that the proposed computer vision system is successful in distance measurement using stereo camera view."
9116906,From Brain Decoding To Brain-Driven Computer Vision,"Brain driven computer vision includes methods and tools for computer vision inspired by the human brain. Brain driven includes computer vision are brain visual representation decoding, understanding and learning. In this paper, we focus on the brain decoding and review on research in this field. In addition, we review and introduce its main approaches and its application in computer vision."
9270494,The Application of Computer Vision in Responding to the Emergencies of Autonomous Driving,"Nowadays, artificial intelligence is developing rapidly, and greatly promotes the appearance of relevant industries. Automated driving is one of those hot industries. With the development of autonomous vehicles, higher requirements such as intervention when an accident or system failure occurs mean more problems. Many shortcomings arose during emergencies. For example, the system cannot make a correct judgment in a short time and calculate the most suitable solution. This paper analyzes several algorithms for emergency situations at the current stage of autonomous vehicles from the perspective of computer vision. In addition, this paper finds suitable algorithms by collecting, collating and researching literature. Although these algorithms can solve the problems in most cases, there are still defects. For example, there are problems such as incomplete consideration and insufficient optimization. This paper also proposes possible solutions to the defects of each algorithm: The algorithm of edge recognition algorithm under extreme weather is added. Besides, the HMM model is used to optimize the pedestrian motion recognition algorithm. In addition, the standard of driver behavior recognition is also optimized. It is hoped that autonomous vehicles will have a more comprehensive emergency handling system in the future such as faster emergency response and more accurate decision."
9182641,A perspective transformation method based on computer vision,"Blind spots are ubiquitous in everyday life, such as A-pillar blind spot caused by the structure of a car, A visual blind area on the opposite side of a turning road and so on. The problem of visual blindness has always brought many security risks to people's daily life, especially some scenes that need to make an instant response. The existence of visual blindness will greatly affect our judgment and even the safety of life. We use to decorate the obstacles on the other side of the fisheye camera, image cutting by providing a series of OpenCV function and simulate the shape of the obstacles that block area and effect, and by installing in the human side of camera to capture the human eye coordinates information, using the affine transformation, made in the image on the screen close to the human eye can see the picture “perspective” obstacles, to achieve the effect of perspective. The system has a simple structure and can be installed in a fixed place to solve the problem of visual blindness."
9022143,Why Does Data-Driven Beat Theory-Driven Computer Vision?,"This paper proposes that despite the success of deep learning methods in computer vision, the dominance we see would not have been possible by the methods of deep learning alone: the tacit change has been the evolution of empirical practice in computer vision. We demonstrate this by examining the distribution of sensor settings in vision datasets, only one potential dataset bias, and performance of both classic and deep learning algorithms under various camera settings. This reveals a strong mismatch between optimal performance ranges of theory-driven algorithms and sensor setting distributions in common vision datasets."
9324850,Computer Vision Based Framework For Detecting Phishing Webpages,"One of the most dangerous threats on the internet nowadays is phishing attacks. This type of attack can lead to data breaches, and with it to image and financial loss in a company. The most common technique to exploit this type of attack is by sending emails to the target users to trick them to send their credentials to the attacker servers. If the user clicks on the link from the email, then good detection is needed to protect the user credentials. Many papers presented Computer Vision as a good detection technique, but we will explain why this solution can generate lots of false positives in some important environments. This paper focuses on challenges of the Computer Vision detection technique and proposes a combination of multiple techniques together with Computer Vision technique in order to solve the challenges we have shown. We also will present a methodology to detect phishing attacks that will work with the proposed combination techniques."
9574411,Recognizing Human Emotion Using Computer Vision,"Detecting and recognizing human emotion is not an easy thing to do. This will be a big challenge in computer vision because many factors have to be taken while developing an emotion recognition system like facial hair, the presence of spectacles and diverse environments. This paper will use CNN (Convolution Neural Network) model to recognize human emotion from the livefeed using the FER2013 (Facial Expressions Recognition-2013)dataset to train the model. This model delivers 54% accuracy from recognizing seven basic human emotions such as neutral, angry, disgust, fear, happy, sad, and surprise."
8645995,Underwater Computer Vision of the ZEABUS AUV,this paper presents the implementation of underwater computer vision of the ZEABUS Autonomous Underwater Vehicle (AUV) at Kasetsart University. The purpose of the implementation is to augment previously used algorithm for the 2018 International RoboSub Competition and Singapore AUV 2018 Challenge. The result shows that new algorithm has higher precision level and can achieve better object detection.
9190851,Camera Placement Meeting Restrictions of Computer Vision,"In the blooming era of smart edge devices, surveillance cameras have been deployed in many locations. Surveillance cameras are most useful when they are spaced out to maximize coverage of an area. However, deciding where to place cameras is an NP-hard problem and researchers have proposed heuristic solutions. Existing work does not consider a significant restriction of computer vision: in order to track a moving object, the object must occupy enough pixels. The number of pixels depends on many factors (How far away is the object? What is the camera resolution? What is the focal length?). In this study, we propose a camera placement method that identifies effective camera placement in arbitrary spaces and can account for different camera types as well. Our strategy represents spaces as polygons, then uses a greedy algorithm to partition the polygons and determine the cameras' locations to provide the desired coverage. Our solution also makes it possible to perform object tracking via overlapping camera placement. Our method is evaluated against complex shapes and real-world museum floor plans, achieving up to 85% coverage and 25% overlap."
9301078,End-to-End Computer Vision Framework,"Computer Vision is a cross research field with the main purpose of understanding the surrounding environment as closely as possible to human perception. In addition to research on techniques and algorithms that enhance the capabilities of a Computer Vision system, research on the architecture and design of such systems is also important. In this paper we present an End-to-End Computer Vision Framework open-source solution, based on Python programming language, that aims to support researchers in this field. The main focus of the framework is the configurability and scalability of the system in the continuous need to add new Computer Vision algorithms or machine learning models for a day-to-day research activity."
9148583,Application Research of Computer Vision Technology in Automation,"With the continuous development of society and economy and the continuous advancement of science and technology, the application of computer information technology in various fields has become more and more extensive. People have begun to use computer technology to innovate artificial intelligence systems that can replace humans. Among them, the application of computer vision technology is indispensable. This kind of technology can ensure the quality of products and can improve the efficiency of enterprise production to a certain extent. Therefore, this paper mainly studies the computer vision technology and its application in automation, in order to provide a certain reference."
8844942,An Automated Computer Vision Based System for Bottle Cap Fitting Inspection,"Inspection of the cap is one of the most crucial phases of packaging bottles. Defects like loosely fitted cap, scratches/broken cap may occur. It is important to detect these errors as soon as possible. This paper proposes an automated system by which the bottle cap defects can be identified. The bottles with these defects will be rejected by the system. The methods used in this paper are based on computer vision. The system comprises of four methods, which are utilized for bottle cap defect detection: Pattern recognition, Clustering, Object Detection and Line Detection. This paper also presents a comprehensive analysis and a comparison of all the four methods on various parameters. The system has an extensive social and practical value with increasing the productivity, improve the quality of inspection and profitability."
9626573,Vision Processing for Assistive Vision: A Deep Reinforcement Learning Approach,"There is increasing interest in using computer vision and machine learning to enhance human decision making with computer-mediated assistive vision systems. In particular, retinal implants are a rapidly advancing technology offering individuals suffering vision loss due to retinal dystrophies, an opportunity to restore partial vision. However, the visual representations achievable with current and near-term implants are severely limited in resolution and contrast, placing high importance on the selection of visual features to convey via the implant. Using vision processing algorithms on camera-captured input, functional outcomes can be enhanced with such devices. To this end, we propose a novel end-to-end vision processing pipeline for prosthetic vision that learns task-salient visual filters in simulation offline via deep reinforcement learning (DRL). Once learnt, these filters are deployable on a prosthetic vision device to process camera-captured images and produce task-guiding scene representations in real-time. We show how a set of learnt visual features enabling a virtual agent to optimally perform the task of navigation in a 3-D environment can be extracted and applied to enhance the same features in real world images. We evaluate and validate our proposed approach quantitatively and qualitatively using simulated prosthetic vision. To our knowledge, this is the first application of DRL to the derivation of scene representations for human-centric computer-mediated displays such as prosthetic vision devices."
9447637,Comparison of Different Computer Vision Approaches for E-waste Components Detection to Automate E-waste Disassembly,"Electronic Waste (E-waste) is generated in a tremendous amount due to our increasing dependence on electronic devices and rapid upgrading in technological innovations. Environmental and health risks are posed because of e-waste toxic constituents. On the other hand, e-waste contains valuable recoverable materials that can gain economic benefits. Efficient e-waste recycling is optimally conducted when different components are separated and processed by their appropriate chemical techniques. If component separation were conducted by humans, it could put their health at risk and consume an unnecessary amount of time. Thus, Automation and robotics solutions are needed to carry out the recycling. At the heart of such solutions is a computer vision algorithm that can detect, localize and classify e-waste components. Different Computer vision approaches, both traditional and deep learning-based, were compared to know which approach will be more suitable for such a task."
9567757,A Concise Review of Deep Learning Deployment in 3D Computer Vision Systems,"Deep Learning (DL) has advanced the concept of digital image processing, but this does not rule out the relevance of the conventional computer vision techniques as they have continuously been rebranded even before the emergence of DL. DL techniques have over the years been the method of choice in several fields due to their superior performance compared to the state-of-the-art machine learning methods. Among the numerous DL methods, computer vision has to be one of the most prominent methods, and given this, this article overviewed the concept of DL in computer vision, focusing more on the recent advancements in tasks that involve 3D visual data. Some of the DL schemes that are relevant in solving computer vision problems were also briefly reviewed. Furthermore, the history, structure, merits, and drawbacks of these DL schemes were provided. The application of these DL schemes in various computer vision tasks, like object detection, was also described in this article."
9243353,Feasibility Study of Beef Quality Assessment using Computer Vision and Deep Neural Network (DNN) Algorithm,"The beef quality relies upon the colour score of muscle during the grading stage. Colour scoring to be used in beef grading would be very critical and the current way of identification and determination of the quality of beef is still being done manually and susceptible to human error. The ability to automate the prediction of the beef quality will assist the meat industry through the grading phase to establish the colour score. Therefore, computer vision and deep neural network (DNN) were used to predict the beef quality by determining colour scores of beef muscle. Four hundred of beef rib-eye steaks were chosen and acquired for each image, which is the colour score of beef were assigned by expertise according to the standard colour cards. The image was processed and went through DNN classifier for determining beef quality. The proposed DNN classifier achieved the best performance percentage of 90.0%, showing that the computer vision integrated with the DNN algorithm can deliver an efficient implementation for predicting beef quality using colour scores of beef muscle."
9604557,Convolutional Neural Network Architectures Used in Computer Vision,"Studies on computer vision have gained momentum especially with the development of convolutional neural networks (CNN). In addition, the creation of better data sets in terms of quality and quantity, providing access to these data sets, developments in computational power and parallel programming are other issues that affect the achievement of high levels of accuracy. In this study, the concept of computer vision, the factors that make computer vision difficult, the studies that have been done in computer vision until today are summarized. The algorithms developed for this purpose are examined and the studies done in the last five years are analyzed and presented comparatively. With this study, it is aimed to provide a general perspective for future studies."
9225499,A Computer Vision Technique To Detect Scab on Malabar Nightshade,"Agribusiness and its efficiency have a decent effect on the financial development of each nation. In Agriculture, leaf ailments have become an issue because it may a create basic decline in both quality and measure of agrarian output. In this manner, computerized acknowledgement of ailments on leaves assumes a significant job in the farming area. This paper gives a basic and computationally capable strategy utilized for leaf sickness recognizable proof and reviewing utilizing Image processing and computer vision. In this study, a computerized approach is created to distinguish deformities of Malabar nightshade and perceive infections by utilizing machine vision-based picture preparing method which is actualized in MATLAB including an AI calculation with the Multiclass SVM classifier."
9184619,A-MnasNet: Augmented MnasNet for Computer Vision,"Convolutional Neural Networks (CNNs) play an essential role in Deep Learning. They are extensively used in Computer Vision. They are complicated but very effective in extracting features from an image or a video stream. After AlexNet [5] won the ILSVRC [8] in 2012, there was a drastic increase in research related with CNNs. Many state-of-the-art architectures like VGG Net [12], GoogleNet [13], ResNet [18], Inception-v4 [14], Inception-Resnet-v2 [14], ShuffleNet [23], Xception [24], MobileNet [6], MobileNetV2 [7], SqueezeNet [16], SqueezeNext [17] and many more were introduced. The trend behind the research depicts an increase in the number of layers of CNN to make them more efficient but with that the size of the model increased as well. This problem was fixed with the advent of new algorithms which resulted in a decrease in model size. As a result, today we have CNN models which are implemented on mobile devices. These mobile models are small and fast which in turn reduce the computational cost of the embedded system. This paper resembles similar idea, it proposes a new model Augmented MnasNet (A-MnasNet) which has been derived from MnasNet [1]. The model is trained with CIFAR-10 [4] dataset and has a validation accuracy of 96.89% and a model size of 11.6 MB. It outperforms its baseline architecture MnasNet which has a validation accuracy of 80.8% and a model size of 12.7 MB when trained with CIFAR-10."
9418483,Application of VR Technology in Museum Narrative Design with Computer Vision Models,"The establishment of a virtual museum is an epoch-making thing. Its conceptual development has gone through four basic core stages: exhibition, reconstruction, replacement and reproduction, hence, pplication of modern VR technology in museum narrative design with computer vision models together with the scenario test is provided in this study. Virtual reality technology is a high-tech computer science technology that combines multiple technologies, including computer three-dimensional graphics technology, computer vision and data analysis. This designed freamework applied the technology and the vision minotirng to the systematci design. The hardware implementations are also validated through the platform constructions. The test, comparison analysis and the efficiency testing are provided."
9515102,Computer vision application to determine crowdedness in public transport stations,"Computer vision has advanced greatly in the last years, providing a useful tool to extract information from images and video streams, enhancing the capabilities of classical video detection algorithms. In the transport field, it may be used in many applications, like information systems (for people and vehicle count, identification of lost objects, etc.), but is mature enough to be trusted also safety systems (monitoring rails for foreign objects, illegal crossings, and many more). In this article, the authors present a solution to detect crowdedness in public transport stations, that can be helpful for local authorities/public transport operators to make proper decisions regarding anti-Covid measures, but also enhance the response of the public transport system."
9429674,A New Method for Measuring Leveling Heights Using Computer Vision Techniques,"The levels and leveling techniques have developed over the last decades. The development of level instruments started with the appearance of precise levels, they are precise instruments used for precise leveling. The operation of digital levels is based on the digital processing of video indications of a coded staff. A new method for measuring leveling heights using computer vision techniques is proposed in this paper. The methodology implementation comprises capturing an image of a metric staff using a digital camera, then the height is computed by processing the image using a developed algorithm. The algorithm consists of two major steps. In the first step, the implemented software computes the meter and tenth of meter height values through recognizing numbers in the image. Then, it computes the rest of the decimal part digits from the coordinate difference between the image's middle row (reading line) and E shape's (or its mirror image) end row. The proposed algorithm works well for the tested precisions (1, 0.1, and 0.01milimeters), also it can measure the heights from a distance up to more than 100m. The testing results indicate that the measurement algorithm is workable and can be used as a low-cost alternative to the barcoded staff digital levels."
9148179,Design of Automatic Scoring Device for Soccer Goal Based on Computer Vision,"Football is one of the most fashionable games in the world, meanwhile the refereeing results may be controversial. In this work, an automatic goal scoring system is designed for the specific games based on computer vision. The aim is to simplify the referee's work, reduce the human error, and provide an accurate performance for the operators. This subject can realize the tracking and identification of football as the movement of the entity, the detection of the ball into the goal of the scoring area. Through the actual test, the design can accurately display the score results."
8832897,Design and Realization of Fire Detection Using Computer Vision Technology,"Recently, vision-based target positioning technology has received considerable attention in the field of computer vision. The positioning technology has many advantages such as faster speed, higher accuracy and more stable and reliable positioning results. Therefore, it plays an important role in the fields of robot technology, military reconnaissance, geographic survey and field measurement. Based on the positioning technology, the paper designs an embedded vision system with DM816x microprocessor (the processing module) and CCD camera equipped with an infrared filter (the acquisition module), which realizes the recognition and positioning of the fire. Moreover, with 200 groups of experiments in a warehouses, the fire detection system has an alarm accuracy rate over 98%, and positioning accuracy of fire is far higher than the national standard of China. In addition, an automatic fire protection system is proposed, which consists of the fire detector and the water cannon, then it can automatically extinguish the fire when a fire is detected."
9243264,Parking Management by Means of Computer Vision,"Car park management currently relies on parking attendants to show where empty parking lots are by monitoring all available parking areas. In some shopping centers sometimes do not have information about parking conditions, whether parking is full or empty. This condition causes the driver to have difficulty finding a parking space. When drivers are looking for a parking space, many cars are milling about causing congestion and wasting time. This paper presents a breakthrough to make it easier for drivers to find an empty parking space with builds a car parking monitoring and management system based on computer vision. The system input is a camera that is installed in the center of the parking area. The HAAR Cascade Classifier method detects and counts the cars that are parked and then compared with the available parking slots using. If the number of cars currently parked is less than the slots provided, the parking area will inform you that it is still empty along with the number of available slots. Detection of available parking spaces is based on coordinates that have been determined manually on the camera. A total of five slot areas are determined by this camera. The result of this research shows the system accuracy is 90% for the available car parking."
9458522,The 2020 Low-Power Computer Vision Challenge,"AI computer vision has advanced significantly in recent years. IoT and edge computing devices such as mobile phones have become the primary computing platform for many end users. Mobile devices such as robots and drones that rely on batteries demand for energy efficient computation. Since 2015, the IEEE Annual International Low-Power Computer Vision Challenge (LPCVC) was held to identify energy-efficient AI and computer vision solutions. The 2020 LPCVC includes three challenge tracks: (1) PyTorch UAV Video Track, (2) FPGA Image Track, and (3) On-device Visual Intelligence Competition (OVIC) Tenforflow Track. This paper summarizes the 2020 winning solutions from the three tracks of LPCVC competitions. Methods and future directions for energy-efficient AI and computer vision research are discussed."
9377120,Computer Vision and Iot Based Smart System for Visually Impaired People,"For people who are visually impaired, navigation is a challenge they encounter on a daily basis. For the same, use of walking sticks have become a common practice. Although, there are a lot of limitation of just relying on a blind stick. A more suitable method will alert the user about the nature of the obstacle and should also be of assistance in guiding the user to their location. In this paper, we propose an architecture of an assistance system revolving around a shoe that employees IoT devices and sensors along with computer vision algorithms to provide functionalities like obstacle detection, avoidance and navigation. The method uses a smartphone based voice assistance and guides the user with appropriate haptic feedback calculated using various sensors and actuators."
9396468,Computer Vision in Android Application Development,"This article is devoted to the application development for Android OS using computer vision technology. It demonstrates the implementation process step by step and also describes working with such development tools like: Java, ML Kit, Android Studio. The result of the study is application that the user interacts with by turning their head and blinking their eyes."
9164071,Manipulator Package Sorting and Placing System Based on Computer Vision,"Nowadays the logistics industry is developing rapidly. This paper proposes and designs a vision-based manipulator express sorting and delivery system for the problem of express stacking. This system combines computer vision, deep learning, manipulator control and motion planning technologies. This paper uses the YOLO algorithm and PCL to determine the position and normal vector of the pick-up point of the package, plans and controls the movement of the manipulator to the target position to suck the package, and uses vision to determine the size of the package and obtain customer information. Compared with RCNN and SDD algorithm, YOLO has a faster detection speed [1], and at the same time, it handles small stacked targets such as express boxes with higher accuracy. Therefore, this system uses the YOLOv3 algorithm to improve the system's extraction accuracy and speed. Finally, the success rate of the system was 75%, and the completion time was about 15s."
9368818,Research on Computer Vision Image Multimedia Technology Based on Big Data,"With the development of the times, the progress of society, and the continuous improvement of science and technology, people's daily production and life have changed greatly compared with before. Internet, Internet of things, deep algorithm and other technologies are widely used in people's life, which makes big data technology trigger a new round of development trend. In this era of big data, many industries and technology research also ushered in great development opportunities. In order to better study the multimedia technology of computer vision image in the new era; this paper analyzes the application of big data technology, so as to better and more efficient research on computer vision image multimedia technology. The rapid development of the current era has brought great challenges to computer vision image multimedia technology. Therefore, this paper makes an in-depth study of computer vision image multimedia technology under the background of big data. In the research, this paper systematically describes the current computer vision image multimedia technology, as well as the future development direction of computer vision image multimedia technology. Through the analysis, the big data analysis method proposed in this paper plays a key role in the research of computer vision image multimedia technology."
8846702,Bio-Inspired Stereo Vision Calibration for Dynamic Vision Sensors,"Many advances have been made in the field of computer vision. Several recent research trends have focused on mimicking human vision by using a stereo vision system. In multi-camera systems, a calibration process is usually implemented to improve the results accuracy. However, these systems generate a large amount of data to be processed; therefore, a powerful computer is required and, in many cases, this cannot be done in real time. Neuromorphic Engineering attempts to create bio-inspired systems that mimic the information processing that takes place in the human brain. This information is encoded using pulses (or spikes) and the generated systems are much simpler (in computational operations and resources), which allows them to perform similar tasks with much lower power consumption, thus these processes can be developed over specialized hardware with real-time processing. In this work, a bio-inspired stereo-vision system is presented, where a calibration mechanism for this system is implemented and evaluated using several tests. The result is a novel calibration technique for a neuromorphic stereo vision system, implemented over specialized hardware (FPGA - Field-Programmable Gate Array), which allows obtaining reduced latencies on hardware implementation for stand-alone systems, and working in real time."
9544643,A Deep Learning Survey on Content Detection from Images Powered by Computer Vision Framework,"In online services, images have a great impact on a customer's move from knowing the product, accessing and then buying it. It is not a one-to-one process of buying but there is involvement of third-party sellers where the content has to be monitored. Though there are cautions and guidelines, irrelevant images pop up, the customer may get a wrong notion. This paper presents an ideology of a computer vision driven image detection system. This paper also deals about deep learning to real-world product images. The proposed research work has shown a number of technical goals such as lack of training data, class imbalance, fine-grained class definitions. The system combines image classification and object detection techniques with budgeted crowdsourcing to develop a solution for huge, varied and continuous evolving product catalogue. The final section deals with implementation."
9020554,Guidance for Specific Target Doors in Hallway using the Computer Vision for Autonomous Vehicles,"Computer Vision would complement the autonomous robotics goals of accuracy and precision by adding “eyes” to the performance of the robot. We will be researching on computer vision techniques and experimenting their benefits with autonomous robotics. We investigated both Arduino MegaTM Controller Board and Raspberry Pi for analyzing the flexibility of chipset for computer vision using the 4-wire dual data bus Pixi Camera (CMU's 5th version). Both C/C++ and Python libraries were implemented to test the vision camera using I2C and USB communication. Pixi camera scans at a rate of 50 frames per second. We investigated the object recognition, object classification and object tracking capabilities using various algorithms for detecting the specific target doors in a hallway. The autonomous robot is going to maneuver in the hall way by avoiding the obstacles and stop precisely at a door-front of the assigned door number. This is possible by associating the door numbers with color codes."
9214281,Pattern Design of Knit and Purl based on CAD System of Automatic Knitting Machine based on Computer Vision Integration,"Pattern design of knit and purl based on CAD system of automatic knitting machine based on computer vision integration is studied in this paper. Through the computer knitting machine CAD and knitting experiment, summed up the system is a different combination of needle and needle anti the law and its application in pattern design, so as to avoid the actual knitting machine. And the design of the original deviation, flower pattern deformation etc. in some unnecessary extent, and for design of knitted apparel provide more experience to reference. Besides this, the machine vision technologies are combined to then achieve the automatic analysis. The experiment results have then validated the effectiveness."
9396099,Automatic Generation of Preview Images Based on Video Sequence Analysis Using Computer Vision,"The article is devoted to the problems of video analysis, selecting keyframes from it, searching for the most attractive thumbnails in it, as well as analyzing existing approaches to solving these problems. The article offers a new method for selecting attractive frames based on selecting frames with the largest number of attractive objects. This results in a frame that is visually appealing and relevant to the video. The solution also considers the selection of the most attractive thumbnails based on the analysis of various visual qualities and aesthetic indicators of video frames, and then performs cluster analysis to ensure that the resulting images match the video content. The research also considers the method of selecting the frames with the greatest activity through the emotional analysis of people in the video. If necessary, a style is applied to the final image to get a more interesting result."
9025507,Towards Computer Vision Powered Color-Nutrient Assessment of Puréed Food,"With one in four individuals afflicted with malnutrition, computer vision may provide a way of introducing a new level of automation in the nutrition field to reliably monitor food and nutrient intake. We present results on the link between color and vitamin A content using transmittance imaging of a pureed foods dilution series in a computer-vision powered intelligent nutrient sensing system prototype and use a fine-tuned deep autoencoder network to predict the relative concentration of sweet potato purees. Results indicate an network accuracy of 80% across beginner (6 month) and intermediate (8 month) commercially prepared pureed sweet potato samples. Network errors may be explained by fundamental differences in optical properties which are further discussed."
8821635,Smart Vision: Assistive Device for the Visually Impaired Community U sing Online Computer Vision Service,"The visually impaired community face several difficulties in their daily life. With the increase of commercial assistive devices, it can greatly improve their lives, especially for navigation and orientation, which are their main obstacles. Recently, it has been introduced many powerful online image processing services, based on machine learning and deep learning. Microsoft is one of the main players in online image processing services. In this paper, we discussed a prototype development by utilizing the online image processing service, Microsoft Cognitive Service. We expect to further evaluate the accuracy and the usage analytics in order to understand long term system behavior."
9395789,Design of Digital Museum Narrative Space Based on Perceptual Experience Data Mining and Computer Vision,"Design of digital museum narrative space based on perceptual experience data mining and the computer vision is presented in this paper. The carrier of the traditional physical museum is the physical exhibition space inside the magnificent building. In contrast, the carrier of the network virtual museum is much more convenient and flexible. A computer server with powerful configuration can become its carrier. The proposed design utilizes the computer vision and data sensing framework to finish the efficient model. In the general terms of the time performance, segmentation based algorithm is generally faster than fuzzy clustering based algorithm. The data mining model is integrated here to estimate the performance. The simulation has provided the numerical understanding of the proposed method."
9593994,FLoW-Vision: Object Recognition and Pose Estimation System based on Three-Dimensional (3D) Computer Vision,"This paper presents a three-dimensional computer vision-based object recognition on FLoW-Vision in RoISC (formerly ER2C) has entered its second phase. Previously, the robot had a basic vision that was used to replicate ‘human-like’ visual skills using 2D computer vision. As a result of the above discussion, we proposed the design and implementation of an object recognition and pose estimation system based on three-dimensional computer vision to handle object recognition and pose estimation tasks in real-world environments simultaneously. In the object recognition process, a point-cloud segmentation method is used to obtain possible object clusters before starting the calculation of feature description. Then, a keypoints-based two-stage matching process is performed to speed up the computation of finding correspondences between the object clusters of the current scene and a colored point cloud model of an object. Next, a Hough voting algorithm is employed to filter out matching errors in the correspondence set and estimate the initial 3D pose of the object. Last process process the pose estimation from clustered object using RANSAC to search the largest surface as Z surface. Experimental validate the object recognition can work correctly with percentage 100% and pose estimation accuracy of the proposed system can work correctly with percentage 60% in a complex real-world scene."
9216434,R-MnasNet: Reduced MnasNet for Computer Vision,"In Deep Learning, Convolutional Neural Networks (CNNs) are widely used for Computer Vision applications. With the advent of new technology, there is an inevitable necessity for CNNs to be computationally less expensive. It has become a key factor in determining its competence. CNN models must be compact in size and work efficiently when deployed on embedded systems. In order to achieve this goal, researchers have invented new algorithms which make CNNs lightweight yet accurate enough to be used for applications like object detection. In this paper, we have tried to do the same by modifying an architecture to make it compact with a fair trade-off between model size and accuracy. A new architecture, R-MnasNet (Reduced MnasNet), has been introduced which has a model size of 3 MB. It is trained on CIFAR-10 [4] and has a validation accuracy of 91.13%. Whereas the baseline architecture, MnasNet [1], has a model size of 12.7 MB with a validation accuracy of 80.8% when trained with CIFAR-10 dataset. R-MnasNet can be used on resource-constrained devices. It can be deployed on embedded systems for vision applications."
9522852,AFRIFASHION1600: A Contemporary African Fashion Dataset for Computer Vision,"This work presents AFRIFASHION1600, an openly accessible contemporary African fashion image dataset containing 1600 samples labelled into 8 classes representing some African fashion styles. Each sample is coloured and has an image size of 128 x 128. This is a niche dataset that aims to improve visibility, inclusion, and familiarity of African fashion in computer vision tasks.AFRIFASHION1600 dataset is available here."
8693826,"Low-Power Computer Vision: Status, Challenges, and Opportunities","Computer vision has achieved impressive progress in recent years. Meanwhile, mobile phones have become the primary computing platforms for millions of people. In addition to mobile phones, many autonomous systems rely on visual data for making decisions, and some of these systems have limited energy (such as unmanned aerial vehicles also called drones and mobile robots). These systems rely on batteries, and energy efficiency is critical. This paper serves the following two main purposes. First, examine the state of the art for low-power solutions to detect objects in images. Since 2015, the IEEE Annual International Low-Power Image Recognition Challenge (LPIRC) has been held to identify the most energy-efficient computer vision solutions. This paper summarizes the 2018 winners’ solutions. Second, suggest directions for research as well as opportunities for low-power computer vision."
8715022,Vehicular Obstruction Detection In The Zebra Lane Using Computer Vision,"Computer Vision and Image Analysis are used in researches with an objective of extracting information from a set of scenarios. Multiple researches with varying objectives like vehicle speed detection, traffic density estimation, vehicle counting, or in general, observation of behaviors of multiple objects, have been applying Computer Vision. This research paper is about utilizing Computer Vision for obstruction detection by observing temporal state of vehicles situated in a pedestrian crossing lane. The researchers gathered data by taking videos of real traffic in a road containing a pedestrian crossing lane (PCL). The method starts with a pre-processing phase wherein the image was de-noised, converted to grayscale and derived the Image Binarization, and establishment of the PCL for region of interest (ROI). Connected components are extracted then assigned its own structure with corresponding properties called `track'. Tracks are monitored by using Kalman Filter and Hungarian Algorithm. Then, Ray-Casting algorithm is applied to determine if an object violates the traffic rule. For violators, a snapshot will be taken and determine the license plate. Based on the result, True Positive Rate of 65.28% and True Negative Rate of 98.26% were obtained."
9436701,Thai Tangerine Size Classification via Computer Vision,"In the citrus industry in Thailand, fruit sizing is generally performed by machines, which entails mechanical damages to the fruits. Computer vision technology can serve as a non-destructive alternative. Our focus is on the size classification of Thai tangerines, regarded as major economic fruits, by means of this technology. Specifically, we apply five classification methods, namely area thresholding, diameter thresholding, K-nearest neighbors, support vector machine, and artificial neural network, to a dataset of Thai tangerine images created by our computer vision system. We compare these methods in terms of their classification accuracy and visualize the classification performance using cobweb representation. Our results provide insights into the potential of computer vision technology for fruit size classification."
9318682,Computer Vision for Hand Gestures,"The model developed here is used to detect specific items from the environment. The desired objects to be detected from the environment is hands(gestures). So, for computer to look at the environment computer vision is the most necessary aspect. Based, on the gesture present in the image captured along with the unnecessary objects, the image is processed and the important message is kept and rest is discarded. After processing the neural networks are introduced for to elevate the standards of computer vision there by allowing computer to know about the gesture provided by the humans. Here the recognition of the image is done by using Convolution neural network (CNN) algorithm, the gesture is predicted and this predicted result is shown on the screen connected or an audio device connected."
9225331,Leveraging Computer Vision for Emergency Vehicle Detection-Implementation and Analysis,"Recent advances in Computer Vision technology have revolutionized the field of Intelligent Transportation Systems. The applications are far reaching- right from traffic monitoring systems to self-driving cars. Most applications entail at least simple, if not advanced image or video analytics at a fundamental level. This paper is an attempt to examine the use of object detection and instance segmentation for emergency vehicle detection, which is indispensable to any Intelligent Transportation System. More particularly, emergency vehicle detection can be programmed into autonomous vehicles as well as traffic signal controllers for preferential signal switching upon encountering emergency vehicles. The architectures implemented are Faster RCNN for object detection and Mask RCNN for instance segmentation. The computational results of these implementations, their accuracies and most importantly, their suitability for emergency vehicle detection in disordered traffic conditions are deliberated. Additionally, the object detection model is contrasted with instance segmentation and the merits and demerits of each are identified, again in the context of emergency vehicle detection."
9259826,Computer Vision Based Intelligent 3D CNC Machines,"Innovative, digital, embedded processing units and systems and easy-to-use design tools help deliver the digitalization of systems to enable smarter, safer, more efficient machines and factories. All these new technologies and innovations help us to design what’s new and what’s next in industrial processing, monitoring, sensing, command and control, communication, functional safety and power management. This study presents a collection of practical applications of computer vision and a plan to prepare utility software, which is based on latest technology of computer vision, which makes easy to prepare a CAD drawing from an image for CNC machines. This method could be useful in the field of reverse engineering for reproducing existing products which has not a vector drawing or the restoration of a historical item."
9093363,Kornia: an Open Source Differentiable Computer Vision Library for PyTorch,"This work presents Kornia - an open source computer vision library which consists of a set of differentiable routines and modules to solve generic computer vision problems. The package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be inserted inside neural networks to train models to perform image transformations, camera calibration, epipolar geometry, and low level image processing techniques, such as filtering and edge detection that operate directly on high dimensional tensor representations. Examples of classical vision problems implemented using our framework are provided including a benchmark comparing to existing vision libraries."
9616715,Identifying Fake Rice Using Computer Vision in Perspective of Bangladesh,"Bangladesh is an agricultural country. Rice is one of the major agricultural products of this country. Rice is the staple food of most of the people in Bangladesh. As a result, there is a considerable demand for rice. So, there are many types of rice supply in the market. But lately it is seen that fake rice is sold in the market along with real rice. These rices cannot be easily identified with the naked eye whether it is real or fake. Now, it is often seen that people are being deceived by buying fake rice instead of real rice. Which is deadly harmful to health. For this real and fake rice was collected and made a large dataset by capturing their images. Using this dataset, two different computer vision models have been created and then their accuracy have been checked if those models can identify whether a particular rice is real or fake. Among them CNN algorithm gives the highest accuracy which is 98%."
8936033,A pH Regulation System for Perfusion Machine Using Computer Vision and Fuzzy Logic,"As current liver perfusion machines use pH probes for monitoring the acidity level in their machine, it poses an increased risk of contamination due to the probes direct contact with the blood, which would then greatly increase the risk of transplant failure. In order to detect pH levels of blood in a liver perfusion machine with minimal contact, this study will use computer vision as a tool in detecting and the given ph. In order to determine the pH level of blood, multiple computer vision techniques with built in knowledge-based systems will be used in the study. The knowledge-based system will continue to improve overtime as more data is collected. In this study, perfusion liquid will be used as the medium for blood due to ethical concerns. The pH is regulated using fuzzy logic control from the data taken from the pH detection system. Through the data taken from the results, it shows that this system when compared to the traditional pH monitoring system is accurate and viable in determining the pH level of the blood. And the system is viable in regulating the pH level of the system effectively."
9033150,A Unified Framework of Intelligent Vehicle Damage Assessment based on Computer Vision Technology,"Due to the development of deep learning, in recent years, the field of computer vision grows rapidly. A large amount of computer vision technologies have been applied in actual problems. At present, the industry of vehicle damage assessment requires a lot of manpower, and new automatic intelligent damage assessment technology can greatly reduce industrial costs. In this paper, a framework of intelligent vehicle damage assessment algorithm based on object detection technology and image classification technology is proposed. This algorithm can automatically identify the damage position, type and degree according to photos provided by users, so as to offer appropriate maintenance price and reach the accuracy that can meet actual application requirements."
9378016,A differentiation between Image Mining and Computer Vision in the application area of Big Data,"The technological progress in image acquisition, image processing, and improvements in the field of big data in the last decades opens a wide range of possible applications for image processing using analytical methods. The most popular concepts in this field of tension are computer vision and image mining. However, the issue is that there is no clear distinction between these two concepts in the current scientific discussion. To address this need, a structured literature review was carried out. Thereby, the key characteristics as well as the process of image mining could be identified and compared to computer vision. Thus, this research in progress study was a first step towards differentiating these two concepts. In conclusion, a hypothesis could be derived, which will be considered now in our ongoing research."
9617445,Virtual Metrology of Visualizing Copper Microstructure Featured with Computer Vision and Artificial Neural Network,"Virtual metrology of visualizing copper microstructure featured with computer vision and artificial neural network is reported. The focused ion beam images of copper microstructures are modeled. The input is the process variables of fabricating thin copper film and output is the microstructure image corresponding to the process variables. Computer vision and artificial neural network are used to generate model parameters. Images are generated using the model. Nucleation, grains, grain boundaries and twins are all visualized. The features of nucleation and grain growth are quantitatively analyzed in the images. The results of the predicted images support that the inference of microstructure images is proven. The technique is adaptive to most imaging systems and one of the exploratory applications of virtual metrology."
9418638,Computer Vision Based Estimation of Shrimp Population Density and Size,"Shrimp is one of the most important aquaculture products in Vietnam and it is desired to continuously estimate shrimp population density and size. This paper presents our initial steps in supervising aqua farming based on computer vision techniques. The computer vision system includes U-net segmentation, marker-controlled watershed segmentation, thresholding, contour tracing, etc. Experiment results show that the proposed method can obtain good results in counting shrimps (mean absolute error of 0.093) and estimating shrimp lengths (RMS error of 0.293 cm) when they are separately located."
9482091,Intelligence information analysis based on computer vision,"Camera imaging principle is applied in this article, based on the current research of computer vision measurement, research the monocular visual range, geometric measurement of single image, binocular vision range in a single image, video, and the geometric measurement of problem such as aerial drones, by combining the actual problem, to deal with pictures and video, the integrated use of a single image object target measurement geometry, monocular visual range and binocular vision range finding method, establish the corresponding mathematical model, finally realizes the problem solving. The model can also be applied to solve similar problems, and has the advantages of simple algorithm and relatively high precision."
8900475,GEOSEG: A Computer Vision Package for Automatic Building Segmentation and Outline Extraction,"Recently, deep learning algorithms, especially fully convolutional network based methods, have become very popular in the field of remote sensing. However, these methods are implemented and evaluated through various datasets and deep learning frameworks. There has not been a package that covers these methods in a unified manner. In this study, we introduce a computer vision package termed Geoseg that focuses on building segmentation and outline extraction. Geoseg implements nine state-of-the-art models as well as utility scripts needed to conduct model training, logging, evaluation, and visualization. The implementation of Geoseg emphasizes unification, simplicity, and flexibility. The performance and computational efficiency of all implemented methods are evaluated by a comparison experiment using a unified, high-quality aerial image dataset."
9050168,Driver Gaze Region Estimation Based on Computer Vision,"Traditional methods of driver's gaze estimation usually need additional equipment to obtain the driver's facial and eye features, which is difficult to apply in real life. In this paper, a gaze region estimation method based on computer vision is proposed, which reduces the hardware requirements of the experiment. In order to achieve this goal, a new eye feature extraction method and an improved random forest algorithm are adopted in this paper. Finally, the driver's line of sight is determined by combining the head and eye features. The driver's head features are obtained by pose from orthography and scaling with iterations (POSIT) algorithm. Then, the accurate positions of the driver's corner of the eye and the pupils are obtained by the from coarse to fine method, and the eye line direction is estimated according to the relative position of the pupil in the eye area. Finally, the improved random forest is used to estimate the driver's gaze region. The experimental results show that the accuracy of this method is 94.12% in the real driver's gaze data set."
9071459,Computer Vision Based Energy Monitoring System using Meter Image Capturing System (MICAPS),"In the era of smart grid, extraction of data from meters is the primary step. Conventionally data extraction from non-communicable meters are done manually. In order to provide complete smart solutions to the customers the utilities are planning for a mass roll out smart meters which is costly. Approximately |3000 to |4000 is required for replacing a noncommunicable meter to communicable meter and also requires to hamper the existing infrastructure. In this context this paper proposes a meter image capturing system (MICAPS). MICAPS is a image based technique for extraction of data. MICAPS is formed by integrating raspberry pi with raspberry pi camera. Python code is used to configure the camera and take the images and send it to cloud. This image is a seven segment display. In order to convert a seven segment display to text a computer vision optical recognition algorithm is used. The overall cost of the MICAPS is |4000 with which it is possible to monitor minimum 2 meters. One more advantage with MICAPS system is that remotely the operator will be able to change the granularity of the data based on the required. Thus this MICAPS will help in monitoring the energy in a community based distribution networks."
9050330,Gray Sequence Image Shadow Removal Algorithm Based on Computer Vision,"Computer vision is a method that includes acquiring, processing, and analyzing digital images. This paper improves the mixed Gaussian model and proposes a shadow detection algorithm that fuses texture features and color features. In order to remove the interference of moving shadows, a shadow removal algorithm combining the gradient of the gray ratio and the normalized cross-correlation feature is proposed. The gray-scale sequence image is used to calculate the ratio of the foreground and its corresponding background gray, and the shadow is judged and removed based on the gradient average of the calculated gray ratio. This paper proposes an improved hybrid Gaussian model and uses different learning rates in different time periods during the background update process, which increases the rate of background model convergence and also improves the detection rate of interesting targets. Color and texture feature shadow detection algorithms have been applied in the process of projected shadow detection based on color features."
8806469,Design of Cricket Automatic Control System Based on Computer Vision Technology,"This paper designs a cricket control system based on computer vision technology. This system is controlled by PID algorithm. The acquisition system is OpenMV camera, and the processing system is STM32 series MCU. The design is used to control the small ball to roll on the evenly distributed smooth square plate, so that the ball can be removed on the square plate without any external force except for the self-gravity, the support force of the plate and the friction force. Various specified requirements are fulfilled between circular areas of 3 cm in outer diameter. In this design, the image data is collected by the camera on the smooth flat panel range, and the collected data is transmitted to the STM32 core board through the serial port for processing, and the center coordinates of the hard ball are found through the processed data, and the coordinates are adjusted according to the set destination coordinates and the actual coordinates. PID (proportional-integral-derivative) controls the steering gear to rotate a certain angle to control the hard ball to achieve the relevant operational requirements."
9376886,Smart Implementation of Computer Vision and Machine Learningfor Pothole Detection,"As the world is modernizing in each and every aspect on a day to day basis and getting prosperous, so the people are. By this growth in one's individual life in the society is getting on better living standards, under which personal vehicle is emerging as a basic need for everyone to go about in daily life, so that number of vehicles is increasing but road conditions are of mostly the same state or say even the worse keeping all this scenario in the mind we have designed a model to detect and predict the potholes and different anomalies present in the road using different suitable algorithms of machine learning and deep learning. For the execution of the project first, we collected the different road image samples and applied various computer vision operations such as preprocessing steps (different blur, smoothing), morphological operations, canny edge detection and decision tree to detect the pothole. After we have the road images with potholes detected, we made a dataset out of it extracting feature and giving it to the deep neural network (DNN) to process further, and it predicts the potholes using linear regression at the end of the process."
9043003,Design and Implementation of an Intelligent Nail Machine with Computer Vision Techniques,Nail painting machine is getting prevailing attention in consumer devices. A smart technique is proposed in this paper to automatically define nail printing area. This study develops a computer vision method to mark the area of nails and merge the user selected patterns. The segmented and merged result is then sent to the nail painting machine for nail printing.
8900416,Geosr: A Computer Vision Package for Deep Learning Based Single-Frame Remote Sensing Imagery Super-Resolution,"Recently, owing to the outstanding capability of deep learning in solving ill-posed problems, the single-frame super-resolution (SR) researches tend to focus on deep learning methods largely. However, related researches are implemented and evaluated through various datasets and different deep learning frameworks, which hinders the comparison of performance among different methods and heavily hampers the progress of SR techniques. In this study, we present GeoSR, an open source computer vision package for deep learning based single-frame remote sensing imagery super-resolution to facilitate the development of the SR community. As a unified, simple, and flexible package, GeoSR contains pipeline-like integrated tools from data retrieval to final result evaluation, which enables users to develop self-defined models conveniently; several state-of-the-art models trained through the same high-quality dataset are provided as the baseline in the package as well. Moreover, the proposed package could potentially serve as a viable backend for other related packages such as image segmentation with high efficiency."
8833248,Passenger Flow Monitoring of Elevator Video Based on Computer Vision,"Passenger flow monitoring plays an important role in elevator intelligent monitoring and elevator operation status analysis. This study gives one of definitions for elevator passenger flow, and proposes a counting method of the passenger flow in elevator based on the fusion of computer vision techniques. This experiment calculates the passenger flow in real-time detection by dividing the task into three parts, which is detecting the door status using the background subtraction method, detecting the current floor with Support Vector Machine (SVM) algorithm and counting passengers by YOLOv3 (You Only Look Once) model. The results show that the proposed method can accurately measure passenger flow even when passengers are heavily obscured."
9454615,Detection of a Surface Defect on an Engine Block Using Computer Vision,"Today the manufacturing world is facing major pressures due to the globalization of markets. Internal and external organizational pressures have led to increased competition, market complexity, and new customer demands. It has been noted that organizations and companies adopt lean or agile manufacturing strategies to overcome this problem. One of those strategies, the use of artificial intelligence and machine vision in the automotive industry, appeared as a good opportunity created by the continuous development of the computer-integrated manufacturing systems and the digitalization of the industry. In the automotive manufacturing process, developing a new automated defect detection system (ADDS), adapted for the automotive manufacturing requirements and particularities, and improved with artificial intelligence techniques, it is of major importance. In this paper we focus on a specific problem from the automotive industry, the problem of testing engine blocks using the information from real images."
8899418,Vision-Face Recognition Attendance Monitoring System for Surveillance using Deep Learning Technology and Computer Vision,"Nowadays, Artificial Neural networks can be trained over several billion images and can be used to detect and recognize faces with relative ease and flexibility in an instant. This concept is used in the implementation of this real time attendance cum surveillance system that can be prototyped and set into action. Some of the major applications of this innovative method include face attendance using a single snap mode in smartphones for university classes, further real-time facial recognition surveillance of lab facilities and work places which can set this as a first line of defense against intruders from gaining access. The user-friendly graphical user interface provides flexibility and ease in running these powerful face recognition algorithms powered by deep-learning. We have achieved a maximum recognition accuracy of 74 percent while running the real time surveillance algorithm. This work was done as a solution to the absence of a robust and user friendly face recognition attendance system."
9142984,VISION- Wearable Speech Based Feedback System for the Visually Impaired using Computer Vision,"With the advent of new technologies, a whole lot realm of possibilities is open to mankind which otherwise would have been either impossible or a miracle. Computer vision is one such field which has millions of possibilities and this project itself being a primary example of it. This project aims to help the blind society to experience the world independently with the help of a speech-based feedback device. This project proposes (1) identifying walkable spaces, (2) text recognition and text-to-speech, (3) identify and locate specific types of objects; and walking navigation which can be incorporated into this project as a future scope. This is implemented with the YOLO algorithm for object detection, which uses the COCO dataset. Our project will help a blind person to walk easily by finding the path, detect obstacles in front of them and thus avoid it. It will help them read texts as well, which is done using OCR which uses python and an API for the text recognition. Thereafter, gTTS is used to convert text to speech, which is the final output for the users."
9095114,Optimal Analysis of Target Dynamic Tracking Strategy Based on Computer Vision,"Computer vision is a highly intensive interdisciplinary subject which combines artificial intelligence and advanced algorithms, and its target tracking has become a hot research topic at present. At the operational level of target tracking, PF(Particle Filter) technology has its own technical advantages in processing image interpretation interference and target movement change. On the one hand, BPNN(Back Propagation Neural Network) technology is introduced to improve the diversity of particles and improve the accuracy of target information at the next moment so as to meet the dual requirements of dynamic intelligent capture and timeliness at the next moment of target tracking; on the other hand, PSO(Particle Swarm Optimization) technology is introduced to prevent particle degradation and quickly find the global optimal solution of particle information. The simulation results show that the method of using BPNN + PSO to improve the particle filter algorithm, not only improves the accuracy of target tracking information, but also reduces the timeconsuming of target tracking."
9221475,Identification of LED Lights with Different Colors Based on Computer Vision,"In this study, a computer vision-based identification system is established for the identification of LED lights of different colors for industrial robots. By using a CCD camera, images of products on the industrial assembly line are collected. To reduce the effect of environmental brightness, the obtained images based on RGB model are converted into HSI color format. According to the obtained hue and saturation values, this system could accurately and effectively identify the color of the LED lights, which provides accurate identification and judgment for subsequent robot arm placement, correction, and other operations. The computer vision system could be applied to the industry to improve the automation level and product quality of industrial production."
9182318,Hardware Security Reconnaissance Application using FCC ID Lookup and Computer Vision,"The reconnaissance process is a time consuming and risky task for hardware researchers and pen-testers. First, the exterior of the device is assessed and then opened to inspect the interior. The device can be seriously damaged due to this and cannot be reused. With the help of Computer Vision - Optical Character Recognition with Machine Learning, implemented in the app, one has to click on the text containing FCC ID real-time and is directed to the site containing all the required information. This saves time and the information of the device is readily available without the need to open up the device. FCC ID is considered as one of the protocols by the pen-testers."
9214101,Computer vision based automated billing system for fruit stores,"This paper describes about the new billing system which is more accurate and time efficient than conventional billing system used in fruit retail shops. The proposed method uses the deep learning neural network approach for classification of fruits and strain gauge type load cell to estimate the weight of the fruit items kept in the basket. The neural network takes input image in the form of clusters and this cluster forms a centroid that is processed to classify fruits like Apple, Banana, Granny Smith, etc. The image processing and weight computation algorithms are implemented in RaspberryPi board with tensor flow and openCV library functions. The proposed system automatically recognize the type of fruits, finds the weight of the fruit in grams, computes the total cost and prints the bill statement."
9219385,Artificial Intelligence Methods Used in Computer Vision,"Artificial intelligence is a branch of research that allows machines to solve intuitive problems by imitating the way the human brain works. Meaningful information can be extracted from image and video analysis using computer vision methods. Computer vision studies in areas such as digital image processing, object recognition, object tracking have been strengthened with artificial intelligence-based components developed in recent years. Thus, faster and more precise results can be obtained. The aim of this study is to reveal the artificial intelligence methods used in the field of computer vision, the principles and constraints of the methods in the light of the related literature."
9071514,A Comparative Analysis on Coordinate Rotation Digital Computer (CORDIC) Algorithm and Its use on Computer Vision Technology,"Computer Vision is a needful technology of present time, further augmented by the current era of Multi Dimension vision System based on high speed network. It has been an established belief that Social media is most needful and promising application for all of us in the present scenario. This generates the need for various computer vision algorithms in order to further improve the quality of social media which in turn leads to impact on the consumer. Most of the computer vision algorithms need the computation consisting of two steps-namely arithmetic computation and trigonometric computation. The complexity is mostly by the trigonometric computations, which generates a need for separate algorithms which performs the entire trigonometric task. In this work one well known algorithm called CORDIC (Rotation Digital Computer), another name is digit by digit approach and Volder's calculation has been addressed. In this, a novel processing strategy is utilized which is particularly reasonable for understanding the trigonometric connections associated with plane organize revolution and change from rectangular to polar directions. CORDIC calculation is additionally appropriate for square root, Logarithmic, Exponential capacity and for computer based computations. In this paper, a comparative study amongst the available CORDIC algorithms has been presented with study of their performance on discrete cosine transform (DCT)application as an example case."
8760862,Computer Vision Approaches to Waste Containers Detection,"The work presented in this article is the result of a prelaminar investigation that aims at using computer vision techniques to replace the current method of performing detection of waste contains via radio-frequency identification. Comparatively to the current method, this approach is more agile and diminishes the resources needed for an implementation. The approach discussed is focused on the use of convolutional neural networks, specifically the network YOLO. Using this method of identification, it was attained an accuracy of 92% of the waste containers."
8867687,Development of Machine Vision System for Tracking Crane Mechanisms,"In this paper, the issue of the technical possibility of a new method of the speed measuring of hoisting and transport equipment is revealed. The research is carried out for a bridge crane mechanical system. It is proposed to replace the system of absolute and incremental encoders with a system of computer vision. This technical solution makes it possible to replace several speed measurement devices with one camera. In addition, the computer vision system creates conditions for the development of a system for positioning of the bridge crane mechanisms. The main purpose of the research is to validate the possibility of technical implementation of the speed measurement system by the described method. This method of mechanism speed calculation is new for this application in the field of hoisting and transport equipment. As a result, the computer vision system was implemented to determine the speed and position of bridge crane mechanisms. The results of the speed measurement by two methods are compared: traditional methods are compared with the use of encoders and the proposed one with a computer vision system."
9587991,Research on Image Segmentation based on Multibranch Convolution Model,"Image semantic segmentation is a very important research field in computer vision task. In view of the discontinuity of segmentation in existing semantic segmentation models, poor segmentation effect on small target objects and large amount of computation, a module similar to Iception is designed, which is called Multi-Branch Convolution (MBC) in this paper to decompose the convolution layer of the atrous space pyramid pooling (ASPP). The experimental results show that the improved network model achieves segmentation results of 71.3%(MIoU) and 90.9%(FWIoU)."
9421306,Research on Human Target Detection and Tracking Based on Artificial Intelligence Vision,"With the rapid development of multimedia technology and the continuous improvement of computer performance, digital video monitoring system has gradually replaced the traditional analog monitoring system and has been widely used in public places such as airports, stations, banks, hotels and so on. As an important part of intelligent video surveillance system, target detection and target tracking technology has become an important research direction in the field of vision. Human behavior detection and recognition based on artificial intelligence vision has developed rapidly due to the increasing demand, and has become a hot spot in the research field of computer vision and artificial intelligence. Under complex background, the detection, recognition and automatic tracking of human moving objects based on artificial intelligence vision are the key and difficult points in the field of computer vision research. In this paper, the image preprocessing, detection and extraction, matching recognition and automatic tracking of human moving targets based on artificial intelligence vision in complex environment are analyzed and studied, which lays a good foundation for the research and implementation of human target vision monitoring system."
9357374,Demonstration of Forward Collision Warning System Based on Real-Time Computer Vision,"This paper demonstrates the software and hardware of a forward-collision warning system using techniques of realtime computer vision which helps self-driving cars and autonomous vehicles systems to merge with the road environment safely and ensure the reliability of these systems. The software approach of the paper consists of five parts: car detection, depth estimation, lane assignation, the relative speed of other cars and their corresponding speed limit and finally ultrasonic sensors which completes the front of the vehicle as the camera can't cover it alone. Besides these five objectives, a real-time notification system exists which interacts with the driver. In this paper, a hardware approach is proposed which implements the software approach techniques on a NVidia Jetson TX2 development kit and AVR microcontroller to prove the feasibility of the proposed collision warning system. The results show the reliability and promptness of the proposed approach by achieving up to 23 FPS (frame per second)."
9537548,Development of a Module for Analyzing Milling Defects Using Computer Vision Defects Using Computer Vision,The article proposes an approach to the use of flaw detection of workpieces using computer vision. The architecture of the system nondestructive testing developed is presented and its main components are described. The module for analyzing milling defects consists of an active mode based on the OpenCV library and a deep analysis mode using the Keras library. The article shows practical tests of the active mode to determine various types of rejects during milling of test workpieces. The sequence of steps for processing photographs of processed workpieces using the Sobel operator and Gaussian filters is described. The article also discusses the basic preparatory operations that must be taken into account before conducting experiments on the detection of defects using computer vision.
9237151,A Survey on the Use of Computer Vision to Improve Software Engineering Tasks,"Software engineering (SE) research has traditionally revolved around engineering the source code. However, novel approaches that analyze software through computer vision have been increasingly adopted in SE. These approaches allow analyzing the software from a different complementary perspective other than the source code, and they are used to either complement existing source code-based methods, or to overcome their limitations. The goal of this manuscript is to survey the use of computer vision techniques in SE with the aim of assessing their potential in advancing the field of SE research. We examined an extensive body of literature from top-tier SE venues, as well as venues from closely related fields (machine learning, computer vision, and human-computer interaction). Our inclusion criteria targeted papers applying computer vision techniques that address problems related to any area of SE. We collected an initial pool of 2,716 papers, from which we obtained 66 final relevant papers covering a variety of SE areas. We analyzed what computer vision techniques have been adopted or designed, for what reasons, how they are used, what benefits they provide, and how they are evaluated. Our findings highlight that visual approaches have been adopted in a wide variety of SE tasks, predominantly for effectively tackling software analysis and testing challenges in the web and mobile domains. The results also show a rapid growth trend of the use of computer vision techniques in SE research."
9487613,PharmaCV: A Cloud Vision Approach on Generating Labels of Pharmaceutical Drugs,"This paper presents a novel and straightforward way of generating labels from images of common pharmaceutical drugs found in the Philippine market using Cloud Vision. The images used were given labels that indicate the name of the drug and its language code used, the application of the drug, other alternate names of the drug and related labels with their corresponding confidence scores. In addition to that, the uniform resource locator or URL that direct to pages with full and partial matching images were given. Furthermore, the images were stored in a cloud storage bucket which offers good documentation, a high durability of 99.99%, in which the data survives even in the event of simultaneous losses, and easy integration with state-of-the-art Google cloud services such as AutoML, Kubernetes, Compute Engine. The Cloud Vision API was used due to its superb features compared to the similar computer vision APIs that are existing today. This study will be useful to both medical practitioners and drug users since it will be easier for them to gather all the necessary information about a particular drug from its image alone. Furthermore, the system developed in this study can be used in drug dispensing machines where the drug users can be provided enough information about the drug and its applications despite having little knowledge about them."
9289768,Integrating Computer Vision and Natural Language Instruction for Collaborative Robot Human-Robot Interaction,"In this paper, we report how we design a human-robot co-working scheme. We believe that natural language is the best way that human and machine can exchange messages. However, understand voice command is not enough, what object is referred in what location is also very import. To better understand a natural language instruction, a robot also require computer vision to resolve the real-world object referencing problem. In this paper we show how to integrate the Google voice recognition result with the YOLO computer vision software with the KINOVA robot arm and achieve a collaborative robot interaction."
9596976,Use of Computer Vision Based Hand Tracking in Educational Environments,"Computer vision and it's use is starting to represent a major part of our daily life. Analysis of captured pictures and the use of the information that they contain can support specific tasks in an increasing number of applications. In this paper we present two out of the box solutions for making teamwork-based education more interesting by using wireless plugins. For this reason we used two different systems for detecting palms and fingers of user's hands. We developed two different applications to test each of the two out of the box solutions. The first application supports teacher-student cooperation. Students on remote computers can draw various shapes and lines fully via hand tracking and the shapes are then simultaneously redrawn on the teacher's or student's computer. It uses Google's MediaPipe library for hand tracking and Node.js for server communication. The second application is a simple video game aimed primarily at preschool children. By playing the game, children learn basic shapes, colors and logical thinking. The application uses the handtrack.js library for hand-tracking and HTML/CSS with JavaScript for the base application. Both applications demonstrate how computer vision can be easily integrated into remote educational user interfaces. We hope that this will stimulate other developers of educational applications to integrate into their application also computer vision based interaction."
9523128,Thermal Image Super-Resolution Challenge - PBVS 2021,"This paper presents results from the second Thermal Image Super-Resolution (TISR) challenge organized in the framework of the Perception Beyond the Visible Spectrum (PBVS) 2021 workshop. For this second edition, the same thermal image dataset considered during the first challenge has been used; only mid-resolution (MR) and high-resolution (HR) sets have been considered. The dataset consists of 951 training images and 50 testing images for each resolution. A set of 20 images for each resolution is kept aside for evaluation. The two evaluation methodologies proposed for the first challenge are also considered in this opportunity. The first evaluation task consists of measuring the PSNR and SSIM between the obtained SR image and the corresponding ground truth (i.e., the HR thermal image downsampled by four). The second evaluation also consists of measuring the PSNR and SSIM, but in this case, considers the ×2 SR obtained from the given MR thermal image; this evaluation is performed between the SR image with respect to the semi-registered HR image, which has been acquired with another camera. The results out-performed those from the first challenge, thus showing an improvement in both evaluation metrics."
8805865,Research on Fresh Quality Detection of Citrus Based on Computer Vision and Neural Network,"In this study, the Tribute Citru was taken as the research object. The color image of Tribute Citru was segmented under B component of color image collected by computer vision system, and the hue frequency sequence of Tribute Citru was extracted under H component of color image as the color feature of Tribute Citru epidermis. Then the structure of the neural network was constructed and optimized, and the mapper structure for nondestructive detection of Tribute Citru fresh quality was 141-8-1. The results showed that the accuracy of computer vision and neural network technology in detecting the taste parameters of fresh Tribute Citru was 86.67%."
9142812,User identification by matching radio “vision” and computer vision through means of machine learning,"In the context of 5G networks, the possibility to fusion ""radio vision"" with ""computer vision"" is a must-have asset, enabler of building the extensive navigation maps through the so-called MirrorWorld. This demo will showcase an essential building block for merging the two environments: matching a user equipment's identity in video stream and in radio measurements. We demonstrate the integration of a computer vision system with a radio access network and showcase the identification of the true radio transmitter between two equipment existing in a video feed."
8969486,Intelligent Optimization Analysis of Target Tracking Based on Computer Vision,"In view of the problems exposed by particle filtering in solving computer vision target tracking, this paper introduces a solution combining ant colony algorithm and Mean Shift. On the one hand, with the help of the ant colony algorithm's spontaneous artificial intelligence search ability, the problem of particle depletion is improved, and the selection of the local optimal position of the target tracking is optimized during the target tracking observation model of particle filtering. On the other hand, the Mean Shift target tracking algorithm is introduced, and the gradient tracking function is used to optimize the target tracking accuracy; in the target state transition model of particle filtering, the global convergence response time of the target tracking is compressed. Simulation experiments show that the improved particle filtering method by ant colony algorithm +Mean Shift not only improves the timeliness and accuracy of target tracking, but also enhances the artificial intelligence attribute of target tracking."
8867786,Fast Method for Triangulation of Point Clouds Obtained from Computer Vision Systems of Industrial Robotic Complexes,"In summary, the paper proposed an algorithm for the fast triangulation of point clouds obtained using computer vision systems based on profile-type laser scanner. The proposed method uses the features of the formed point clouds. The incoming at each scanning step data from laser scanners is formed as a point cloud representing the profile of the surface falling into the scanning plane. These profiles are located one behind the other with a given step and allow to easily detect points for constructing triangles that form a polygonal model. This fact makes it possible to reduce the computational complexity of converting these points to polygonal models tenfold compared with traditional triangulation algorithms. Because of polygonal model usage, it is possible to correctly set the working tools movement trajectory and their desired orientation in the process of movement."
9607570,LCOMS Lab’s approach to the Vision For Vitals (V4V) Challenge,"We present in this paper the LCOMS Lab’s approach to the 1st Vision For Vitals (V4V) Challenge organized within ICCV2021. The V4V challenge was focused on computer vision methods for vitals signs measurement from facial videos, including pulse rate (PR) and respiratory rate.We propose a novel end-to-end architecture based on a deep spatiotemporal network for pulse rate estimation from facial video recordings. Unlike existing methods, we predict the pulse rate value directly without passing by iPPG signal extraction and without incorporating any prior knowledge or additional processing steps. We built our network using 3D Depthwise Separable Convolution layers with residual connections to extract spatial and temporal features simultaneously. This is very suitable for real-time measurement because it requires a reduced number of parameters and a short video fragment. The obtained results seem very satisfactory and promising, especially since the experiments were conducted in challenging dataset collected in uncontrolled conditions."
9268990,Computer Vision in the Operating Room: Opportunities and Caveats,"Effectiveness of computer vision techniques has been demonstrated through a number of applications, both within and outside healthcare. The operating room environment specifically is a setting with rich data sources compatible with computational approaches and high potential for direct patient benefit. The aim of this review is to summarize major topics in computer vision for surgical domains. The major capabilities of computer vision are described as an aid to surgical teams to improve performance and contribute to enhanced patient safety. Literature was identified through leading experts in the fields of surgery, computational analysis and modeling in medicine, and computer vision in healthcare. The literature supports the application of computer vision principles to surgery. Potential applications within surgery include operating room vigilance, endoscopic vigilance, and individual and team-wide behavioral analysis. To advance the field, we recommend collecting and publishing carefully annotated datasets. Doing so will enable the surgery community to collectively define well-specified common objectives for automated systems, spur academic research, mobilize industry, and provide benchmarks with which we can track progress. Leveraging computer vision approaches through interdisciplinary collaboration and advanced approaches to data acquisition, modeling, interpretation, and integration promises a powerful impact on patient safety, public health, and financial costs."
9688554,A Simple Approach for Scripting in Air and Display using Computer Vision,"Online teaching-learning platforms have become an integral part of life, during and post Covid-19 pandemic. In this regard, many teaching-learning accessories have been developed for use. AirPad, the work presented in this article is an application that helps draw one’s imagination on screen by just capturing the motion of object of interest with a camera in air. Computer vision is concerned with the extraction of meaningful information from image data. Continued explorations on computer vision are often concerned with the development of computer algorithms for specific applications. Computer vision is a field of artificial intelligence that works on enabling computers to see, identify and process images in the same way as human vision does, and then provide the appropriate output. Three important tasks involved during computer vision processing are: 1) Detection 2) Tracking 3) Recognition. Computer vision algorithms are utilized to perform the task. The preferred language used is Python due to its exhaustive libraries and easy to use syntax, but can be implemented in any OpenCV supported language. Present work uses PyQt5 which is a python interface for Qt library. It is one of the most used modules in building GUI apps in Python, and that’s due to its simplicity. Following features have been implemented in the present work: i. Functions to draw square, rectangle and circle. ii. Testing Tab to ensure proper traction of object of interest in each frame. iii. Export the work in multiple formats like images and pdf. The user requires a laptop with webcam and a virtual pen which could be a finger to use the application and enjoy its features."
9591790,Visual communication design in tourism products based on computer vision and complex feature mining algorithm,"Visual communication design in tourism products based on computer vision and complex feature mining algorithm is studied in this paper. In essential aspect, the visual communication should be considered. A core typical industrial robot vision application system includes light source, optical imaging system, and image capture system. Hence, this paper considers the complex feature information analysis as the basis for processing the related data. Then, the designed framework is applied to the visual analysis of the tourism products. Furthermore, the proposed model is simulated from different perspectives."
9163467,Smart Parking with Computer Vision and IoT Technology,"Some elemental problems faced by the parking lots in large cities include the difficulty in locating a free parking spot, security of the parked vehicle as well as people parking in a reserved parking spot. In this paper we propose a blended use of the mobile application, computer vision and IoT technologies to counter these problems and if it is implemented, it will surely save some valuable time. We will also be able to guarantee the security of the parked vehicle using automatic security bollards. We will be using Node MCU as a microcontroller and ultrasonic sensors as proximity sensors. We will also be using CCTV camera live footage for verifying readings from the IoT devices to eliminate all the false positives. At all times the system will display the live status of the parking spaces in the parking lots to all the users of the mobile application."
9262275,An Intelligent Computer Vision for Children Affected with Cerebral Palsy,"This article is to improve communication with the children affected with cerebral palsy by using a computer vision. cerebral palsy is a permanent movements disorder that appears in childhood. It affects their movements, sensation, and speaking so the children differ from normal children. The technology can improve communication between the children and parents by using an open cv python programming and convolutional neural network(CNN). It detects the facial expression and body pattern of the children to give accurate results of the emotion or needs of the children. then it intimates the alert message to the parents through the mobile application."
9103956,OpenCV Basics: A Mobile Application to Support the Teaching of Computer Vision Concepts,"Contribution: Open Source Computer Vision Library (OpenCV) Basics is an application designed with the purpose of facilitating the initiation of industrial engineering students in the field of Computer Vision, making the learning process easier, more dynamic and more direct. To this end, an application has been developed for the Android operating system with which users can make use of a wide variety of algorithms available in the OpenCV library. Background: Teaching topics related to Computer Vision can rely on the use of new technologies such as mobile applications. With this type of support, students can learn concepts that might otherwise be difficult to understand. Intended Outcomes: The objective is to facilitate the assimilation of concepts related to Computer Vision by taking advantage of the camera and the processing power of a mobile device to observe in real time the effects produced on an image by many of the image processing algorithms included in OpenCV. This application is currently available to be downloaded for free through the Google Play Store so that anyone interested in the field of Computer Vision can make use of it. Application Design: The proposed approach introduces students to concepts related to Computer Vision by making use of the developed application, complementing the theoretical contents taught by the teacher with specific examples. Findings: The degree of satisfaction of OpenCV Basics users has been evaluated within the framework of the course advanced robotized systems, taught in the industrial engineering degree at the University of La Laguna."
9566211,"Comparative evaluation of computer vision technologies, targeting object identification and localization scenarios","The main objective of this work, is the comparative study, design, and development of techniques towards a more efficient identification of existence of objects, the recognition of spatial localization in cases of multiple identification, as well as the solution of placement problems of objects from one fixed location to another, with the same object characteristics. Two state of the art approaches are considered to achieve these goals. On one hand the TensorFlow framework and on the other hand the OpenCV library. Aiming towards a comprehensive study, we will create a custom demo that will include parameters such as three types of geometric shapes, specific peripheral colors for the three types of shapes, random placement of the shapes on the Cartesian plane and a camera for video capturing at a constant angle. Thus, guided by the common working pattern, a thorough and objective comparison of the performances of the two methodologies in the field of real-time object identification is achieved. This objective is of great technological interest, as mechanical vision has managed to be assimilated by more and more scientific fields, rendering as a prerequisite in the common benefit of their leapfrog development, but also with many possibilities of particularly important extensions. During the development of this paper, both an algorithmic approach has been applied through the OpenCV library and the object-oriented Python programming language, as well as the creation of our own object detection classifier model through the TensorFlow framework. In this way the developed components make the most of the two tools and offering real-time results with exceptional accuracy, through the custom demo in a common and objective framework. Finally, an in-depth analysis of the performance of the two approaches is offered, listing both their positive features and their black spots in multiple scenarios beyond the main demo, in order to cover a wide range of factors which can possibly ...
(Show More)"
9674065,Investigating Vision Transformer Models for Low-Resolution Medical Image Recognition,"Vision Transformers use self-attention techniques to learn long-range spatial relations to focus on the relevant parts of an image. They have achieved state-of-the-art results in many computer vision tasks. Recently, some methods have to leverage Vision Transformer-based models to tackle tasks in medical imaging. However, Vision Transformer emphasizes the low-resolution features due to the repetitive downsamplings, which result in a loss or lack of detailed localization information, making it highly unfit for low-level image recognition. In this paper, we investigate the performance of Vision Transformer on low-level medical images and contrast it with convolutional neural networks. The experimental results show that Convolutional Neural Network outperforms the Vision Transformer-based models on all four datasets."
9447034,A Review of Computer Vision Methods in Network Security,"Network security has become an area of significant importance more than ever as highlighted by the eye-opening numbers of data breaches, attacks on critical infrastructure, and malware/ransomware/cryptojacker attacks that are reported almost every day. Increasingly, we are relying on networked infrastructure and with the advent of IoT, billions of devices will be connected to the Internet, providing attackers with more opportunities to exploit. Traditional machine learning methods have been frequently used in the context of network security. However, such methods are more based on statistical features extracted from sources such as binaries, emails, and packet flows. On the other hand, recent years witnessed a phenomenal growth in computer vision mainly driven by the advances in the area of convolutional neural networks. At a glance, it is not trivial to see how computer vision methods are related to network security. Nonetheless, there is a significant amount of work that highlighted how methods from computer vision can be applied in network security for detecting attacks or building security solutions. In this paper, we provide a comprehensive survey of such work under three topics; i) phishing attempt detection, ii) malware detection, and iii) traffic anomaly detection. We also discuss existing research gaps and future research directions, especially focusing on how network security research community and the industry can leverage the exponential growth of computer vision methods to build much secure networked systems. Finally, we review a set of such commercial products for which public information is available and explore how computer vision methods are effectively used in those products and conclude with a brief overview of commonly used computer vision methods in this domain."
8965907,Implementation and Verification of a Virtual Testing System Based on ROS and Unity for Computer Vision Algorithms,"With the development of artificial intelligence technology, Computer vision algorithm is playing an increasingly important role. Computer vision algorithm testing is a vital link to ensure the safe and reliable operation of agents. However, the traditional testing methods based on real scenes provide single samples and are challenging to obtain ground truth, which makes it inefficient to test computer vision algorithms. To solve this problem, the testing method of computer vision algorithms using virtual scenes instead of real scenes has been applied. In this paper, Unity and robot operating system (ROS) are selected to build a virtual testing system named URCV for computer vision algorithms. The feasibility of the system and the influence of virtual scene elements on the testing of the monocular ORB_SLAM2 algorithm are verified, including the rendering path of Unity's RGB camera, texture accuracy, and illumination model."
9260214,Application of Computer Vision for Estimation of Moving Vehicle Weight,"Heavy vehicle weights need to be closely monitored for preventing fatigue-induced deterioration and critical fractures to highway infrastructure, among many other purposes, but development of a cost-effective weigh-in-motion (WIM) system remains challenging. This paper describes the creation and experimental validations of a computer vision-based non-contact WIM system. The underlining physics is that the force exerted by each tire onto the road is the product of the tire-road contact pressure and contact area. Computer vision is applied (1) to measure the tire deformation parameters so that the tire-roadway contact area can be accurately estimated; and (2) to recognize the marking texts on the tire sidewall so that the manufacturer-recommended tire inflation pressure can be found. In this research, a computer vision system is developed, which is comprised of a camera and computer vision software for measuring tire deformation parameters and recognizing the tire sidewall markings from images of individual tires of a moving vehicle. Computer vision techniques such as edge detection and optical character recognition are applied to enhance the measurement and recognition accuracy. Field experiments were conducted on fully loaded or empty concrete trucks and the truck weights estimated by this novel computer vision-based non-contact WIM system agreed well with the curb weights verified by static weighing. This research has demonstrated a novel application of the computer vision technology to solve a challenging vehicle WIM problem. Requiring no sensor installation on the roadway or the vehicle, this cost-effective non-contact computer vision system has demonstrated a great potential to be implemented."
8922831,A Study of User Interface with Wearable Devices Based on Computer Vision,"Smart wearable devices are widely used in the field of healthcare. This article presents three approaches to human-computer interaction (HCI) via computer vision and hand gestures. They are timeline-user interface, virtual keyboard user interface, and handwritten digit user interface, respectively. These user interfaces are achieved by including the image process and machine learning, such as Convolutional Neuron Networks and AdaBoost. To evaluate the approaches, a prototype of the android device with a built-in color camera was employed. The result of the handwritten digit model was significant for high accuracy of 92.9%. In our view, the results emphasize the validity of our models."
8729605,Convolutional neural networks of the YOLO class in computer vision systems for mobile robotic complexes,"An important scientific direction is the development and study of computer vision systems (CVS) for mobile robotic complexes. Today, developers of CVS are most often using convolutional neural networks (CNN). For increasing the speed detection of objects on images in CVS, there has been a trend of using CNN, which are hardware-implemented on field-programmable gate array (FPGAs).This article shows that the perspective for hardware implementation on the FPGA is the tiny-YOLO CNN from the YOLO class. For reduce required FPGA computing resources in this CNN, was proposed to use Inception-ResNet modules. We was found that with high detection accuracy of objects in images with minimum resources requirements provide by the tiny-YOLO-Inception-ResNet2 network architecture. It is obtained from replacing the fifth tiny-YOLO convolutional layer of the tiny-YOLO CNN with two sequential processing Inception-ResNet modules. Also results of the study of the detection accuracy of objects using the CNN for this architecture with the lack of resource-intensive operations: batch normalization and bias from calculations were given. These studies were performed for different formats of representation numbers in the FPGA."
8956759,Imaging Flow Cytometry at >13K events/s Using GPU-Accelerated Computer Vision,"Flow cytometers are widely used to rapidly measure characteristics of single cells. Typical laser-based instruments provide throughputs of >10,000 events/s; however, the number of measured features is typically small and apply to the entire cell volume. Imaging flow cytometers (IFC) rely instead on 2D images of the objects, providing hundreds to millions of spatially resolved features. However, the throughput of IFCs is typically lower (several thousand events/s) due to the computational overhead of 2D image processing. Here, we demonstrate a GPU-accelerated computer vision analyzer which substantially increases computational throughput. When coupled to a 300 frame per second (fps) real-time camera, the system is limited by the camera and analyzes 1260 particles/s in a 500x700 pixel video with 4-5 particles/frame. When reading from a solid state disk, the throughput increases to 4500 fps with ~3 particles per frame, resulting in a throughput of 13,500 particles/s. The reported throughput is 2.5-4X higher than existing technologies, paving the way for ultra-high throughput IFC."
9522718,NTIRE 2021 Depth Guided Image Relighting Challenge,"Image relighting is attracting increasing interest due to its various applications. From a research perspective, im-age relighting can be exploited to conduct both image normalization for domain adaptation, and also for data augmentation. It also has multiple direct uses for photo montage and aesthetic enhancement. In this paper, we review the NTIRE 2021 depth guided image relighting challenge.We rely on the VIDIT dataset for each of our two challenge tracks, including depth information. The first track is on one-to-one relighting where the goal is to transform the illumination setup of an input image (color temperature and light source position) to the target illumination setup. In the second track, the any-to-any relighting challenge, the objective is to transform the illumination settings of the in-put image to match those of another guide image, similar to style transfer. In both tracks, participants were given depth information about the captured scenes. We had nearly 250 registered participants, leading to 18 confirmed team sub-missions in the final competition stage. The competitions, methods, and final results are presented in this paper."
9474360,Adaptive Autonomy in Human-on-the-Loop Vision-Based Robotics Systems,"Computer vision approaches are widely used by autonomous robotic systems to sense the world around them and to guide their decision making as they perform diverse tasks such as collision avoidance, search and rescue, and object manipulation. High accuracy is critical, particularly for Human-on-the-loop (HoTL) systems where decisions are made autonomously by the system, and humans play only a supervisory role. Failures of the vision model can lead to erroneous decisions with potentially life or death consequences. In this paper, we propose a solution based upon adaptive autonomy levels, whereby the system detects loss of reliability of these models and responds by temporarily lowering its own autonomy levels and increasing engagement of the human in the decision-making process. Our solution is applicable for vision-based tasks in which humans have time to react and provide guidance. When implemented, our approach would estimate the reliability of the vision task by considering uncertainty in its model, and by performing covariate analysis to determine when the current operating environment is illmatched to the model's training data. We provide examples from DroneResponse, in which small Unmanned Aerial Systems are deployed for Emergency Response missions, and show how the vision model's reliability would be used in addition to confidence scores to drive and specify the behavior and adaptation of the system's autonomy. This workshop paper outlines our proposed approach and describes open challenges at the intersection of Computer Vision and Software Engineering for the safe and reliable deployment of vision models in the decision making of autonomous systems."
9484015,Performance of Texture Compression Algorithms in Low-Latency Computer Vision Tasks,"Deep learning has been successfully used for computer vision tasks, but its high computational cost limits the adoption in lightweight devices such as camera sensors. For this reason, many low-latency vision systems offload the inference computation to a local server, requiring fast (de)compression of the source images. Texture compression is a compelling alternative to existing compression schemes, such as JPEG or HEVC, due to its low decoding overhead, straightforward parallelization, robustness, and a fixed compression ratio. In this paper, we study the impact of lightweight bounding box-based texture compression algorithms, BC1 and YCoCg-BC3, on the accuracy of two computer vision tasks: object detection and semantic segmentation. While JPEG achieves superior per-pixel error rate, the YCoCg-BC3 encoding can provide comparable vision accuracy. The BC1 encoding results in significant degradation of vision performance. However, by retraining the FasterSeg teacher network with a BC1-compressed dataset, we reduced its segmentation mIoU loss from 2.7 to 0.5 percent. Thus, both BC1 and YCoCg-BC3 encoders are suitable for use in low latency vision systems, since they both achieve significantly higher encoding speed than JPEG and their decoding overhead is negligible."
8821822,Computer Vision Based Vessel Seam Detection And Tracking In Fetoscopic Images,"In Twin to twin transfusion syndrome in monochrome twin pregnancies, fetus communicated through Arterio arteries, Veno Venus, Artery to Vein seam. During fetoscopic laser occlusion surgery, the identification of artery and vein separation hasthe misperception due to its color resemblance. In digital fetoscopic images the artery occurs in the bright red region, vein occurs in dark red region, Artery to vein communicative blood vessel and it is correlation regions are needed for surgery. Manually identification of these vessels is highly complicated when the laser beam is passed through the vessels. To overcome this problem, Color region based Vector Quantization method is proposed to identify the artery to vein junction. This method differentiate regions based on the colour resemblance. In this proposed method, Artery to artery (AA),Vein to Vein (VV),Artery to vein(AV) region based samples are taken to distinguish AV anastomos and coagulation. Various state of fetoscopic images are analysed based on different radiation conditions. Proposed method separates AA, VV & AV regions. Automated vessel detection and separation of different vessel regions were achieved using a system based on fetoscopic laser applied images. The automated system provides a clinically feasible and supportive method during fetus surgery. This method may be improved further for computer- or robot-assisted applications."
9150658,Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision,"While deep neural networks have become the go-to approach in computer vision, the vast majority of these models fail to properly capture the uncertainty inherent in their predictions. Estimating this predictive uncertainty can be crucial, for example in automotive applications. In Bayesian deep learning, predictive uncertainty is commonly decomposed into the distinct types of aleatoric and epistemic uncertainty. The former can be estimated by letting a neural network output the parameters of a certain probability distribution. Epistemic uncertainty estimation is a more challenging problem, and while different scalable methods recently have emerged, no extensive comparison has been performed in a real-world setting. We therefore accept this task and propose a comprehensive evaluation framework for scalable epistemic uncertainty estimation methods in deep learning. Our proposed framework is specifically designed to test the robustness required in real-world computer vision applications. We also apply this framework to provide the first properly extensive and conclusive comparison of the two current state-of-the- art scalable methods: ensembling and MC-dropout. Our comparison demonstrates that ensembling consistently provides more reliable and practically useful uncertainty estimates. Code is available at https://github.com/fregu856/evaluating_bdl."
9087013,Drowsiness Detection of a Driver using Conventional Computer Vision Application,"The developments in technology over the years bring the support to drivers using smart vehicle systems. In past few years, there has been substantial increase in road accidents in India and worldwide as well. The most significant reasons for the same are drowsiness and fatigue. Therefore, driver drowsiness and fatigue detection is major possible area to prevent a large number of sleep induced road accidents. Considering this problem, this article proposes a Real-Time Drowsiness Detection System (RT-DDS) applicable in motor vehicles with the help of conventional Computer Vision applications. The system employed various Computer Vision applications using blink rate, eye closure, yawning to effectively and quickly identify the drowsiness of a driver during driving the vehicle and alter the driver accordingly. The proposed work tried to contribute in reducing the increased number of road accidents while keeping the methodologies simple and intact."
9400691,Sliding Holt Algorithm Implementation in Mobile Robots Collision Detection with Dynamic Obstacles Based on Computer Vision Technologies,"The aim of this research is to implement Computer Vision technologies on existing published concept proposed by the same author in previous researches ""Collaborative and Non-Collaborative Dynamic Path Prediction for Mobile Agents Collision Detection with Dynamic Obstacles"". Author proposed usage of Computer Vision technologies in order to increase independency of single robotic units in the swarm. This new method and algorithm is based on analysis of behavior of human objects and its implementation in form of functional method and algorithm which can be used in mobile robotics. In prior research papers, several new terms are proposed and explained such as Metamorphous Hyperspace, Relevant predicted collision time, Coefficient of agility etc. The method implements human behavior in mobile robotics in a way it allows full decentralization of collision detection and ensures many other advantages starting from minimizing network traffic to simplifying inclusion of additional agents in relevant workspace. Algorithm requires a negligible amount of resources allowing mobile agents to exploit more resources for additional tasks. This method and algorithm can be implemented in all kinds of vehicles: ground, naval or airborne objects. Experimental model using Computer Vision technology OpenCV library is implemented and experimental result are described in this paper."
9065627,A Self-Driving Car Implementation using Computer Vision for Detection and Navigation,"In recent years, lidar has been used as primary sensors for self-driving cars, however, due to their high expense, it becomes infeasible for mass production. Hence, we present the working of a self-driving car prototype that relies upon a cheaper alternative, viz. cameras. The primary objective of our prototype is to navigate safely, quickly, efficiently and comfortably through our virtual environment using computer vision. We have performed detection of lanes, traffic cars, obstacles, signals, etc. and have used the concept of stereo vision for depth calculation. Trajectory planning and steering control have also been implemented. Experimental results show that camera-based self-driving cars are viable and thus our paper can provide a foundation for all future real-world implementations."
9491776,Computer Vision Based Skin Disorder Recognition using EfficientNet: A Transfer Learning Approach,"Skin disorders have a vital impact on people's health and quality of life, it is essential to develop a reliable and accurate computer vision approach to recognize multiple skin disorders. In this paper, a new rapid recognition approach using EfficientNet has been introduced to diagnose twenty types of skin disorders. Initially, image augmentation techniques have employed, and then eight architectures of EfficientNet between B0 and B7 have trained using the transfer learning approach. To evaluate the performance of models, different experimental studies have employed using a test set of 6300 images of skin disorders dataset that makes the proposed approach more reliable and accurate. EfficientNet-B7 has achieved the highest accuracy 97.10% among all architectures but has taken longer training time. EfficientNet-B0 has taken the lowest training time and has achieved 93.35% accuracy. EfficientNet-B7 has also taken the lowest time in recognizing unseen new images of skin disorders accurately than others."
9036621,Rough Terrain Autonomous Vehicle Control Using Google Cloud Vision API,"Computer vision (CV) represents a technology using which information contained in a visual scenery is obtained by capturing and digital processing of the images. Computer vision is becoming more popular and widely used in automated vehicles, robotics to emulate human vision capability and industrial automation. Google cloud vision API is a remote real time image processing platform to process, classify and acquire required metadata from image and videos. In this work, design and implementation of a rough terrain autonomous vehicle using Rocker Bogie mechanism and Google cloud vision API for obstacle avoidance is presented. Rocker Bogie mechanism is employed for maneuvering the robot on rough terrains."
8982037,Scaling Camouflage: Content Disguising Attack Against Computer Vision Applications,"Recently, deep neural networks have achieved state-of-the-art performance in multiple computer vision tasks, and become core parts of computer vision applications. In most of their implementations, a standard input preprocessing component called image scaling is embedded, in order to resize the original data to match the input size of pre-trained neural networks. This article demonstrates content disguising attacks by exploiting the image scaling procedure, which cause machine's extracted content to be dramatically dissimilar with that before scaled. Different from previous adversarial attacks, our attacks happen in the data preprocessing stage, and hence they are not subject to specific machine learning models. To achieve a better deceiving and disguising effect, we propose and implement three feasible attack approaches with L 0 - and L ∞ -norm distance metrics. We have conducted a comprehensive evaluation on various image classification applications, including three local demos and two remote proprietary services. We also investigate the attack effects on a YOLO-v3 object detection demo. Our experimental results demonstrate successful content disguising against all of them, which validate our approaches are practical."
9409358,Development of Mobile Robot System Based on Computer Vision,"In order to solve the adaptability problem of traditional logistics robots' grabbing and handling operations in automated production lines, and to realize the rapid identification, positioning, grabbing and handling of target objects, a design method of omnidirectional mobile logistics robot with binocular vision is proposed. The system integrates the application of omnidirectional mobile platform, material grabbing robot, binocular visual recognition and other technologies. The mechanical mechanism principle and kinematics model of the robot are analyzed. Image recognition and target contour extraction are completed by the visual recognition algorithm. The target object is positioned by camera calibration. Finally, a set of physical prototypes was designed and tested. The experimental results show that the system can effectively complete item identification, grabbing and handling."
9534988,Deep Learning for Oil Palm Fruit Ripeness Classification with DenseNet,"Oil palm fruit farming is one of the most leading agriculture industries in the South East Asia region. Unfortunately, most of the harvesting method is still done through manual labor. Multiple research has been conducted to help farmers automatically detect the ripeness level of the oil palm fruit. We conducted an experiment of detecting the ripeness level of oil palm fruit by using one of the state-of-the-art computer vision approaches, which was deep learning with a Convolutional Neural Network (CNN). The dataset used consists of 7 levels of ripeness with 400 images of oil palm fruits. The models used in this test scenario were both AlexNet and DenseNet. The result of this study showed that DenseNet has been proven to outperform AlexNet by 8.5% in terms of accuracy and 8% in F1 score."
9564215,Opportunistic Resource Allocation driven by pattern recognition to attain High Availability in Vision based Edge Compute Systems,"Managing High Availability (HA) among Computer Vision Systems deployed on Edge devices requires expensive hardware redundancy. The cost of scaling such systems becomes prohibitive. We present an alternative approach for managing high availability by building this capacity at an application layer. To this end, we propose the use of pattern recognition components to make the application context aware. This intelligent application is then itself able to decide which Edge modules are running redundant or less significant processes. For example, if the module is processing uneventful video streams, the application may decide to make this compute available to some other module. In the event of hardware failures, this application can then provision workloads on idle hardware (or hardware running less significant workload)."
9407682,A Replay Method for Gobang Chessboard Based on Machine Vision,"Aiming at the checkerboard recovery problem, a checkerboard replay method based on machine vision is proposed to process the images and replay the gobang chessboard. In this method, after being processed, the obtained images are converted into the corresponding array matrix, where black chess is represented by 1, white chess is represented by -1, and no chess piece is represented by 0.,the visualization of the replay of gobang chessboard is realized by setting the checkerboard and reading the matrix of the checkerboard data on the images with MATLAB This vision method could also be introduced to bring convenience and profits to other fields like Chinse chessboard and so on."
9498393,Applications of Computer Vision for Defect Detection in Fruits: A Review,"Visual appearance in grading fruits and vegetables play a crucial role on the customer preferences, market value, customer choice as well as internal quality. The surface defects are characterised by colour, texture, shape and size. Over the recent decades variety of vision systems have been designed to inspect and evaluate the quality produce of fruits and vegetables. Among these freuquently used and popular are the hyperspetral and multispectral system used as large scale in food industries along the automation line. Many researchers have designed the computer vision systems in automated industrial operations that help to capture images and extract the details of the infected area of the product. This paper provides a detailed over view of different computer vision systems, pre-processing techniques, segmentation methods, features - colour, texture and morphological as well as different classifiers applied in quality inspection of different fruits."
9670992,Dangerous Object Detection for Visually Impaired People using Computer Vision,"In this contemporary world, Artificial Intelligence and Machine Learning are one of the leading technologies creating an impact in the world by mimicking human behaviour to solve a particular problem. Hence, these technologies are widely used to aid different obstacles encountered by humans. One such problem widely faced by the mankind is visual impairment. According to World Health Organization, approximately 285 million people suffer with vision impairment. Therefore, applications of machine learning and computer vision can be applied to guide the people with such problems. This paper presents the idea of using object detection to aid the visually impaired people. In this paper, an experiment has been proposed which uses a custom-built image dataset of various dangerous objects. The objects have been categorized into 5 broad categories: Sharp objects, Danger signs, Broken glass, Manhole and Fires. A number of different algorithms have been trained on this custom image dataset containing the menacing objects and their performances have been evaluated. The evaluation indicators for the models are the validation error in terms mean Average Precision (mAP) and the processing time for each model. The models have also been tested in real world scenario by evaluating on a custom video to gauge their performance in terms of accuracy in detection of different objects as well as their ease in deployment by suggesting their frame rate handling capacity. The results are discussed and the most robust and balanced model is suggested at the end of the paper."
9074370,Sign Language Translation,"Sign language is the way of communication for hearing impaired people. There is a challenge for common people to communicate with deaf people which makes this system helpful in assisting them. This project aims at implementing computer vision which can take the sign from the users and convert them into text in real time. The proposed system contains four modules such as: image capturing, preprocessing classification and prediction. By using image processing the segmentation can be done. Sign gestures are captured and processed using OpenCV python library. The captured gesture is resized, converted to grey scale image and the noise is filtered to achieve prediction with high accuracy. The classification and predication are done using convolution neural network."
9421232,Design and Implementation of Intelligent Car based on Machine Vision,"With the rapid development of science and technology, the development of Internet of Things technology, machine vision technology and driverless technology is increasingly mature. The intelligent car in this design research is based on OpenCV's machine vision, which can realize automatic driving, image recognition, background monitoring, environmental monitoring and other functions. Background data server is developed based on the open source framework Springboot, which can obtain real-time car monitoring video and various monitoring data."
9022051,The Seventh Visual Object Tracking VOT2019 Challenge Results,"The Visual Object Tracking challenge VOT2019 is the seventh annual tracker benchmarking activity organized by the VOT initiative. Results of 81 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The evaluation included the standard VOT and other popular methodologies for short-term tracking analysis as well as the standard VOT methodology for long-term tracking analysis. The VOT2019 challenge was composed of five challenges focusing on different tracking domains: (i) VOTST2019 challenge focused on short-term tracking in RGB, (ii) VOT-RT2019 challenge focused on ""real-time"" shortterm tracking in RGB, (iii) VOT-LT2019 focused on longterm tracking namely coping with target disappearance and reappearance. Two new challenges have been introduced: (iv) VOT-RGBT2019 challenge focused on short-term tracking in RGB and thermal imagery and (v) VOT-RGBD2019 challenge focused on long-term tracking in RGB and depth imagery. The VOT-ST2019, VOT-RT2019 and VOT-LT2019 datasets were refreshed while new datasets were introduced for VOT-RGBT2019 and VOT-RGBD2019. The VOT toolkit has been updated to support both standard shortterm, long-term tracking and tracking with multi-channel imagery. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website."
9160103,External and internal quality inspection of aerospace components,"In this extended abstract, we present a computer vision systems that, using machine learning approaches, is able to inspect both internal and external parts of aerospace components. The problem is divided in two main inspection tasks: (i) model-checking of avionic components and (ii) duct-inspection for defects detection in cavities."
9576522,A Survey on Automatic Inspections of Overhead Contact Lines by Computer Vision,"Automatic inspections of overhead contact lines (OCLs) are developed to implement anomaly detection during normal operation. It is an essential prerequisite for efficient maintenance of railway electrification system. This paper presents a comprehensive survey on the inspections of OCLs with an emphasis on computer vision technology, which has developed rapidly due to its ability to understand images. Our survey begins with a brief introduction on anomalies in OCL inspections and generic procedures of computer vision inspection for anomaly detection. Subsequently, for detecting deviations of parameters and defective components during OCL inspections, the existing techniques involving stereo vision and object vision, especially convolution neural networks are described in detail from two aspects: measurement of OCL parameters and identification of OCL conditions. Some interference factors in OCL inspection are analyzed. Actual cases of the inspections are also briefly shown. Challenges and suggestions for further research on OCL inspection are drawn toward the end of the paper."
9221786,Preliminary design of a multipurpose UAV situational awareness platform based on novel computer vision and machine learning techniques,"There has been an increased trend in the use of Unmanned Aerial Vehicles (UAVs) providing essential assistance and enhanced situational awareness in various application domains. Considering the sensing and intelligent processing capabilities of UAV platforms, operational managers are able to make smart decisions by having a more complete picture in real-time about the state of the managed event. In this paper, a preliminary design of a multipurpose UAV based situational awareness platform is proposed by exploiting advanced on-board processing capabilities and efficient computer vision, image processing and machine learning techniques. The proposed design consists of aerial and terrestrial surveillance assets able to provide rapid and reliable real-time event information of areas of interests in emergency operations. Experimental results are provided using RGB and thermal video datasets and applying novel object detection and tracking algorithms. The results show the utility and the potentials of the proposed design, while future directions for extension and optimization of the multipurpose UAV based situational awareness system are presented."
8782330,New Approach for Intelligent Street Lights Using Computer Vision and Wireless Sensor Networks,"Smart city is expressed in many different ways by people today. In recent years, smart cities and green technology have become one of the most important and promising items on the world agenda to prepare a better future. The concept of smart city has received great interest in the context of urban development policies. The goal of a smart city is to better use public resources while improving the quality of services provided to citizens while reducing the operational costs of public administration. In this study, wireless sensor networks for smart cities are developed based on smart street light applications. The street lights are equipped with wireless sensor kits to monitor the temperature, humidity and carbon dioxide information of the environment and to monitor the changes in the regions. In addition, the mobility of the environment is analyzed with the camera attached to each sensor kit. The sensor kit was developed using Raspberry pi programming card. A sensor kit for this card has been developed. A wireless sensor network has been created by adding Xbee wireless modules on the sensor kit. The obtained data is collected in a database. The collected data is displayed to the user through an interface."
9098009,Drought Stress Detection Using Low-Cost Computer Vision Systems and Machine Learning Techniques,"The real-time detection of drought stress has major implications for preventing cash crop yield loss due to variable weather conditions and ongoing climate change. The most widely used indicator of drought sensitivity/tolerance in corn and soybean is the presence or absence of leaf wilting during periods of water stress. We develop a low-cost automated drought detection system using computer vision coupled with machine learning (ML) algorithms that document the drought response in corn and soybeans field crops. Using ML, we predict the drought status of crop plants with more than 80% accuracy relative to expert-derived visual drought ratings."
8966217,The obstacles detection for outdoor robot based on computer vision in deep learning,"In this thesis, we propose a guided robot with deep learning techniques and machine vision in order to predict distance and avoid obstacles. The training data images are captured by the stereo camera, and output results are disparity of the single image. Then, the distances are converted by using the triangulation method of computer vision. The back propagation neural network retrains to obtain the actual distance of each pixel in the image. Therefore, the robot with the monocular camera could know the distance between obstacles and itself. Semantic segmentation is utilized to a to distinguish road and obstacles in the image. Fuzzy theory for calculating the area of the road which be cut is designed to avoid walking into the intersection. The obstacle depth and the walkable area are taken as an information for avoiding obstacles control. Because of the robot is proposed to walk on the right side along the road, the edge of road is necessary to navigate the robot. The Hough method is used to find the straight line, and then choose the line we need. The path planning is also achieved in through of navigation and obstacle avoidance control. As a result, the robot can arrive the destination safely and precisely."
9068830,A Computer Vision System for Bangladeshi Local Mango Breed Detection using Convolutional Neural Network (CNN) Models,"Magnifera Indica, traditionally known as mango, is a drupe found around the world in over 500 species. India has produced 19.5 million metric tons of mango in 2017. In Bangladesh, mango has been referred as the national tree and government has included endemic species of mango as geographical index (GI) of Bangladesh. Recognizing specific breeds has become a significant computer vision task. In this paper, we have proposed the convolutional neural network (CNN) based approach for detecting five mango species namely, Chosha, Fazli, Harivanga, Lengra and Rupali from 15000 different images. For better experimentation, we have applied three different models of CNN and analyzed the recognition rates with various criteria. For performance evaluation, we have utilized the classic metrics such as precision, recall, F1-score, ROC and accuracy. Among the experimented three models, the third model, outperformed in terms of accuracy with 92.80%."
8673156,"Human-Centered, Ergonomic Wearable Device with Computer Vision Augmented Intelligence for VR Multimodal Human-Smart Home Object Interaction","In the future, Human-Robot Interaction should be enabled by a compact, human-centered and ergonomic wearable device that can merge human and machine altogether seamlessly by constantly identifying each other's intentions. In this paper, we will showcase the use of an ergonomic and lightweight wearable device that can identify human's eye/facial gestures with physiological signal measurements. Since human's intentions are usually coupled with eye movements and facial expressions, through proper design of interactions using these gestures, we can let people interact with the robots or smart home objects naturally. Combined with Computer Vision object recognition algorithms, we can allow people use very simple and straightforward communication strategies to operate telepresence robot and control smart home objects remotely, totally “Hands-Free”. People can wear a VR head-mounted display and see through the robot's eyes (the remote camera attached on the robot) and interact with the smart home devices intuitively by simple facial gestures or blink of the eyes. It is tremendous beneficial for the people with motor impairment as an assistive tool. For the normal people without disabilities, they can also free their hands to do other tasks and operate the smart home devices at the same time as multimodal control strategies."
9204043,A Survey on how computer vision can response to urgent need to contribute in COVID-19 pandemics,"The coronavirus first outbreak in Wuhan city of China by December 2019. Due to its highly contagious power, they spread promptly in the four continents. Moreover, it devastating our daily lives and cause huge economic damage. Therefore, it is urgent to detect the positive cases at the earliest and put then under isolation. Automatic virus detection using Machine Learning will be a valuable contribution to prevent the spread of this epidemic. The purpose of this paper is to present short reviews on the coronavirus detection. In reviewing the existing works, we summarized and compared some related works performed on a collection of CT and X-ray images provided from infected patients. We conclude the paper with some discussions on how computer vision can response to urgent need to contribute in pandemics and to investigate many aspects of new viral replication and pathogenesis."
8949599,Experimental Performance Evaluation of Computer Vision for an Upper Limbs Rehabilitation Robot,"Rehabilitation robot for upper limbs can help physicians or therapists in conducting repetitive rehabilitation to patients. This robot is constructed by several electromechanical actuators, sensors, controllers, and the main computer. The patient holds a robot gripper in the left and right hands and moves the gripper according to a rehabilitation protocol determined by the physician who is in charge. A computer vision-based position sensor is designed to measure wrist position without using any marker. Moreover, a new algorithm is proposed to filter coordinate values measured by the video camera and extract the position estimate of the wrist. This sensor system estimates positions of the wrist using OpenPose library. The experimental performance was evaluated in terms of precision and speed for several resize values. Experimental results indicate that the pose-based computer vision algorithm can potentially be used for the upper limbs rehabilitation robot. Our finding stimulates further research to find more consistent values of the precision performance index."
8950915,Detection of Gear Tooth Number and Common Normal Length Change Based on Computer Vision,"Combining computer technology and camera equipment, teeth of the collected gear images were ""discretized"" by algorithm and the number of teeth were counted. The radius and modulus of the addendum circle were further obtained to obtain the radius of the base circle. By solving the midpoint where the base circle and tooth profile intersect on the base circle, the equation of the tangent line tangent to the tooth profile was obtained. The length by which the tangent line intersects with the tooth profile across k teeth is the common normal length. It is significant to detect gear tooth number and common normal length using computer vision system instead of manual detection. The shot images were subject to a series of images processing using OpenCV combined with the algorithm, and the length was calculated, which is helpful to achieve non-contact detection of gear precision."
9002256,Landslide detection system using computer vision approach and Raspberry Pi,"Landslide is a natural hazard, which badly affects people's safety and property. Continuous monitoring of such a disaster may lead to reducing of losses of human lives. With this aim, in this paper, we have proposed a surveillance system for real time landslide monitoring using computer vision algorithms. Here we have used a video camera to acquire live view of landslide site and a small computer board `raspberry Pi' to run the algorithm we will introduce here. When landslide is detected SMS text messages are transmitted using a GSM modem. Due to video camera and Raspberry Pi, this method is inexpensive yet efficient and also requires low power, therefore this method can be used in any region. We have used median filtering to remove noise present in the detection algorithm. Here we have also proposed a new implementation method for median filtering using a linked list and parallel processing which is very time efficient."
9093373,Fine-grained Image Classification and Retrieval by Combining Visual and Locally Pooled Textual Features,"Text contained in an image carries high-level semantics that can be exploited to achieve richer image understanding. In particular, the mere presence of text provides strong guiding content that should be employed to tackle a diversity of computer vision tasks such as image retrieval, fine-grained classification, and visual question answering. In this paper, we address the problem of fine-grained classification and image retrieval by leveraging textual information along with visual cues to comprehend the existing intrinsic relation between the two modalities. The novelty of the proposed model consists of the usage of a PHOC descriptor to construct a bag of textual words along with a Fisher Vector Encoding that captures the morphology of text. This approach provides a stronger multimodal representation for this task and as our experiments demonstrate, it achieves state-of-the-art results on two different tasks, fine-grained classification and image retrieval. The code of this model will be publicly available at 1 ."
9214197,Computer vision methods for fragmented skull prototyping: Bio-CAD application,"In biomedical domain, human skull fractures caused by accidents or any other reasons are required to reassemble. The process of reassembling the fragmented skulls is called skull prototyping or skull completion, followed by the skull repairing process. In the human body, skulls are the regions that are fragmented along a wide range of geometric information that may be fragile. Resolved skull has been supposed to possess higher conformity, particularly under the area of frontal facial along the subtle substrate. With the development of the computer-aided design (CAD) tool, it is possible to complete the fragmented skulls and/or repair damaged skulls using the various computer vision techniques. The bio-CAD methods are introduced for fragmented skull completion and/or damaged skull repairing. This paper has presented a study of various skull prototyping methods. The main focus of this study is to review the computer vision techniques used for human skull prototyping. A systematic study of the state of methods and their comparative analysis is presented in this paper. The automatic skull completion and/or repairing identified as the critical challenge and requirement according to the reviewed literature."
9270512,Research on the Metaphorical Features of Computer Language in English from the Perspective of Cognition,"With the rapid development of internet information age, the research of computer is more in-depth. Metaphor embodies people's way of thinking and behavior. The analysis of the basic structure and cognitive function of computer metaphor cannot be separated from the study of cognitive perspective. This paper first expounds the concept of computer metaphor, including the structure of computer metaphor, the general understanding of computer, and metaphor on computer interface. At last, this paper studies the metaphors hidden in the computer network."
9423205,Multi-Modal Reasoning Graph for Scene-Text Based Fine-Grained Image Classification and Retrieval,"Scene text instances found in natural images carry explicit semantic information that can provide important cues to solve a wide array of computer vision problems. In this paper, we focus on leveraging multi-modal content in the form of visual and textual cues to tackle the task of fine-grained image classification and retrieval. First, we obtain the text instances from images by employing a text reading system. Then, we combine textual features with salient image regions to exploit the complementary information carried by the two sources. Specifically, we employ a Graph Convolutional Network to perform multi-modal reasoning and obtain relationship-enhanced features by learning a common semantic space between salient objects and text found in an image. By obtaining an enhanced set of visual and textual features, the proposed model greatly outperforms previous state-of-the-art in two different tasks, fine-grained classification and image retrieval in the Con-Text[23] and Drink Bottle[4] datasets."
9744150,Applications of Computer Vision and Machine Learning in Agriculture: A State-of-the-Art Glimpse,"AI branches many areas including computer vision and machine learning which are growing in a variety of application sectors. In this perspective, the agriculture sector is a promising application space for these two areas. Many efforts have been undertaken to address various agricultural challenges using computer vision and machine learning. Some prominent problem domains are fruit, vegetable, and crop disease diagnosis, recognition of distinct fruits, vegetables, and crops, and quality grading of fruits, vegetables, and crops which we attempt to delineate from state of-the-art perspective."
9716741,A Survey on Vision Transformer,"Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers."
9530146,Leader-Follower System in Convoys:: An Experimental Design Focusing on Computer Vision,"Many modern-day technologies rely on Global Positioning System (GPS) for navigation and location services. Although, in some instances GPS it is not always the most preferred method especially in places where the coverage is lacking, and where privacy is a concern. In the last few years, alternative technologies to GPS systems have been developed and are available not only to enterprises but to consumers as well. This paper presents an idea on how a convoy of vehicles can establish and maintain a leader-follower connection between themselves by utilizing Computer Vision Technology and Received Signal Strength Indicator (RSSI) via wireless low-power networks. The proposed system is to consists of three vehicles, a leader vehicle being controlled by an operator via Bluetooth, and two follower vehicles equipped with DFRobots' Romero V2, BLE Boards, and XBee modules for RSSI navigation. The computer vision system will utilize ZED Stereo Camera for image processing and the Robotic Operating System (ROS). The system will be independent of GPS allowing the operation in environments where GPS is unavailable or unpermitted. The proposed system can be utilized in surveillance missions, rescue missions, and it can serve as a backup to other leader-follower systems."
8813453,Research on Potato Appearance Quality Detection Based on Computer Vision,"Aiming at the appearance quality detection of potato, this paper proposes an image feature detection and recognition algorithm based on computer vision. Firstly, the image of the potato obtained by the computer vision system was analyzed, and the background image of the potato was removed by Otsu method. Then the image processing and recognition were carried out for the green skin, germination and mechanical damage state of the potato. For the green skin state of potato, the normal and green skin areas of potato are discriminated by the similarity of pixel Euclidean distance. For the potato sprouting from the epidermis, the germination area of the potato is detected by the gray difference method of the image G channel. Finally, the edge information and image geometric features are used to determine the damage site of the potato epidermis, and finally the appearance quality of the potato is detected."
8961582,"A Novel Method of Combining Computer Vision, Eye-Tracking, EMG, and IMU to Control Dexterous Prosthetic Hand","Due to poor robustness, instability, and the heavy burden of use, the traditional myoelectric control method is still powerless in the face of the control of the dexterous prosthetic hand. To solve this problem, a new method (CVEEI), that combines computer vision, eye tracking, electromyogram (EMG) and IMU was proposed in this paper. Firstly, through gazing (eye-tracking) in front of the screen, the grasping pattern of the objects can be fed back to the prosthetic hand controller; Then, the prosthetic hand can be controlled to transport the object to the position expected, on collaboration of both EMG and IMU. In this process, the grasping pattern of all objects can be recognized by computer vision in real-time. Importantly, through comparing the traditional EMG control method (co-contraction to switch) in the transport experiment of the objects, the superiority of this new method in operating the dexterous prosthetic hand was further verified (fast > 1s/single object) in this paper."
8711031,Neuromorphic Circuits on FDSOI Technology for Computer Vision Applications,"Potential of neuromorphic circuits on FDSOI technology for computer vision applications is demonstrated in this paper. Computer vision systems based on conventional Von Neumann architecture consume large area and energy. The FDSOI inverter-based circuits proposed in this work, require only 11 transistors per pixel for colour detection, and only 59 transistors per pixel for erosion and dilation operations, whereas the CMOS-based Boolean circuit requires more than 300 transistors per pixel, and 2700 transistors per pixel, respectively, for these operations."
9515664,Do we need expensive equipment to quantify infants’ movement? A cross-validation study between computer vision methods and sensor data,"Recent progress in the study of infant motor development has been achieved by ground-breaking paradigm shifts combined with clever and innovative tasks that place the infant center stage as the acting subject. One of the challenges that developmental scientists are facing today is understanding the complexity of infants’ spontaneous movements. Novel methods such as wearables and computer vision methods have the potential to revolutionize the measurement of infants’ motor behavior in various situational and social contexts. However, a comparison between any computer method and wearables data has not been carried out so far in this age group for spontaneous behavior during social interactions with a caregiver. In this paper, we compare the results of Deeplabcut, an algorithm for tracking user-defined body parts, with simultaneously acquired wearable data and show that computer vision can be a good alternative to advanced wearable systems."
9087125,Generating Graph from 2D Flowchart using Region-Based Segmentation,"Visual content-based information retrieval intended message retrieval from infographics and their applications are gaining attention in many scientific domains. This seeks an understanding of the structure and correct semantics of infographics. Parsing flowchart and analyzing each element of infographics in the correct sequence is challenging. Using computer vision techniques, reconstruction of the structure in digital format is possible and can help to understand the features of the structure. This paper describes technical details to extract various elements of a 2D flowchart in a meaningful sequence, relate elements as connected in a flowchart and represent in conventional data structures. Region-based segmentation with adaptive thresholding and morphological operations are applied for feature extraction. The graph is generated by maintaining the semantics of the flowchart. The experimental results are evaluated for semantic correctness of flowchart and found that 90% of generated graphs are semantically correct as a flowchart."
9441191,Identifying Distracted Students using Computer Vision and Statistical Methods,"Effective interaction between student and teacher makes a class meaningful. While teachers try their best in delivering the lectures, it is the students who need to be checked from time to time. Computer vision and statistical approaches are being rigorously used for the automation of various activities in our day-to-day life. While one helps to process and analyze visual data, the latter helps in arriving at conclusions. Nowadays, it can be easily seen that these technologies are being successfully used in different areas like facial recognition, biometrics, self-driving vehicles, etc. Hence, in this paper, we propose an application, which is a unique combination of computer vision and statistical techniques that can help respective authorities to identify students who are not concentrating in a class, in real-time, or on prerecorded videos."
9422739,Computer Vision Technology Based on Sensor Data and Hybrid Deep Learning for Security Detection of Blast Furnace Bearing,"It is a big challenge to realize accurate security detection of blast furnace bearing at the same time so as to guarantee the security of equipment. To end this problem, this paper proposed a computer vision technology based on sensor data and hybrid deep learning method for the solution. We use Variational Mode Decomposition (VMD) algorithm which is a new time-frequency analysis method, which can decompose multi-component signals into multiple single-component amplitude-modulated signals at one time to decompose and deal with the sensor data of bearing fault, so as to realize the effective stripping of fault components and original components from sensor data. Using the artificial intelligence mentioned above, the features can be quickly and accurately extracted. By combining the advantages of deep learning, we improve the coupling mechanism and implement a hybrid deep learning-based computer vision method which greatly improves the calculation speed and accuracy of bearing fault diagnosis. It can be fully connected with the feature extraction algorithm VMD, which overcomes the problem that the bearing feature component is easy to be submerged and difficult to extract under the condition of high temperature and strong noise. The results show that the optimal selection of parameters of computer vision technology based on sensor data and hybrid deep learning can be realized through training the sensor data obtained from the experiment. The optimized hybrid deep learning-based computer vision algorithm can achieve 97.4% bearing fault diagnosis hit rate, which is an advanced application of deep learning algorithm in the engineering field."
9467225,Banana Ripeness Classification Using Computer Vision-based Mobile Application,"The integration of smartphone applications with the increasingly growing influence of artificial intelligence provides users with new ways to do about anything and allows users to be practical. In this paper, a mobile application to identify the ripeness of banana fruits is built by implementing a computer vision technique. Image classification is performed by adopting transfer learning to extract edges from a pre-trained model. Convolutional neural network (CNN) model is used to train the classifier. Banana is chosen as an instance due to its short shelf life and widely consumed by Malaysian. For this project, Google Colab is utilized for the code execution as it is run on cloud and well-suited for machine learning. TensorFlow Lite with Model Maker library simplified the process of adapting and converting a TensorFlow neuralnetwork model to particular input data before deploying to an Android application. The result emerged with an accuracy of 98.25%. The app can instantly recognize banana live image, display the ripeness level on the screen based on highest percentage matched and display the ripeness, enabling the users to identify the banana ripeness quickly and easily."
9620807,DARS: Data Augmentation using Refined Segmentation on Computer Vision Tasks,"Data augmentation has proven to be effective in improving convolutional neural network performance. As a result, variations of data augmentation methods such as image blending, regional dropout, and copy-paste augmentations has emerged in recent years, enhancing the network's localization and generalization capabilities. However, most data augmentation approach generates unnatural image that contains visible artifacts or strong edges which the network can latch onto. In this paper, we present DARS, a module to refine segmentation in efforts to minimize artifacts for data augmentation. We demonstrated its effectiveness through object classification on Pascal VOC [6] and MS COCO [14] benchmark dataset, with qualitative comparison on refinement from DARS compared to state-of-the-art segmentation models."
9208910,Smart Goal Keeper Prototype using Computer Vision and Raspberry Pi,"This paper presents the prototype of a smart goalkeeper that defends goals. The developed solution is ideal for training football players. Color tracking is used with the synchronization of the servo motors and computer vision to detect football. The mathematical calculations performed are linked with positioning the servo to mark the position for the goalkeeper. The developed solution is perfect for the training of football players while hitting the goals. The robot goalkeeper, through its accurate observations, will be adequate and up to date in putting every effort to save the goal. The limits of the goal are defined concerning the size and movement. The Raspberry Pi 3B+ is used with the webcam for tracking the target, which is the ball in our case."
9022034,Home-Based Physical Therapy with an Interactive Computer Vision System,"In this paper, we present ExerciseCheck. ExerciseCheck is an interactive computer vision system that is sufficiently modular to work with different sources of human pose estimates, i.e., estimates from deep or traditional models that interpret RGB or RGB-D camera input. In a pilot study, we first compare the pose estimates produced by four deep models based on RGB input with those of the MS Kinect based on RGB-D data. The results indicate a performance gap that required us to choose the MS Kinect when we tested ExerciseCheck with Parkinson's disease patients in their homes. ExerciseCheck is capable of customizing exercises, capturing exercise information, evaluating patient performance, providing therapeutic feedback to the patient and the therapist, checking the progress of the user over the course of the physical therapy, and supporting the patient throughout this period. We conclude that ExerciseCheck is a user-friendly computer vision application that can assist patients by providing motivation and guidance to en-sure correct execution of the required exercises. Our re-sults also suggest that while there has been considerable progress in the field of pose estimation using deep learning, current deep learning models are not fully ready to replace RGB-D sensors, especially when the exercises involved are complex, and the patient population being accounted for has to be carefully tracked for its ""active range of motion""."
9551198,Optimal Multidimensional Cyclic Convolution Algorithms For Deep Learning And Computer Vision Applications,"1D, 2D and multidimensional convolutions are basic tools in deep learning, notably in convolutional neural networks (CNNs) and in computer vision (template matching, correlation trackers). Therefore, fast 1D/2D/3D convolution algorithms are essential for advanced machine learning and computer vision. This paper presents: 1) novel optimal n-D cyclic convolution algorithms having minimal multiplicative complexity that are much faster than any competing convolution algorithm internationally and 2) methods for speeding up such optimal convolution algorithms on GPUs and multicore CPUs. Such a speedup is very important both for CNN training and CNN testing, particularly in embedded environments (e.g., on drones) and real-time applications (e.g., fast CNN inference for object detection and correlation trackers for embedded real-time object tracking)."
9028645,Towards Practical Computer Vision in Teaching and Learning of Image Processing Theories,"This research to practice full paper presents a methodology that leads students towards innovative solutions to complex computer vision real problems. The cornerstone of teaching-learning process of image processing theories is based on strong interdisciplinary concepts (e.g. calculus, logic, data structures, perception, graphs, etc). However, just this theoretical background is not sufficient to reach the right practical understanding of how each type of technique interferes in an image considering real problems. To address this issue, we proposed to focus on the application of the image processing theories in a real industrial problem, through a computer vision pipeline. To do so, we used the concept of non-zero-sum game where both players win (e.g. students and industry). To evaluate the effectiveness of our methodology, students answered a questionnaire about their satisfaction. Based on these answers we map how much it helps the undergraduate students to understand the implications of different image processing concepts in a real environment. We also analyzed the satisfaction rate and the students' scores to obtain a well defined analysis. Our methodology presents a strong level of satisfaction according to the students, reaching up to 91% of acceptance rate."
9083950,Subpixel Computer Vision Detection Based on Wavelet Transform,"Computer vision detection technology is one of the most popular topics in the field of computer vision. With the continuous improvement of the relevant algorithm and the performance-price ratio of the corresponding imaging equipment, the corresponding computer vision detection algorithm is also constantly upgraded and deepened. Computer vision detection technology is mainly used in transportation, public security, national defense and military fields, but the pixel accuracy of traditional computer vision detection technology has been unable to meet today's accuracy requirements. In this paper, firstly, the quantum denoising algorithm based on dual-tree and dual-density wavelet transform is used to realize the combination of quantum image coding expression and wavelet transform, and finally achieve a more detailed and accurate description of the image and realize the noise reduction of the image. In order to further realize sub-pixel image processing, cubic spline interpolation edge detection algorithm will be added to wavelet transform, which mainly calculates the zeros of the second-order function corresponding to the cubic spline function on both sides of the image edge points, so as to realize sub-pixel location of the image edge points. Finally, by comparing with the traditional pixel accuracy detection algorithms, it can be found that the proposed sub-pixel computer vision detection algorithm based on wavelet transform has good robustness, and its computing time is relatively faster, so it will have better adaptability in practical applications."
9607741,Simulated Photorealistic Deep Learning Framework and Workflows to Accelerate Computer Vision and Unmanned Aerial Vehicle Research,"Deep learning (DL) is producing state-of-the-art results in a number of unmanned aerial vehicle (UAV) tasks from low level signal processing to object detection, 3D mapping, tracking, fusion, autonomy, control, and beyond. However, barriers exist. For example, most DL algorithms re-quire big data, but supervised ground truth is a bottle-neck, fueling topics like self-supervised learning. While it is well-known that hardware and data augmentation plays a significant role in performance, it is not well understood which data augmentations or what real data need be collected. Furthermore, existing datasets do not have sufficient ground truth nor variety to support adequate con-trolled experimental research into understanding and mitigating limitations in DL algorithms, models, data, and biases. In this article, we address the combination of photo-realistic simulation, open source libraries, and high quality content (models, materials, and environments) to develop workflows to mitigate the above challenges and accelerate DL-enabled computer vision research. Herein, examples are provided relative to data collection, detection, passive ranging, and human-robot teaming. Online video tutorials are also provided at https://github.com/MizzouINDFUL/UEUAVSim."
9388043,The use of technical vision in the robotic cash counting area,"In this research we have installed a computer vision system on a single board computer named ODROID™ XU4. This project is a modification of a more complex and bigger system - the robotic cash counting area. Before our modification a trained neural network was installed on a single board computer, but this caused the computer's CPU (central processing unit) to be overloaded and soft was shutting down. We implemented and compared two methods of object detection in an image and video, namely Shi-Tomasi and FAST (Features from Accelerated Segment) methods. The comparison criteria were accuracy of detection and CPU load."
9590877,Evaluating the application of a computer vision model in the customer service chatbot of an electric utility,"With the increasingly application of Deep Learning models in computer vision, data quality and systematic metric assessment have assumed a preponderant role in the model performance. In this paper we present the evaluation approach of a model built to provide a computer vision solution, aiming to visually identify the energy meter box in households able to be connected to the electricity grid. It is provided a discussion on the real need for a careful analysis of the metrics adopted to evaluate the assertiveness of the results obtained, highlighting the evaluation framework, the importance of image quality and the main issues that must be followed to meet such a purpose. The approach herein described turned out to be on a good methodological path, presenting prediction results able to add value to the chosen application and which can be continuously improved, how is it intrinsic to the machine learning process."
9248564,Robust Iris Recognition Framework Using Computer Vision Algorithms,"Nowadays under human observations iris recognition is playing an essential role, it observed that there are some points are very essential for this method like accuracy, efficiency & processing time. There are several problems has been recorded for this kind of technique. Under previous researches, many concepts have been reported for iris-based segmentation, classification & feature extraction techniques. This research dealt with implementing a novel framework of robust iris segmentation utilizing computer vision technique, FFNN classifier & hybrid feature extraction technique. For the non-iris images method of the novel iris, segmentation has been proposed in this paper. The technique of novel iris segmentation based on 2 methods regards pupil segmentation, after that fusion of enhancing & shrinking visible contour has been framed for segmentation of iris through collaborating novel burden forcing for visible contour model. Moreover, then UN wrapped iris segmentation performed well with the technique of normalization non-circular iris. Whatever, for better iris segmentation research, utilized a proposed method of feature extraction under Discrete Wavelet Transform, letter utilized geometric & texture features of the segmented image. Achieved features have collaborated through a vector of a hybrid feature. For the reason of recognition, we are utilizing a vision algorithm with an FFNN classifier. This paper focused on the concepts related to a robust iris recognition framework using a vision algorithm. Daugman demodulates the yield of the Gabor channels to pack the information. It separates the stage data into four levels and for every conceivable quadrant in-plane to get a minimal 256-byte layout, which takes into consideration proficient capacity and correlation of irises. Experimental outputs have been described as the proposed robust iris segmentation method adopted greater accuracy on the challenging segmentation algorithm thousand databases.
(Show More)"
8981311,Computer Vision Measurement System for Measuring Elasticity Modulus of Straws,"The elasticity modulus of the straw is one of the most important parameters for the adjustment of the reel in the harvester. In order to measure the elasticity modulus of straw rapidly, a computer vision measurement system based on the cantilever beam method has been developed in this research. In the measuring process, the extra weight was suspended on the straw to exert force on the straw. The computer vision measurement system analyzed the image of straw under external force, and worked out the elasticity modulus with the nonlinear deflection differential equation. The computer vision algorithm was implemented using C/C++ with the open source computer vision library (opencv3.1.0) on the platform visual studio 2015. According to the verification experiments, the accuracy of computer vision measurement system was verified. The average error between universal testing machine and computer vision measurement system was 3.735%. The results seems acceptable. What's more, it appears that this system was more efficient than universal testing machine."
8901619,Mobile Robot Position Control Using Computer Vision,"This paper describes computer vision for controlling board cutter mobile robot position in a known environment. This technique develops camera calibration to control the position of the robot that is directed at a certain position in a known wide area. The system calculate x, y, and orientation of mobile robot position from pixel coordinate to world scale coordinate by changing the pixel scale displayed on the monitor screen in millimeters using camera calibration, therefore the actual position in world coordinate could be known. Fixed camera is placed above environment to make it available to view mobile robot clearly while the mobile robot placed in work area. To find out the actual position of the robot in world scale units, it can be done by using focal length as one of the camera's intrinsic parameters. The experiment results show that the best technique is measuring the distance between the camera and the work area as far as 1500 millimeters. The developed method gives a command to the controller by serial communication to set the track length and also the direction of the robot as input by user. The experiment results show that the accuracy is in average 5% - 10%."
8900612,A Computer Vision Perspective on Analyzing and Synthesizing Geospatial Data,"The AI era sustains its foundations from the availability of large datasets. Especially geospatial datasets are very interesting from a computer vision perspective, as they enable us to understand the world we live in. Although many application domains arise from analyzing such big data, analysis itself is not enough for impacting lives. As its counter part, synthesis approaches are recently being developed for mimicking real-world data for completing and creating new worlds. In this paper, we will explore not only example analysis methods developed using large public datasets, but also some generative models to propose realistic and impactful solutions for going beyond observations."
9230742,An Assistance System for Visually Challenged People Based on Computer Vision and IOT,"This work presents a working wearable glass that can simplify some of the basic difficulties of a visually impaired person with the help of computer vision and internet of things. The glass can see the surroundings, hear voice commands, process information and send feedback to the wearer through bone conduction technology without blocking ear hole so that the wearer can be connected with the glass and surrounding world simultaneously. The glass recognizes many common objects and known human faces accurately in real time, which provides the wearer a certain degree of freedom to move alone with less fear in limited environment. The use of scientific inventions and technological advancements in developing such systems will enhance social empathy towards the least observed ones."
8941412,Research on Recognition Method of Common Corn Diseases Based on Computer Vision,"In recent years, due to climate warming, changes in agricultural ecological environment and changes in planting structure, farming system, planting varieties, production methods and production conditions, the living and epidemic conditions of some pests or media insects have also changed, creating an ecological environment suitable for the accumulation of harmful organisms, and the occurrence of corn diseases is increasing year by year. Aiming at common diseases and insect pests, gray spot and brown spot of corn, the recognition model of gray spot and brown spot of corn is established by using computer vision technology and support vector machine, and the generalization ability of the model is tested. The simulation results show that the model can effectively identify gray spot and brown spot of corn, and the correct rate of disease recognition reaches 86.1%, which can be used as a precondition for corn diseases and insect pests, and provides a certain technical reference and has a good practical application value."
9071283,Computer vision based identification of abnormal tissues in biomedical images,"Easy visualization and recognition of slight abnormality in the body of the human can be done through various methods of biomedical imaging. Abnormalities might be due to presence of tumor, also known as the group of abnormal cells that can directly destroy all healthy cells. In case of Brain these abnormal cells grows inside or around the brain. These abnormalities turn destructive and plays a determinative role in the quality of the health of the human and thus increasing the life expectancy and longevity. In early times the diagnosis of the tumors in brain was exhausting task as the symptoms that can be detected physically can only be seen in the advance stages of the tumor. In modern times imaging methods like Magnetic Resonance Imaging (MRI) provides efficient and meticulous insight of tumor condition. It supports the treatment at preliminary stage. In Digital Imaging and Communications in Medicines (DICOM) images, implementation of the image processing techniques help in the detection of the most minute cell with less probability of human error, better speed and high efficiency. Here the identification of abnormal tissue in biomedical images based on computer vision has been used. The features on which the abnormal and the normal images are differentiated are namely area, perimeter and entropy. Entropy has been extracted using the feature extraction methodology from Gray Level Co-occurrence Matrix (GLCM) of the sampled of tumor image."
8862603,Performance Evaluation of Deep Learning frameworks on Computer Vision problems,"Deep Learning (DL) applications have skyrocketed in recent years and are being applied in various domains. There has been a tremendous surge in the development of DL frameworks to make implementation easier. In this paper, we aim to make a comparative study of GPU-accelerated deep learning software frameworks such as Torch and TenserFlow (with Keras API). We attempt to benchmark the performance of these frameworks by implementing three different neural networks, each designed for a popular Computer Vision problem (MNIST, CIFAR10, Fashion MNIST). We performed this experiment on both CPU and GPU(Nvidia GeForce GTX 960M) settings. The performance metrics used here include evaluation time, training time, and accuracy. This paper aims to act as a guide to selecting the most suitable framework for a particular problem. The special interest of the paper is to evaluate the performance lost due to the utility of an API like Keras and a comparative study of the performance over a user-defined neural network and a standard network. Our interest also lies in their performance when subjected to networks of different sizes."
9622654,Automatic Optical Inspection System for wiring harness using Computer Vision,"In any industry visual inspection of finished product is the basic and most important step of quality control department. It is performed at the end when all the manufacturing steps are completed. The inspection process is mostly manually conducted by operators. With trending technology development, complexity and importance of wire harnesses increased along with complex composition of wires. Inspection of such wire is time consuming along with various inspection parameters. This raises load on quality control manpower. Manual process faces considerable limitations which weakens its efficiency that makes it unreliable. Therefore, there arises a great need to automatize this inspection process. The main goal of this thesis is to design an automatic optical inspection system using computer vision for wiring harness. Parameters such as wire length, component, sequence and presence of label is observed for selecting sample as good or bad. For length measurement, sequence detection, component and label detection algorithms are developed based on edge detection, thresholding, segmentation methods based on color etc. Raspberry pi module with raspberry pi camera and light source is used for realization of the visual inspection system. Experiments were performed in laboratory in enclosed hardware setup to achieve results accurately and to achieve stability. Wiring harness is tested for correct and incorrect parameters by the AOI system. Results were reliable and accurate."
8777652,Computer Vision based drowsiness detection for motorized vehicles with Web Push Notifications,"Since the advent of high-speed motorized vehicles drowsy driving accidents have claimed the lives of millions of people across the globe. With more research happening in advanced high-speed engines, the risk of such accidents has increased ten-fold times. To avoid such accidents, this paper proposes a Computer Vision-basedsystem drowsiness system for motorized vehicles with Web Push Notifications to notify the driver before any accident occurs. A real-time video system captures the face of the driver and a pre-trained machine learning model detects the eye boundaries from that real-time video stream. Then each eye is represented by 6 - coordinates (x,y) starting from the left corner of the eye and then working clockwise around the eye. The EAR (Ear Aspect Ratio) is calculated for 20 consecutive frames, which if less than a threshold sounds an alarm and sends an alert on your mobile device through a Web Push Notification. The alert when opened also shows some coffee shops near the driver's location to increase the driver's alertness."
9574762,A Distributed Fall Detection Architecture Using Ensemble Learning,"Falling accidents are serious and costly for people, especially for elders. Each year, millions of older people fall. Particularly, more than one out of four older people falls each year, and falling doubles the chance of falling again. Moreover, without immediate treatment, it would cause higher medical costs. Hence, we intend to design a novel distributed fall detection architecture, where the distributed nature and ensemble learning enhance the classical concept of fall detection based on computer vision. Lastly, in the experiments, we show that the prototype is accurate and stable enough to alert the emergency contact person."
9284118,Interpreting Cloud Computer Vision Pain-Points: A Mining Study of Stack Overflow,"Intelligent services are becoming increasingly more pervasive; application developers want to leverage the latest advances in areas such as computer vision to provide new services and products to users, and large technology firms enable this via RESTful APIs. While such APIs promise an easy-to-integrate on-demand machine intelligence, their current design, documentation and developer interface hides much of the underlying machine learning techniques that power them. Such APIs look and feel like conventional APIs but abstract away data-driven probabilistic behaviour-the implications of a developer treating these APIs in the same way as other, traditional cloud services, such as cloud storage, is of concern. The objective of this study is to determine the various pain-points developers face when implementing systems that rely on the most mature of these intelligent services, specifically those that provide computer vision. We use Stack Overflow to mine indications of the frustrations that developers appear to face when using computer vision services, classifying their questions against two recent classification taxonomies (documentation-related and general questions). We find that, unlike mature fields like mobile development, there is a contrast in the types of questions asked by developers. These indicate a shallow understanding of the underlying technology that empower such systems. We discuss several implications of these findings via the lens of learning taxonomies to suggest how the software engineering community can improve these services and comment on the nature by which developers use them."
9401313,Naturalizing Neuromorphic Vision Event Streams Using Generative Adversarial Networks,"Dynamic vision sensors are able to operate at high temporal resolutions within resource constrained environments, though at the expense of capturing static content. The sparse nature of event streams enables efficient downstream processing tasks as they are suited for power-efficient spiking neural networks. One of the challenges associated with neuromorphic vision is the lack of interpretability of event streams. While most application use-cases do not intend for the event stream to be visually interpreted by anything other than a classification network, there is a lost opportunity to integrating these sensors in spaces that conventional high-speed CMOS sensors cannot go. For example, biologically invasive sensors such as endoscopes must fit within stringent power budgets, which do not allow MHz-speeds of image integration. While dynamic vision sensing can fill this void, the interpretation challenge remains and will degrade confidence in clinical diagnostics. The use of generative adversarial networks presents a possible solution to overcoming and compensating for a vision chip's poor spatial resolution and lack of interpretability. In this paper, we methodically apply the Pix2Pix network to naturalize the event stream from spike-converted CIFAR-10 and Linnaeus 5 datasets. The quality of the network is benchmarked by performing image classification of naturalized event streams, which converges to within 2.81% of equivalent raw images, and an associated improvement over unprocessed event streams by 13.19% for the CIFAR-10 and Linnaeus 5 datasets."
9491783,A Computer Vision Based Food Recognition Approach for Controlling Inflammation to Enhance Quality of Life of Psoriasis Patients,"Deep learning becomes the spotlight in computer vision based recognition approaches in recent years. Psoriasis affects people of all ages around the world and causes inflammation on the skin with significant systemic disability and illness. Inflammatory foods increase inflammation rapidly, patients can easily control inflammation to enhance the quality of life by eliminating these foods from their everyday diet. This paper addresses a rapid food recognition approach to assist psoriasis patients to recognize fifteen highly inflammatory foods. Using image augmentation techniques, a dataset of 41250 images of different inflammatory foods have generated from 10000 images. AlexNet, VGG16, and EfficientNet-B0 have used in this study using the transfer learning approach, and EfficientNet-B0 has achieved the highest accuracy of 98.63% under the test set of 5250 images. AlexNet and VGG16 have achieved 87.22% and 93.79% accuracy, respectively. EfficientNet-B0 has consumed the lowest time in recognizing unseen images compared to others."
9522921,Rethinking the Self-Attention in Vision Transformers,"Self-attention is a corner stone for transformer models. However, our analysis shows that self-attention in vision transformer inference is extremely sparse. When applying a sparsity constraint, our experiments on image (ImageNet- 1K) and video (Kinetics-400) understanding show we can achieve 95% sparsity on the self-attention maps while main-taining the performance drop to be less than 2 points. This motivates us to rethink the role of self-attention in vision transformer models."
9591845,American Sign Language Static Gesture Recognition using Deep Learning and Computer Vision,"Specially-abled people (speech and hearing impaired) rely on hand-gestures for communication on a daily basis. Majority of the people aren’t aware of the universally accepted hand-gestures alphabet, making communication difficult between the two groups of people. In an attempt to fill this void, this research work proposes a real time hand-gesture based recognition system based on the American Sign Language (ASL) dataset and capturing data through a BGR webcam and processing it using Computer Vision (OpenCV). The 29 static gestures (the alphabet) from the official, standard ASL dataset were trained with the help of Vision Transformer Model (ViT). The model showed an accuracy rate of 99.99% after being trained with 87,000 RGB samples for 1 epoch (2719 batches of 32 images each)."
9091752,An Observation of Vision Based Underwater Object Detection and Tracking,"The detection of an object or moving object is a comprehensive goal in the research domain in the era of computer vision. Numerous scientists are quick to think about the underwater world and the living articles in the deep ocean and they are incompletely effective to their objective. In this research work, , we observe the potentiality in underwater by a vision based object detection sphere by applying various previous work for highlighting the arising consequences in various underwater situation.. We have collected the dataset from various source of water such as clean water, turbid water, bubbles water etc. We used OpenCV and Python programming language to detect the object. Finally the detection of object in underwater is studied for connecting with the basic color based HSV(Hue, Saturation, Value) segmentation."
8823097,Detection and Classification Defects on Exported Banana Leaves by Computer Vision,"Some effective techniques for identifying defects and estimating defect areas are the main requirements for computer vision and visual processing. This study provides an image processing algorithm to identify and calculate areas of defects on banana leaves. The algorithm consists of the main steps of processing images, segmenting images, labeling, size filtering, determining the boundaries for candidate areas such as chalks, spider webs, pus banana, soils, torn leave. Extracting colour characteristics to identify defects and estimate ultimately areas. Defect of banana has a lot of leak so we use number of defects to classify in good or reject leaf. Extracting boundary features and estimating boundary lengths to determine torn leaves. Experiments were conducted on 200 leaves to be identified. The accuracy of the proposed method is 89.8% for the method of color identification of disability defects and 94.7% for the method of identifying torn leaves."
9417993,Computer Vision Techniques for Crowd Density and Motion Direction Analysis,"A system using computer vision techniques for tracking and providing early information of hazardous locations in huge gatherings is the need of the hour. Also, the number of video streams generated are huge and are challenging to watch them personally. The proposed system is based on the Optical Flow based estimations and detects sequences of crowd motions that are characteristic for devastating congestions. Initially, the temporal features of the scenes are extracted using Motion History Image (MHI) technique. MHI technique involves the weighted subtraction of consecutive frames of the video stream. Then the Optical Flow vectors are calculated using the Lucas-Kanade method. Segmentation of Optical Flow fields is done, and hence directional histograms of motion magnitude against motion direction are determined for respective segments. Their entropy and absolute bin magnitudes characterize the graphs. Thresholds are chosen such that there is a demarcation between sparsely crowded and densely crowded segments in the frames. Localization of crowd density levels as unpopulated, sparse and dense helps in providing immediate attention to critical areas of congestion. The directional analysis helps in having knowledge of the significant direction of motion in sparsely crowded regions."
9204339,Comparative Analysis of Classic Computer Vision Methods and Deep Convolutional Neural Networks for Floor Segmentation,"In the paper, we analyze the problem of automatic room floor segmentation. For this purpose, we consider several classic computer vision algorithms as well as some of the deep convolutional neural network architectures. The segmentation results are illustrated and compared. An idea for combining two groups of methods is proposed. It is demonstrated that a proper fusion provides the best segmentation quality."
9084913,Design of Moving Object Detection Algorithm Based on Computer Vision,"Image recognition technology of computer vision is one of the hotspots in the research field. The target detection and recognition technology is becoming mature. On the basis of this technology, a moving target capture algorithm with simple and fast operation, resource saving and high potential value is designed. Real-time monitoring is carried out for the object tracking of shell trail. A moving object detection algorithm based on static background was designed and the definition of target image was optimized. Because of the moving target image is not clear, wavelet transform was used to remove noise and wiener filter restoration method was used to remove fuzzy shadows to make the image clear. An appropriate threshold value was selected by image threshold segmentation, and the target was extracted from the background image by differential image binarization. Finally, morphological corrosion and Hough transform are used to calculate the location of moving targets, so as to detect the target of moving shells in the environment, and capture the track of moving target on this basis, which provide a basis for the next step of visual servo control."
9602931,Investigation on Error-Tolerability Enhancement of Videos via Re-Encoding for Computer Vision: A Case Study on Object Detection,"Run-time video re-encoding via adjusting the Group of Pictures (GOP) size has been shown to be able to effectively enhance the error-tolerability of videos. However, this study was done by focusing on the human visual system’s perception to video errors. In this paper we investigate on this issue by taking machine’s tolerability of errors into consideration. Objection detection carried out by yolov3 is employed as a case study. In this study, errors are injected into a benchmark video to generate more than 100,000 erroneous videos, and then yolov3 is employed to evaluate the quality (acceptability) of these videos. The results demonstrate that video re-encoding can have higher application efficiency (e.g., larger GOP size) for computer vision, mainly because that the error-tolerability of machines is larger than that of humans. This can lead to better video encoding efficiency (smaller video size) and lower implementation cost for re-encoding."
9537452,Intel OpenVINO Toolkit for Computer Vision: Object Detection and Semantic Segmentation,"The paper provides an overview of the neural networks implementation current state, their methods of execution, and the Intel® OpenVINO ™ Toolkit for executing neural networks on various hardware platforms from Intel. This work describes the selection of computer vision neural networks and datasets for object detection and semantic segmentation of images for subsequent testing. It gives the description of the experiment on various hardware platforms. Moreover, it provides an analysis of the performance and cost of running selected neural networks using the OpenVINO ™ Toolkit in normal mode, as well as using plug-ins for multiple devices and heterogeneous plug-ins on multiple connected devices."
9396600,Computer Vision Algorithms for Recognizing Basic Primitives of Objects Based on Digital Filters,"The implementation of algorithms for primary image processing using one-dimensional digital filters adapted for DSP is considered. Methods of replacing a high bit operands with a lower one are used to increase performance, index arithmetic and methods using identifiers of graphic objects, replacing nonlinear operations with binary ones are presented. Algorithms for the detection of lines from the image, paths based on lines that form open or closed contours have been developed. The efficiency is shown when implemented on general-purpose processors."
9684616,Range Measurement by Computer Vision Systems Based on Invariant Moments,"We review range measurement methods based on various configurations of computer vision sensors. We discuss the mathematical model of a stereoscopic pair and the conditionality of its fundamental matrix, which is essential for correct recognition and range measurement. A lot of researchers study the measurement based on single video or photo cameras, they are often installed on small mobile flying vehicles. We focus on the methods based on invariant moments. It is demonstrated that range measurement can be performed by a single camera. A complex of experiments was conducted in order to study single camera range measurement based on invariant moments. The deviation of experimental values of distance measurement that we have done didn't exceed 9%. The error can be explained by the inaccuracy in measuring the distance from the camera lens to the object, unstable position of the camera, and low quality of the obtained image due to the camera restrictions."
8991808,Study on the Computer Vision of the Biped Robot for Stable Walking on the Stairs,"Because of the social change and the advanced technologies, artificial intelligence technology has been developed rapidly. Robots are entering the human's living life soon. Biped robots are expected to be employed in some dangerous places. This study presented that the biped robot moved up and down stairs through computer vision. The processes included 1) Gray level co-occurrence matrix detected to move up or down; 2) Canny edge detection operator is adopted for the clear stair edge images; 3) Compute the distance between the stairs and the camera using the Zhang's calibration."
9307437,Benchmarking of Computer Vision Algorithms for Driver Monitoring on Automotive-grade Devices,"The continuing evolution of technologies in the automotive industry has led to the development of the so-called Advanced Driver Assistance Systems (ADAS). ADAS is the term used to describe vehicle-based intelligent safety systems designed to support the driver, with the aim to significantly improve his safety, and the driving safety in general. In terms of development, current ADAS technologies are based on control functions about the vehicle movements with respect to the objects and entities detected in the same environment (e.g., other vehicles, pedestrian, roads, etc.). However, there is an ever growing interest on the use of internal cameras to infer additional information regarding the driver status (e.g., weakness, level of attention). The purpose of such technologies is to provide accurate details about the environment in order to increase safety and smart driving. In the last few years, Computer Vision technology has achieved impressive results on several tasks related to recognition and detection of customized objects/entities on images and videos. However, automotive-grade devices' hardware resources are limited, with respect to the once usually required for the implementation of modern Computer Vision algorithms. In this work, we present a benchmarking evaluation of a standard Computer Vision algorithm for the driver behaviour monitoring through face detection and analysis, comparing the performances obtained on a common laptop with the same experiments on an existing commercial automotive-grade device based on the Accordo5 processor by STMicroelectronics."
8708282,Efficient CIEDE2000-Based Color Similarity Decision for Computer Vision,"Color and color differences are critical aspects in many image processing and computer vision applications. A paradigmatic example is object segmentation, where color distances can greatly influence the performance of the algorithms. Metrics for color difference have been proposed in the literature, including the definition of standards such as CIEDE2000, which quantifies the change in visual perception of two given colors. This standard has been recommended for industrial computer vision applications, but the benefits of its application have been impaired by the complexity of the formula. This paper proposes a new strategy that improves the usability of the CIEDE2000 metric when a maximum acceptable distance can be imposed. We argue that, for applications where a maximum value, above which colors are considered to be different, can be established, then it is possible to reduce the amount of calculations of the metric, by preemptively analyzing the color features. This methodology encompasses the benefits of the metric while overcoming its computational limitations, thus broadening the range of applications of CIEDE2000 in both the computer vision algorithms and computational resource requirements."
9528235,A Review of Groundbreaking Changes in the Poultry Industry in Bangladesh Using the Internet of Things (IoT) and Computer Vision Technology,"As the number of people increases daily, the requirement for animal meat is also growing in the country. Therefore, the use of technology in the poultry industry is essential to meet all these needs adequately. IoT & Computer Vision Technology is playing a crucial role in the transformation of the poultry industry. IoT & Computer Vision Technology is advancing so much day by day that it will turn the poultry industry into a groundbreaking poultry industry. The primary function of a poultry farm is to produce meat and eggs. At present, the cost of meat and egg production in our country needs to be further reduced. Proper maintenance of poultry boilers increases meat and egg production. Still, poultry farmers in our country do not have a clear idea about smart poultry farms, so they cannot use innovative technology properly, but developed countries use smart technology. In addition to making more profit at a lower cost, they can meet the demand for animal meat of the people in their country. This paper’s main objective is to monitor poultry farms using IoT and computer vision technology and be aware of the poultry farm environment, which will help in transforming the poultry industry into a groundbreaking poultry industry."
9574047,Visual Recognition of Local Kashmiri Objects with Limited Image Data using Transfer Learning,"Learning to recognize object categories is a challenging task for computers and the task becomes more difficult if the image data is small in size. Traditional machine learning methods require extensive training data to generalize and produce accurate results. Seeking inspiration from human perception, it has been found using prior knowledge about related tasks helps in learning new tasks. Transfer Learning is based on this natural learning process and can help to reproduce the remarkable human capability of recognizing objects from just one single view. In this manuscript we explored transfer learning techniques along with state-of-the-art object recognition models to discover improved ways of performing Visual Object Recognition for Object categories with limited image data. A small data set was built from scratch having images for four local object categories and then a model was developed using pre trained Inception-v3 model for classifying them. The results were compared with stock 3 Layer Convolutional model. The proposed model obtained a respectable accuracy result of around 90% while the stock model had an accuracy of 70%. Considering the fact that the dataset used is very tiny (training portion of the data set had only 320 images for all categories, 80 per category) the results obtained are encouraging. Thus, this work further strengthens the fact that transfer-based techniques can be utilized for computer vision tasks with limited data."
9527586,An Improved Method of CRIMINISI Algorithm,"Nowadays, the research on image inpainting technology is a research topic of great significance in computer vision and graphics. As for the missing part of the image, humans can only analyze the remaining areas based on their own visual senses and experience, and then fill in the remaining missing areas because they do not know the original appearance of the image. In general, image restoration is a technology based on human vision and experience. In 2003, Criminisi et al. proposed a patching algorithm based on sample blocks. The algorithm gives priority to pixels based on the confidence level and adopts the best matching module method. This article provides a method to improve its confidence, changing its factor multiplication to linear addition and adding an additional judgment factor, which improves the effect of the inpainting."
8848933,Late Breaking Report - Development of a mobile robot for industrial plants inspections using computer vision,"Industrial accidents are a main issue for companies with capital intensive infrastructure; one way to reduce this is avoiding the employees exposure to hazardous environments that affect their physical integrity and health. Inspection of gauges and manipulation of valves are important tasks that are typically performed in all industries, these industrial components are located in hard to access and dangerous places, for this reason it is necessary a system that can achieve this task without direct human intervention. In this report, the preliminary results of a teleoperated robotic system with computer vision capabilities is presented. In addition, a vision algorithm for analog gauge measurements was tested in different gauge models and the results show that a 1% error was achieved in less than 1 second."
9394803,CIE XYZ Net: Unprocessing Images for Low-Level Computer Vision Tasks,"Cameras currently allow access to two image states: (i) a minimally processed linear raw-RGB image state or (ii) a highly-processed nonlinear image state (i.e., sRGB). There are many computer vision tasks that work best with a linear image state. A number of methods have been proposed to ""unprocess'' nonlinear images back to a raw-RGB state. However, existing methods have a drawback because raw-RGB images are sensor-specific. As a result, it is necessary to know which camera produced the sRGB output and use a method or network tailored for that sensor to properly unprocess it. This paper addresses this limitation by exploiting another camera image state that is not available as an output, but it is available inside the camera pipeline. In particular, cameras apply a colorimetric conversion step to convert the raw-RGB image to a device-independent space based on the CIE XYZ color space before they apply the nonlinear photo-finishing. Leveraging this canonical state, we propose a deep learning framework that can unprocess a nonlinear image back to the canonical CIE XYZ image. This image can then be processed by any low-level computer vision operator. We demonstrate the usefulness of our framework on several vision tasks and show significant improvements."
9457921,A Computer Vision Framework for Quantification of Feather Growth Patterns,"This study aims to examine phenotypic variations in pigeon feather growth patterns by rendering computed tomography (CT) scans as point clouds, and developing machine learning based, feature extraction techniques to isolate the feathers, and map the growth patterns on the skin."
9526948,A Frame-Based Feature Model for Violence Detection from Surveillance Cameras Using ConvLSTM Network,"Vision-based action detection is a puzzling research topic of computer vision and pattern recognition. Violence detection in video has found a particularly fundamental application in real-life scenes, aiming at monitoring and protecting the safety of people and pedestrians. In previous years, violence detection in surveillance cameras gained more and more attention from researchers, due to a huge amount of video data collected by Closed-circuit television (CCTV) cameras that covers different places such as schools, hospitals, shopping malls, streets, etc… However, the manual control and detection of violent behaviors and report of those incidents on time based on videos is very tiresome and laborious for operators which may results in the loss of lives. With the rise of deep learning and its applications in different domains of human daily lives, automatic detection models for abnormal behaviors or violence incidents are highly needed for this concern. In this paper, we address this research problem and explore ConvLSTM network as a solution. We conducted extensive experiments on six benchmark datasets, where our method shows superior performance on all these datasets by improving the state-of-the-art performance accuracy for violence detection."
9322655,HHARNet: Taking inspiration from Inception and Dense Networks for Human Activity Recognition using Inertial Sensors,"Human Activity Recognition (HAR) is an important area of research in the light of enormous applications that it provides, such as health monitoring, sports, entertainment, efficient human-computer interface, child care, education, and many more. The use of Computer Vision for Human Activity Recognition has many limitations. The use of inertial sensors which include an accelerometer and gyroscopic sensors for HAR is becoming the norm these days considering their benefits over traditional Computer Vision techniques. In this paper, we have proposed a l-dimensional Convolutions Neural Network which is inspired by two state-of-the-art architectures proposed for image classifications; namely Inception Net and Dense Net. We have evaluated its performance on two different publicly available datasets for HAR. Precision, Recall, Fl-measure, and accuracies are reported."
9148165,Research on Gesture Recognition Method Based on Computer Vision Technology,"From the directions of the framework model of gesture recognition system, gesture segmentation, gesture modeling and analysis, and gesture recognition, we systematically summarize the current research status of dynamic vision recognition technology in computer vision, analyze its shortcomings. The results show that gesture recognition based on simple wearable devices, gesture recognition based on deep vision sensors, and multi-method cross-fusion gesture recognition will be the development trends in this field in the future."
9447516,Considering Filter Importance and Irreplaceability In Filter Pruning,"Deep convolutional neural network (CNNs) have gained a great success in computer vision tasks. However, the computation and parameter storage costs of CNNs are very large, thus a large number of studies have tried to reduce the computation and parameters of CNNs. Quantization and pruning are the usual strategies for model compression. Previous filter pruning works usually prune filters with small norm because the common viewpoint is that the smaller norm of filters, the less contribution of filters. However, above strategy ignores that features extracted by those filters with large norm may be redundancy and the features extracted by small norm filters are irreplaceable. In this paper, in light of above findings, we propose a novel filter pruning method called Importance Diversity Filter Pruning (IDFP), which considers both filter contribution and filter irreplaceability. We conduct extensive experiments on CIFAR-10 and CIFAR-100 datasets. The results illustrate the effectiveness of our method."
9525592,Sensitivity of camera’s height based on coordinate variation of monocular vision ranging model,"Using computer vision technology to perceive the distance of obstacles is an important technology in intelligent driving decision-making. Monocular vision ranging technology has a wide application range and reduces the requirement of camera quality, so it is a hot research direction in the field of visual ranging. In practical application, the change of the camera’s external parameters will have a certain impact on the ranging accuracy. This paper will study the sensitivity of the monocular vision ranging model based on the coordinate change to the camera height. We propose a conjecture that the product of the model’s formula accuracy and the height measurement accuracy is equal to the final ranging accuracy of the model. This conjecture will be preliminarily verified through experiments in this paper, and an error correction mechanism is proposed to reduce the impact of camera height changes on ranging accuracy."
9208943,"A Comparative Approach between Different Computer Vision Tools, Including Commercial and Open-source, for Improving Cultural Image Access and Analysis","Digital cultural heritage objects can benefit greatly from the application of Artificial Intelligence such as computer vision based tools to automatically extract valuable information from them. Novel methods and technologies have been used in the last few years to perform image classification, object detection, caption generation, and other techniques on different types of digital objects from different disciplines. In this pilot study, carried out in the context of the Digital Humanities project ChIA, we present an approach for testing different commercial (Clarifai, IBM Watson, Microsoft Cognitive Services, Google Cloud Vision) and open-source (YOLO) computer vision (CV) tools on a set of selected cultural food images from the Europeana collection with regard to producing relevant concepts. The project generally aims at improving access to implicit cultural knowledge contained in images, and increase analysis possibilities for scientific research as well as for content providers and educational purposes. Preliminary results showed that not only quantitative output results are important, but also the quality of concepts generated. Types of digital objects can pose a challenge to CV solutions."
9114610,Content Based Image Retrieval - Inspired by Computer Vision & Deep Learning Techniques,"With the growing popularity of digital services and internet technology billions of people are prone to information sharing and uploading photos. For accurate retrieval of images from huge digital image databases, Content Based Image Retrieval (CBIR) method are emerging as an influential next generation tools, with wide range of applications in fields like criminal investigation, shape recognition, medical diagnosis, remote sensing, digital forensic, radar engineering and robotics. The key challenge of CBIR systems lies in building the “semantic gap” that exists between the differences in the way of perceiving things from basic to complex image features. The intention behind our paper is to provide deeper analysis and impact of continuous advancements in Image Retrieval techniques since its origin. An empirical examination of most popular and upcoming techniques in this survey paved the way in selecting the suitable computer vision and Deep Learning Techniques to improve the performances of retrieval systems. Several Benchmark datasets are considered to validate and study the robustness of the techniques influencing feature extraction and classification."
9294901,A low-cost Raspberry PI-based vision system for upper-limb prosthetics,"To achieve human like stable grasp with prosthetic, the correct grasp type along with the appropriate grasp aperture need to be identified from the targeted object. In most of the advance myo-electric prosthetics, the problem has been solved by using computer vision system in addition with the prosthetics. The vision systems are placed over the prosthetic itself and it needs to move towards the target object. Upon reaching the target object, the system capture the object image and process it to generate the correct grasp type. The processing time delays the initiation of the grasp process. To reduce this time delay and make the grasp instant like normal human hand, we have developed a vision system with Raspberry PI along with the PI camera. The system moves toward the targeted object and captures images of the focused region at two intermediate positions. OpenCV DNN model was used to identify all the objects within the captured frame. The employed model provided the efficiency of deep learning models in object identification without using of any GPU. The width of the objects in terms of pixels were also estimated from the images. The distance travelled by the system was measured by the SensorHAT (integrated with the system) which recorded the acceleration and direction of the hand movement. The device was tested in the cluttered environment in recognising four different grasp types- Compliant, Lateral, Tripod closed and Wrist-flexion aided grasp."
9607843,Moving Object Detection for Event-based Vision using Graph Spectral Clustering,"Moving object detection has been a central topic of discussion in computer vision for its wide range of applications like in self-driving cars, video surveillance, security, and enforcement. Neuromorphic Vision Sensors (NVS) are bio-inspired sensors that mimic the working of the human eye. Unlike conventional frame-based cameras, these sensors capture a stream of asynchronous ‘events’ that pose multiple advantages over the former, like high dynamic range, low latency, low power consumption, and reduced motion blur. However, these advantages come at a high cost, as the event camera data typically contains more noise and has low resolution. Moreover, as event-based cameras can only capture the relative changes in brightness of a scene, event data do not contain usual visual information (like texture and color) as available in video data from normal cameras. So, moving object detection in event-based cameras becomes an extremely challenging task. In this paper, we present an unsupervised Graph Spectral Clustering technique for Moving Object Detection in Event-based data (GSCEventMOD). We additionally show how the optimum number of moving objects can be automatically determined. Experimental comparisons on publicly available datasets show that the proposed GSCEventMOD algorithm outperforms a number of state-of-the-art techniques by a maximum margin of 30%."
9544941,A Schematic Review on Applications of Deep Learning and Computer Vision,"Artificial intelligence(AI) and Deep learning (DL) become supreme problem-solving strategies in many areas of research and industrial applications.[1]. Deep learning is one of the most efficient, accurate and cost efficient algorithms. Applications of Deep learning via computer vision in Bio medical and healthcare, security, education and latest trends in technologies are stated here. This paper focuses on giving an overview of work done by many authors on Deep learning and computer vision applications and algorithms. At last common findings from the papers are reviewed."
9221198,A Survey of Methods for Low-Power Deep Learning and Computer Vision,"Deep neural networks (DNNs) are successful in many computer vision tasks. However, the most accurate DNNs require millions of parameters and operations, making them energy, computation and memory intensive. This impedes the deployment of large DNNs in low-power devices with limited compute resources. Recent research improves DNN models by reducing the memory requirement, energy consumption, and number of operations without significantly decreasing the accuracy. This paper surveys the progress of low-power deep learning and computer vision, specifically in regards to inference, and discusses the methods for compacting and accelerating DNN models. The techniques can be divided into four major categories: (1) parameter quantization and pruning, (2) compressed convolutional filters and matrix factorization, (3) network architecture search, and (4) knowledge distillation. We analyze the accuracy, advantages, disadvantages, and potential solutions to the problems with the techniques in each category. We also discuss new evaluation metrics as a guideline for future research."
8946358,Image Captioning in Turkish Language,"Image captioning is one of the everlasting challenging tasks in the field of artificial intelligence that requires computer vision and natural language processing. Plenty of salient works have been proposed throughout the time for English language however, the number of studies in Turkish language is still too limited. This paper couples an encoder CNN -the component that is responsible for extracting the features of the given images-, with a decoder RNN -the component that is responsible for generating captions using the given inputs-to generate Turkish captions within human gold-standards. We conducted the experiments using the most common evaluation metrics such as BLEU, METEOR, ROUGE and CIDEr. Results show that the performance of the proposed model is satisfactory in both qualitative and quantitatively evaluations. A Web App is already deployed to allow volunteers to contribute to the improvements of the Turkish captioned dataset."
9711550,A Simplified Traffic Flow Monitoring System Using Computer Vision Techniques,"Millions of Muslims from all over the world visit the holy city of Makkah to perform Umrah throughout the year, but especially in the month of Ramadan the quantity of pilgrims increases significantly. Same conditions can be observed in the month of Dhu al-Hijjah during the Hajj season. To transfer the pilgrims from one place to another various many vehicles are used. It is very important to monitor the traffic flow during these days to avoid any congestion which may lead to any mishap. Therefore, there is a need for a traffic flow monitoring system that can be used to count vehicles, and classify them according to their size to facilitate traffic management. A computer vision based system is proposed in this research that uses MATLAB vision tool to identify and count the vehicles. The novelty of the system is its simplicity that uses a camera as a video source, then suitable algorithms are implemented on it for processing. A portion of a video is used for system training that first obtains a binary foreground from the initial frame, then removes noise and then counts the vehicles present in the frame. The functionality of the proposed system has been verified by a number of experiments with the accuracy of 90%. It can be easily implemented in various environments for vehicles counting that can be used for better traffic management."
9644513,Intelligent research method of traditional Chinese medicine based on computer vision thinking,"Modern TCM diagnostic technology is guided by the four-diagnosis principles of TCM, with the direction of dataization, objectification, and intelligence, using image acquisition and processing technology, spectral analysis technology, machine learning technology, etc. to collect, analyze and process human visual information. In addition to objectification and standardization, the application of computer for clinical auxiliary diagnosis and treatment also has certain advantages in actual operation, and has certain feasibility and practical application value. The use of computer vision and other modern technologies to quantify and scientificize traditional Chinese medicine diagnostic methods can reduce human interference factors to a certain extent, and allow traditional Chinese medicine to emit a new light under the blessing of modern technology."
8960011,Development of Hardware Setup of an Autonomous Robotic Vehicle Based on Computer Vision Using Raspberry Pi,"In this paper, the hardware setup of an autonomous robotic vehicle is developed. The controller of the vehicle is developed based on the computer vision technology which became more smart due to the application of popular tool open CV. An algorithm based on color detection is developed to navigate the vehicle in different directions. Raspberry Pi camera is used to access images which are the indications of the directions of the movements such as stop, left turn, right turn. The efficacy of the proposed algorithm is verified experimentally."
9162101,X-Ray Baggage Inspection With Computer Vision: A Survey,"In the last decades, baggage inspection based on X-ray imaging has been established to protect environments in which access control is of vital significance. In several public entrances, like airports, government buildings, stadiums and large event venues, security checks are carried out on all baggage to detect suspicious objects (e.g., handguns and explosives). Although improvements in X-ray technology and computer vision have made many X-ray detection tasks that were previously unfeasible a reality, the progress that has been made in automated baggage inspection is very limited compared to what is needed. For this reason, X-ray screening systems are usually being manipulated by human inspectors. Research and development experts who focus on X-ray testing are moving towards new approaches that can be used to aid human operators. This paper reports the state of the art in baggage inspection identifying three research fields that have been used to deal with this problem: i) X-ray energies, because there is enough research evidence to show that multi-energy X-ray testing must be used when the material characterization is required; ii) X-ray multi-views, because they can be an effective option for examining complex objects where the uncertainty of only one view can lead to misinterpretation; and iii) X-ray computer vision algorithms, because there are a plethora of computer vision approaches that can address many 3D object recognition problems. Besides, this paper presents useful public datasets that can be used for training and testing, and also summarizes the reported experimental results in this field. Finally, this paper addresses the general limitations and show new avenues for future research."
9108699,CVNodes: A Visual Programming Paradigm for Developing Computer Vision Algorithms,"Advances in machine learning have led to a rapid pace of innovation in Computer vision and deep learning classification algorithms. Deep learning classification models are often limited in flexibility due to their fixed pre-processing steps embedded into the algorithm and lack ways to easily iterate, debug, and analyze developed algorithms without programming knowledge. The lack of high-level tools for developing vision algorithms leads to longer development times that require significant knowledge of underlying algorithms. What about individuals without this deep knowledge of machine learning and vision yet wish to develop algorithms to prototype ideas? What about nonprogrammers such as designers and artists that wish to utilize the state-of-the-art in computer vision in their work? To address this under-served community, we propose a visual-programming solution akin to those found in modern game engines geared towards computer vision algorithm development. These results in a new prototyping tool to empower researchers and nonprogrammers to easily iterate algorithm development, use pretrained classification models, and provide statistical post-analysis tools."
9295947,A Comparative Research on the Influence of Commercial Complex Waterscape Atrium on Human Emotion Based on Computer Visual Attention and EEG Data,"With the rapid development of computer physiological measurement technology, more and more computer technology has been applied to the design. In this paper, a wearable EEG acquisition system based on computer vision measurement was used to test the concentration and meditation degree of computer image vision and the proportion of dominant brain waves in commercial complex with and without waterscape atrium. By comparing the average value index of concentration, meditation degree, mind α wave and β wave as dominant brain wave, the main factors influencing human emotion change were analyzed, and the influence law of atrium space with waterscape and atrium without waterscape on human emotion change was summarized from the perspective of physiological measurement. The results showed that the time and effect of mind α wave on human brain were significantly increased (P <; 0.01), while the time and effect of β wave on human brain were weakened. The average value of concentration had an increasing trend, but the total value did not increase significantly, and the average value of meditation degree showed a significant increase (P <; 0.01), indicating that compared with the atrium without waterscape, the waterscape atrium can make people relax and calm without weakening their mental attention."
9590812,A Vision Transformer with Improved LeFF and Vision Combinative Self-attention Mechanism for Waste Image Classification,"At present, many challenges exist in the application of waste image classification including high image resolution, background noise interference and low computational efficiency, which makes a great impact on the accuracy of the algorithm. Generally, convolutional neural network related strategy is adopted, which can improve accuracy to a certain extent. But it is prone to getting interfered by irrelevant information and has large size of parameters, which makes it difficult to be deployed for those resource-limited applications. Thus, a Vision Transformer is proposed. By using Vision Transformer, it significantly improves the anti-interference ability and reduces parameter scale. Furthermore, distillation strategy is adopted to improve the generalization ability of the model. In addition, this paper replaces multilayer perceptron (MLP) layer in base Vision Transformer with improved Locally-enhanced Feed-Forward (improved LeFF) to improve the convergence speed and accuracy of the model. This paper also proposes Vision combinative self-attention algorithm to reduce the requirements of computational resources especially for image with high resolution. The results show that in 79002 waste image datasets with 198 types, the accuracy of training set and test set can reach 99.98% and 95.03% respectively. Meanwhile, a relatively high accuracy rate could be achieved within less training epoch and less computational resources are required during the operation."
9568372,DOHMO: Embedded Computer Vision in Co-Housing Scenarios,"This paper presents DOHMO, an embedded computer vision system where multiple sensors, including intelligent cameras, are connected to actuators that regulate illumination and doors. The system aims at assisting elderly and impaired people in co-housing scenarios, in accordance with privacy design principles. The paper provides details of two core elements of the system: The first one is the BOX-IO controller, a fully scalable and customizable hardware and software IoT ecosystem that can collect, control, and monitor data, operational flows and business scenarios, whether indoor or outdoor. The second one is the embedded 3DEverywhere intelligent camera, a device composed of an embedded system that receives input data provided by a 3D/2D camera, analyzes it, and returns the metadata of this analysis. We illustrate how they can be connected and how simple decision mechanisms can be implemented in such a framework. In particular, illumination can be triggered on and off by the detected presence of people, overcoming the limitations of typical sensors, while doors can be opened or closed based on person trajectories in an intelligent manner. To substantiate the proposed system, numerous experiments are performed in a lab and a co-housina scenario."
9289158,A Light-weight stereo matching network for an embedded vision system,"In the embedded computer vision field, the stereo vision matching using two cameras can identify the distance of the surrounding environment at low prices. Recently, CNN based artificial intelligence algorithms have been showing good results with high accuracy. However, huge network size and slow computation prevent them used in embedded edge systems. In this paper, a CNN algorithm to use reduced parameters to find a accurate disparity map of stereoscopic images based on Siamese network is proposed. The proposed method performs network training through GPU-based parallel operation and produces better results than the existing one using fewer parameters."
8658965,Learning From Less Data: A Unified Data Subset Selection and Active Learning Framework for Computer Vision,"Supervised machine learning based state-of-the-art computer vision techniques are in general data hungry. Their data curation poses the challenges of expensive human labeling, inadequate computing resources and larger experiment turn around times. Training data subset selection and active learning techniques have been proposed as possible solutions to these challenges. A special class of subset selection functions naturally model notions of diversity, coverage and representation and can be used to eliminate redundancy thus lending themselves well for training data subset selection. They can also help improve the efficiency of active learning in further reducing human labeling efforts by selecting a subset of the examples obtained using the conventional uncertainty sampling based techniques. In this work, we empirically demonstrate the effectiveness of two diversity models, namely the Facility-Location and Dispersion models for training-data subset selection and reducing labeling effort. We demonstrate this across the board for a variety of computer vision tasks including Gender Recognition, Face Recognition, Scene Recognition, Object Detection and Object Recognition. Our results show that diversity based subset selection done in the right way can increase the accuracy by upto 5 - 10% over existing baselines, particularly in settings in which less training data is available. This allows the training of complex machine learning models like Convolutional Neural Networks with much less training data and labeling costs while incurring minimal performance loss."
9670066,Image Features Selection Based on Computer Vision Techniques to Detect Potholes for Intelligent Transport System,"Computer vision is an emerging area for image processing that is well utilized in the field of Intelligent Transport Systems. There are various techniques used to increase the efficiency of ITS such as GPS, radar-based, sensors utilization, more importantly, through Surveillance, etc. It found that costs for all techniques were high to implement on a large scale. The need of the hour is to have efficient techniques or models that will help to reduce the cost of ITS. Computer vision is a field that provides cutting-edge solutions in ITS. This paper explores the image features selection based on computer vision techniques to detect the potholes on the road. It will help to improve the Intelligent Transport System for a smart city. In this paper, the authors will discuss the computer vision techniques like Corner Detection-Harris, HOG, and Feature Extraction through FAST."
9184433,Single Photon Avalanche Diode based Vision Sensor with On-Chip Memristive Spiking Neuromorphic Processing,"In this paper, we present a novel single photon avalanche diode (SPAD) based vision sensor with on-chip memristive spiking neuromorphic processing. For this prototype design, a 5 × 5 SPAD based pixel array with asynchronous address event representation (AER) readout has been implemented in a 65 nm CMOS process. The proposed system also consists of a memristive spiking neuromorphic system to process the events in a more area and power efficient bio-inspired way. The sensor exhibits array-level dynamic range of 152 dB with power consumption of 2.1 mW. The complete event based vision sensing system is tested using a small scale shape recognition task. Results show great potential for the proposed memristive spiking neuromorphic SPAD based vision sensing system for robotic vision used in health, autonomous vehicles, and security applications."
9260152,Combination of Augmented Reality Based Brain- Computer Interface and Computer Vision for High-Level Control of a Robotic Arm,"Recent advances in robotics, neuroscience, and signal processing make it possible to operate a robot through electroencephalography (EEG)-based brain-computer interface (BCI). Although some successful attempts have been made in recent years, the practicality of the entire system still has much room for improvement. The present study designed and realized a robotic arm control system by combing augmented reality (AR), computer vision, and steady-state visual evoked potential (SSVEP)-BCI. AR environment was implemented by a Microsoft HoloLens. Flickering stimuli for eliciting SSVEPs were presented on the HoloLens, which allowed users to see both the robotic arm and the user interface of the BCI. Thus users did not need to switch attention between the visual stimulator and the robotic arm. A four-command SSVEP-BCI was built for users to choose the specific object to be operated by the robotic arm. Once an object was selected, the computer vision would provide the location and color of the object in the workspace. Subsequently, the object was autonomously picked up and placed by the robotic arm. According to the online results obtained from twelve participants, the mean classification accuracy of the proposed system was 93.96 ± 5.05%. Moreover, all subjects could utilize the proposed system to successfully pick and place objects in a specific order. These results demonstrated the potential of combining AR-BCI and computer vision to control robotic arms, which is expected to further promote the practicality of BCI-controlled robots."
9156808,Uncertainty-Aware CNNs for Depth Completion: Uncertainty from Beginning to End,"The focus in deep learning research has been mostly to push the limits of prediction accuracy. However, this was often achieved at the cost of increased complexity, raising concerns about the interpretability and the reliability of deep networks. Recently, an increasing attention has been given to untangling the complexity of deep networks and quantifying their uncertainty for different computer vision tasks. Differently, the task of depth completion has not received enough attention despite the inherent noisy nature of depth sensors. In this work, we thus focus on modeling the uncertainty of depth data in depth completion starting from the sparse noisy input all the way to the final prediction. We propose a novel approach to identify disturbed measurements in the input by learning an input confidence estimator in a self-supervised manner based on the normalized convolutional neural networks (NCNNs). Further, we propose a probabilistic version of NCNNs that produces a statistically meaningful uncertainty measure for the final prediction. When we evaluate our approach on the KITTI dataset for depth completion, we outperform all the existing Bayesian Deep Learning approaches in terms of prediction accuracy, quality of the uncertainty measure, and the computational efficiency. Moreover, our small network with 670k parameters performs on-par with conventional approaches with millions of parameters. These results give strong evidence that separating the network into parallel uncertainty and prediction streams leads to state-of-the-art performance with accurate uncertainty estimates."
9523169,"Fast and Accurate Single-Image Depth Estimation on Mobile Devices, Mobile AI 2021 Challenge: Report","Depth estimation is an important computer vision problem with many practical applications to mobile devices. While many solutions have been proposed for this task, they are usually very computationally expensive and thus are not applicable for on-device inference. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based depth estimation solutions that can demonstrate a nearly real-time performance on smartphones and IoT platforms. For this, the participants were provided with a new large-scale dataset containing RGB-depth image pairs obtained with a dedicated stereo ZED camera producing high-resolution depth maps for objects located at up to 50 meters. The run-time of all models was evaluated on the popular Raspberry Pi 4 platform with a mobile ARM-based Broadcom chipset. The proposed solutions can generate VGA resolution depth maps at up to 10 FPS on the Raspberry Pi 4 while achieving high fidelity results, and are compatible with any Android or Linux-based mobile devices. A detailed description of all models developed in the challenge is provided in this paper."
9607463,Dyadformer: A Multi-modal Transformer for Long-Range Modeling of Dyadic Interactions,"Personality computing has become an emerging topic in computer vision, due to the wide range of applications it can be used for. However, most works on the topic have focused on analyzing the individual, even when applied to interaction scenarios, and for short periods of time. To address these limitations, we present the Dyadformer, a novel multi-modal multi-subject Transformer architecture to model individual and interpersonal features in dyadic interactions using variable time windows, thus allowing the capture of long-term interdependencies. Our proposed cross-subject layer allows the network to explicitly model interactions among subjects through attentional operations. This proof-of-concept approach shows how multi-modality and joint modeling of both interactants for longer periods of time helps to predict individual attributes. With Dyadformer, we improve state-of-the-art self-reported personality inference results on individual subjects on the UDIVA v0.5 dataset."
9339485,An IoT based System for Public Transport Surveillance using real-time Data Analysis and Computer Vision,"The need for a real-time surveillance system in public transport in Bangladesh along with the Indian subcontinent is skyrocketing. The increasing number of working and school-going citizens of Bangladesh need safety in the way of their destination. The Internet of things (IoT) is widely used in our society to provide safety and security to our homes and organization. Ensure the safety of passengers and monitoring drivers and local buses is getting the primary concern of the bus owner. With the collaboration of IoT, Hardware, computer vision, and AI we build an internet-based system that can provide an end-to-end solution to all problems in public transport. The movement of local transport is also a concerning matter in our country like over speeding the vehicle which causes a massive amount of road accidents. Integrating GPS tracking system with our automation safety system, we can create a geofencing to control and monitor the local buses of our country. This paper will delineate the design and the prototype of our surveillance system using computer vision and real-time data analysis."
9602852,Research and Development of Vision Based Target System with Somatosensory Control,"At present, the somatosensory target system game equipment in the market is still in its infancy. According to the current market situation and future development trends, this paper designs a home somatosensory target system. This design uses computer vision technology to realize somatosensory operation, discarding the throwing objects in traditional target system games, making it convenient, safe and improving its entertainment. This thesis first analyzes the requirements and performance index of the home somatosensory target, determines the overall design concept, and then designs the recognition algorithm, using computer vision technology to determine the thrower's throwing posture, and then determine it the accuracy of its throw. Finally, the system is programmed to realize its various functions, and to debug the various functions of the system. Experiments have proved that the system can determine the posture of the body after the throw is completed by the relative positional relationship between the hand and the head in the image, and use this as a basis to determine the accuracy of the throw, and then give the corresponding score. The system functions well and achieve the expected results."
9460988,FloodNet: A High Resolution Aerial Imagery Dataset for Post Flood Scene Understanding,"Visual scene understanding is the core task in making any crucial decision in any computer vision system. Although popular computer vision datasets like Cityscapes, MS-COCO, PASCAL provide good benchmarks for several tasks (e.g. image classification, segmentation, object detection), these datasets are hardly suitable for post disaster damage assessments. On the other hand, existing natural disaster datasets include mainly satellite imagery which has low spatial resolution and a high revisit period. Therefore, they do not have a scope to provide quick and efficient damage assessment tasks. Unmanned Aerial Vehicle (UAV) can effortlessly access difficult places during any disaster and collect high resolution imagery that is required for aforementioned tasks of computer vision. To address these issues we present a high resolution UAV imagery, FloodNet, captured after the hurricane Harvey. This dataset demonstrates the post flooded damages of the affected areas. The images are labeled pixel-wise for semantic segmentation task and questions are produced for the task of visual question answering. FloodNet poses several challenges including detection of flooded roads and buildings and distinguishing between natural water and flooded water. With the advancement of deep learning algorithms, we can analyze the impact of any disaster which can make a precise understanding of the affected areas. In this paper, we compare and contrast the performances of baseline methods for image classification, semantic segmentation, and visual question answering on our dataset. FloodNet dataset can be downloaded from here: https://github.com/BinaLab/FloodNet-Supervised_v1.0."
9588070,A device for recognizing and correcting star wheel datum based on Machine Vision,"Star wheel is a key component in the uniform speed universal joint of automobile steering structure. It is used for power transmission between any pair of rotating shafts whose axes intersect and their relative positions often change. With the popularization of intelligent manufacturing and automatic production line, the automatic detection of star wheel has important practical value. The correct placement of the star wheel directly affects the subsequent automatic measurement results, which is on the automatic detection line. The accurate and fast identification of the datum is the key factor to determine whether the star wheel can realize the automatic measurement and the measurement speed. This system takes Mitsubishi FX3GA-24MT-CM PLC as the core to design the star wheel datum recognition and correction device. Based on the machine vision image processing algorithm to accurately identify the datum level, the mechanical structure of transmission and correction is designed, as well as the PLC electrical control system to complete the automatic transmission, recognition and correction of the star wheel. The test results show that the recognition rate of the system is above 96%, and the detection time is 0.581s, which meets the technical requirements of the star wheel detection line."
9115153,A Maglev Gap Measurement Method based on Machine Vision,"During the gap measurement of maglev system, aiming at the problem that gap sensor is easily affected by temperature, flatness of detection surface, inconvenient installation and other factors, a method of maglev gap measurement based on machine vision is proposed. First, according to the scale space theory, find the relative coordinates of the floating gap in the original image. Then use Canny operator to detect the edge characteristics of the electromagnet and the maglev ball, and sample the edge points of the electromagnet and the maglev ball according to the relative coordinates of the maglev gap. Finally, calculate the floating gap based on the pixel distance between the edge points. Experiments show that under the embedded ARM platform, the speed and accuracy of gap measurement meet the requirements of maglev control."
9189027,The virtual training platform for computer vision,"With the development of deep learning, computer vision has made great progress. Computer vision training based on deep learning requires a large number of data sets. However, manual obtaining of relevant data sets is costly, and some special samples are not easy to obtain. Therefore, in order to solve the lack of data sets, a virtual training platform is designed. The platform is able to render three-dimensional scene simulation by using Unity 3D, and automatically generates visual training data sets through corresponding script file. Through the interactive interface of the platform, users can select and adjust the model, scene, light and other variables independently according to different needs, and quickly generate multi-angle pictures and corresponding annotation information. Compared with the previous computer vision training data sets obtained by manual obtain of image information and label, it has the advantages of fast speed, low cost and large scale. At the same time, use the object detection network to evaluate the data generated by the platform and the data obtained by the previous way. It can be observed that the virtual data set can achieve certain effects. Therefore, the data sets generated by this platform can replace the traditional ones to a certain extent."
9157571,EcoNAS: Finding Proxies for Economical Neural Architecture Search,"Neural Architecture Search (NAS) achieves significant progress in many computer vision tasks. While many methods are proposed to improve the efficiency of NAS, the search progress is still laborious because training and evaluating plausible architectures over large search space is time-consuming. Assessing network candidates under a proxy (i.e., computationally reduced setting) thus becomes inevitable. In this paper, we observe that most existing proxies exhibit different behaviors in maintaining the rank consistency among network candidates. In particular, some proxies can be more reliable - the rank of candidates does not differ much comparing their reduced setting performance and final performance. In this paper, we systematically investigate some widely adopted reduction factors and report our observations. Inspired by these observations, we present a reliable proxy and further formulate a hierarchical proxy strategy that spends more computations on candidate networks that are potentially more accurate, while discards unpromising ones in early stage with a fast proxy. This leads to an economical evolutionary-based NAS (EcoNAS), which achieves an impressive 400×search time reduction in comparison to the evolutionary-based state of the art [19] (8 v.s. 3150 GPU days). Some new proxies led by our observations can also be applied to accelerate other NAS methods while still able to discover good candidate networks with performance matching those found by previous proxy strategies. Codes and models will be released to facilitate future research."
9445397,Design and Deployment of a Generic Software for Managing Industrial Vision Systems,"Computer vision systems have become a key element of modern companies. However, its implantation not only requires specialized hardware components but also software to control the correct performance of all these components. The purpose of this article is to present the design and deployment of a software created to manage the different computer vision systems of any company. First, using our experience in mechanization and industrialization of company processes, a general company specification will be proposed as well as the main software requirements that have to be satisfied. Then, a three-level modular design composed of the core, a configuration module, and user interfaces with functionalities capable of satisfying the defined requirements will be presented. Special attention will be given to time restrictions and components' synchronization. In addition, the different tests that have been carried out to control the correct performance of the software will be shown. The development process will end with a generic, modular, and scalable software able to fit different industrial scenarios by simply modifying a set of input parameters. To illustrate the correct performance of the proposal, the details of its installation in four real companies with different needs will be presented. The proposed work has a practical use in industry and it also provides a thorough description of the main components involved in computer vision systems of real company environments and how to manage them."
9633593,Study on Airport Runway Incursion Monitoring Based on Computer Vision,"The runway serves as the start and end facilities for aircrafts operations, so its safety cannot be underestimated. With the increasing traffic flow at airports, runway incursions have occurred frequently, which has a huge impact on the safety of civil aviation. Research on preventing runway incursion is of urgency. In order to more accurately determine the occurrence of runway incursion and its specific location, the actual coordinates of the airport surface target are reversed through the conversion of the camera coordinate system and the method of camera calibration. Besides, the key nodes of the airport surveillance are determined, and the airport surface model is constructed. Combined with the construction of the airport surface model, the actual position of the target is determined, and the runway monitoring model is supplemented by fusing the runway status lights."
9065793,A computer vision based smart mirror with virtual assistant,"In order to provide better usability of the home automation system as well as the smart home system this System, which is based on a smart mirror (i.e. a two way mirror with a display behind it) that recognizes faces and according to the user it gives relevant information that a user has requested, or selected to receive. The smart mirror provides the information as well as the reflection of the user as a normal mirror usually do. It is powered by the Raspberry Pi for all the computing purpose. This system is designed to be used at home but also can be used elsewhere according to the requirement."
9003733,Automatic Cacao Pod Detection Under Outdoor Condition Using Computer Vision,"This study aims to detect and count cacao pods on trees using a 4K resolution drone. The image was taken with the difference in distances and illumination in the plantation area which affect the color variation of the object's surface. The data variation is a challenge in this study. There are 85 images of cacao fruit on trees were taken with three variations of distance, which are 50 cm, 100 cm, and 150 cm. For each distance, all images processed through several stages, i.e. image quality improvement using image enhancement at the preprocessing stage, image segmentation using the K-means method, and BLOB analysis method to detect and count the number of cacao pods on the tree. The result shows that the best distance between the camera and the object is 50 cm with an average accuracy of 93.3%."
9553713,Recent Advances in Artificial Intelligence and Computer Vision for Unmanned Aerial Vehicles,"In recent years we have seen an accelerated development in the technologies enabling advanced navigation and control of Unmanned Aerial Vehicles (UAVs). Such progress has been fueled by the combination of improved hardware as well as breakthrough advances in Artificial Intelligence methods to accomplish tasks that were previously though extremely difficult to automate. In this document we present a brief summary of some of the most representative methods focused on enabling advanced perception, collision avoidance, flight planning and control, as well as some key industry applications of these capabilities."
9306637,Computer Vision Aided 2D Motion Sequence Test Supervision,"This study presents an object trajectory tracking system that would increase the reliability and accuracy of motion sequence testing for devices such as accelerometers. By attaching an ArUco marker to the device, we are able to perform pose estimation for the device and calculate the coordinates of the device in a coordinate system of our choosing. By tracking the coordinates in consecutive frames, the trajectory is noted. While testing, speed corrections are displayed. We are able to compare the test trajectory to a reference trajectory and determine whether the test device is malfunctioning, or the motion was performed incorrectly. In case of an incorrect trajectory, the exact section that was incorrectly performed is determined, and rectified in subsequent testing. This eliminates uncertainty between the test device malfunction and incorrect device motion performed during the test, thus providing more accurate test results."
9497959,Unmanned ship of algae pollution Monitoring based on Internet of things and Machine Vision,"With the remarkable development of machine vision technology, vision sensor has become an important sensor of unmanned ship. The scale, development trend and influence on water quality of algae pollution are monitored through visual, water quality and water sample extraction. The image of algae pollution is obtained by vision sensor, and then the algae pollution information is obtained by image processing. GPS is used to obtain the ship coordinate, and the path deviation is calculated after Kalman filtering. Then the motor speed is controlled by PID algorithm to adjust the yaw rate. Unmanned ship of algae pollution monitoring provides information for cleaning of algae pollution on the water surface."
9181854,Evaluation of appearance-based eye tracking calibration data selection,"Eye tracking is a valuable topic in computer vision. Appearance-based eye tracking is a promising research direction in recent years. Convolutional neural networks (CNN) had been used in gaze estimation, which cover the significant variability in eye appearance caused by unconstrained head motion. With computation capability of consumer devices rapidly evolving, accurate and efficient appearance-based eye tracking has the potential for multipurpose applications. Person-independent networks have limit in improving gaze estimation accuracy. Person-specific network with calibration is more effective than person-independent approaches. Unlike classical eye tracking methods, appearance-based eye tracking has not a clear way to calibration. Our goal is to analyze the impact of calibration data selection and calibration target distribution on person-specific gaze estimation accuracy. We trained person-independent network and use SVR to calibration. We choose two kind of typical distribution targets to evaluation. Use different distribution targets to calibration achieves different accuracy."
9421185,Research on Application of Deep Learning Algorithm in Image Classification,"Image classification is an important research direction in the field of computer vision. Image classification algorithm can distinguish different kinds of pictures by classifying the features extracted from the original pictures. In essence, deep learning is actually the technology of simulating and analyzing the human brain through the construction of deep neural network, or learning and interpreting related data by simulating the human brain. As an important information carrier, image has become an important form for people to acquire and transmit information. For massive images, the number of images that each user really needs is very small, so it is of great significance to find the images that users need within the effective time. Based on this, this paper analyzes the research and application of deep learning in image recognition, hoping to further enhance the application effect of deep learning and make it play a greater role in the field of image recognition."
9225571,Detection of Customer Interested Garments in Surveillance Video using Computer Vision,"One of the basic requirements of humans is clothing and this approach aims to identify the garments selected by customer during shopping, from surveillance video. The existing approaches to detect garments were developed on western wear using datasets of western clothing. They do not address Indian garments due to the increased complexity. In this work, we propose a computer vision based framework to address this problem through video surveillance. The proposed framework uses the Mixture of Gaussians background subtraction algorithm to identify the foreground present in a video frame. The visual information present in this foreground is analysed using computer vision techniques such as image segmentation to detect the various garments, the customer is interested in. The framework was tested on a dataset, that comprises of CCTV videos from a garments store. When presented with raw surveillance footage, the proposed framework demonstrated its effectiveness in detecting the interest of customer in choosing their garments by achieving a high precision and recall."
8770080,Omnidirectional Multicamera Video Stitching Using Depth Maps,"Omnidirectional vision has recently captured plenty of attention within the computer vision community. The popularity of cameras able to capture 360° has increased in the last few years. A significant number of these cameras are composed of multiple individual cameras that capture images or videos, which are stitched together at a later postprocess stage. Stitching strategies have the complex objective of seamlessly joining the images, so that the viewer has the feeling the panorama was captured from a single location. Conventional approaches either assume that the world is a simple sphere around the camera, which leads to visible misalignments on the final panoramas, or use feature-based stitching techniques that do not exploit the rigidity of multicamera systems. In this paper, we propose a new stitching pipeline based on state-of-the-art techniques for both online and offline applications. The goal is to stitch the images taking profit of the available information on the multicamera system and the environment. Exploiting the spatial information of the scene helps to achieve significantly better results. While for the online case, sparse data can be obtained from a simultaneous localization and mapping process, for the offline case, it is estimated from a 3-D reconstruction of the scene. The information available is represented in depth maps, which provide all information in a condensed form and allow easy representation of complex shapes. The new pipelines proposed for both online and offline cases are compared, visually and numerically, against conventional approaches, using a real data set. The data set was collected in a challenging underwater scene with a custom-designed multicamera system. The results obtained surpass those of conventional approaches."
9063804,Smart Vehicle Driving System using Computer Vision based Hand Motion Tracking,"A SmartDriving system has been developed by using classical Image Processing and Cartesian Geometry. This smart system aims to outperform the conventional driving system based on steering wheel and pedals, by using an onboard mini camera and powerful algorithms running on an onboard computer. This SmartDriving system eliminates the need of legs for driving, thus making it convenient for the wheelchair-ridden. It also prevents deaths due to accidents as the prime cause of deaths in accidents, the steering wheel, is replaced by the said system. The vehicle is maneuvered as if an imaginary steering wheel is held in air, and controlled with usual left/right turning. The acceleration is controlled by the Euclidean distance between the hands holding the imaginary steering wheel. Braking is controlled by converging the distance between the hands. SmartDriving system aims to replace conventional driving methods to make driving accessible, easier, safer and smarter for everyone."
9461301,Camera Controlled Smart Food Container Design,"Street animals can be exposed to poison from the sources they eat from environment for various reasons or their food cannot be preserved due to unusual changes in seasonal weather conditions. Also, when people leaves pets at home due to obligation of going on vacation or to work, their cat food may become stale and run out by the time. For both cases, it is need to provide food as much as pets need, thanks to a closed system that enables its functioning without human intervention or assistance. This model has a single container of food supply and has been tested on cats as prototype system. The prototype system has been designed with a special hardware using computer vision technology. Cat is detected with TensorFlow library. Whether the fullness rate of the food tank is at the appropriate level or not is checked by the TCS230 / TCS3200 color sensor. When the food container is nearly finished, the user is notified by e-mail using The Simple Mail Transfer Protocol (SMTP)."
9444525,Development of the indoor climate control system,"Nowadays smart home technologies are developing rapidly. Much attention is paid to the issues of intelligent control of microclimate parameters in residential buildings. The article examines the existing approaches to microclimate control, presents the methodology and results of designing the microclimate control system. The system monitors and regulates temperature, air humidity, carbon dioxide concentration in the room autonomously or in the mode of scenarios set by a user. Ventilation adjustment is based on a number of people determined by computer vision algorithms. Ventilation is provided by software-controlled supply valves and an extract fan. A key feature of the system is the ability to add additional climate control devices (for example, a heater, air conditioner or air humidifier) as part of the Smart Home concept (concept of use of electronics to complete various household tasks with minimal human interaction)."
9563921,Smart Light Control Using Thermal Sensor As Human Presence Detection,"Internet of Things (IoT) is one of the technologies that are currently growing and develop really fast. By using and utilizing IoT to the fullest, we can make our daily activities in live a lot easier. For example, with IoT technologies we can control the use of electricity consumption in our home remotely with ease. With this very fast development on IoT, we want to try to use IoT technologies and apply these technologies to our daily lives. Therefore, we trying to create a smart lighting system that can allow us to save more on electricity usage by only turning on the light when it detects presence of human. The system is utilizing thermal sensor to detecting human presence. Thermal camera is chosen rather than motion sensor because by using computer vision, this system is much more accurate than using motion sensor, and also by using thermal sensor it solved privacy problem that plague normal camera system."
8944403,MobileNet Model for Classifying Local Birds of Bangladesh from Image Content Using Convolutional Neural Network,"To classify bird species is quite a challenging task due to complex interdependence on various factors. There have been numerous attempts at perfecting classification. The aim of our work is to classify bird species from image data with a computer vision classification system. In this paper, we put forward a MobileNet model, which gives an amazing accuracy of up to 100%. This is the first work relating to local bird species classification. The proposed model explores a systematic approach to classification. The outcomes prove the efficiency of the model."
9505579,Automatic Dial Comparator Reading using Computer Vision,"This paper presents the design, implementation and tests of the automatic reading algorithms of dial comparators. The main contribution is the study of image processing uncertainties with regard to the components of the measurement, those being the dial marks and main needle."
9498356,A Computer Vision Approach for Automated Driver Assistance System,"In this paper, an approach which address the problem of Road Accidents has been addressed. Considering today’s situation, preventing fatalities during road accidents is the need of the hour. The aim is to address the problem, how could a devise and electronic system that could prevent road accidents in order to save peoples life. According to recent surveys, major accidents are caused only due to negligence and frequently leads to accidents in highways and freeways during lane switching. Thus, this devised system has the ability to detect driver’s drowsiness using facial landmarking and tracks proper in-lane driving using Hough transform. Both these factors are to be monitored all through-out the journey and notifies the driver if any drowsiness or improper lane driving is detected. Hence, the proposed system is a feasible and viable solution to devise a novel, retro-fittable & affordable driver assistance system."
8993260,Driver Distraction Detection using Deep Learning and Computer Vision,"Driver distraction is a foremost cause for motor vehicle accidents and incidents. Driving requires an intensive amount of concentration otherwise the results can be fatal. Yet, most motor vehicles have no system in place to assist the driver when he is feeling drowsy, fatigued or distracted. In this research, we developed a system which detects the driver's drowsiness by using deep learning and computer vision. Whenever the driver is not concentrating, an alarm ring's. Image recognition is made possible through a model called deep convolutional neural networks (CNN). CNN can achieve good performance even on difficult image recognition tasks. Convolutional Neural Networks is a class of artificial neural networks. In this research, we used state of the art model for estimating the position of the face and the eyes to help in the better detection and reduce false positives and false negatives."
9124998,The Algorithm of UAV Automatic Landing System Using Computer Vision,"This paper discusses the algorithms of vision systems for automatic UAV (Unmanned Aerial Vehicle) landing. The basic algorithms for finding helipad and adapting the UAV control system to an autonomous UAV landing are presented. The authors consider algorithms to solve navigation problems, building block diagrams of automatic UAV control. Also, the developed algorithms make it possible to implement an automatic landing system for UAVs using technical vision systems, using various camera parameters, algorithms allowing research, as part of the autonomous navigation of unmanned aerial vehicles. The article discusses the developed algorithm that allows you to highlight certain characteristic points for visual navigation. Also, a search system for characteristic points has been developed that allows automatic UAV landing. These algorithms may be useful for the autonomous landing UAV systems. Algorithms can also be used to track a trajectory of the UAV system."
9423393,Large image datasets: A pyrrhic win for computer vision?,"In this paper we investigate problematic practices and consequences of large scale vision datasets (LSVDs). We examine broad issues such as the question of consent and justice as well as specific concerns such as the inclusion of verifiably pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an example, we perform a cross-sectional model-based quantitative census covering factors such as age, gender, NSFW content scoring, class- wise accuracy, human-cardinality-analysis, and the semanticity of the image class information in order to statistically investigate the extent and subtleties of ethical transgressions. We then use the census to help hand-curate a look-up-table of images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of verifiably pornographic: shot in a non-consensual setting (up-skirt), beach voyeuristic, and exposed private parts. We survey the landscape of harm and threats both the society at large and individuals face due to uncritical and ill-considered dataset curation practices. We then propose possible courses of correction and critique their pros and cons. We have duly open-sourced all of the code and the census meta-datasets generated in this endeavor for the computer vision community to build on. By unveiling the severity of the threats, our hope is to motivate the constitution of mandatory Institutional Review Boards (IRB) for large scale dataset curation."
8861826,Towards Integrated Image Contrast Models in Segmentation of Trees,"Computer vision is an area in high demand which is bringing new trends for urban and rural applications. Some examples can be found in autonomous navigation projects, monitoring services, fruits/grain harvesting, pest control, and so forth. However, drastic or even unperceptive changes in the image acquisition process limit the development of these applications, especially for problems that require solutions for uncontrolled environments such as outdoor areas. Thus, the definition of what a machine is looking at is a challenging task. In this study, we dealt with the image segmentation problem in order to develop a method to delineate tree trunks, their branches, and foliage. As tree detection is a crucial topic in mobile robotics, we investigated it to give an initial interpretation of external scenes. We prepared an image dataset to validate the proposal in which two classes were defined, tree and non-tree. The pixels of each image were classified based on the proposed method, and the results show that our method obtained a positive result of 91% accuracy."
9236889,Research on 3D Reconstruction in Binocular Stereo Vision Based on Feature Point Matching Method,"Computer vision as an important branch of computer science and artificial intelligence has made rapid progress in the past thirty years. Binocular stereo vision is one of the most important parts in computer vision. Binocular stereo vision can well simulate human eyes stereoscopic perception of three-dimensional objects, and has been widely used in various fields of industrial automation production and practical life. Binocular stereo vision technology is a comprehensive technology, whose knowledge includes optics, physics, image processing, computer, artificial intelligence and electronic technology, and other fields of content. Camera calibration and feature point matching are difficult and key points in binocular stereo vision technology. In this paper, the 3D reconstruction of binocular stereo vision based on feature point matching method is discussed. The main research includes the following contents: binocular stereo vision system principle, feature matching algorithm research and 3D reconstruction system implementation. This project puts forward a set of feasible algorithms and finally can get a good three-dimensional reconstruction effect based on the analysis and research of a series of algorithms."
9258900,"Deep Learning and Computer Vision for Estimating Date Fruits Type, Maturity Level, and Weight","According to the Food and Agriculture Organization, the world production of date fruits is 8,526,218 tons and around 1,302,859 tons in the Kingdom of Saudi Arabia (KSA) in 2018. There are several types of date fruits, and the most common in KSA are Barhi, Khalas, Meneifi, Naboot Saif, and Sullaj. Moreover, there are around five main maturity levels: Immature, Khalal, Khalal with Rutab, Pre-Tamar, and Tamar. Harvesting date fruits is performed according to its maturity level and type, which is a critical decision that significantly affects profit. In this paper, we propose a smart harvesting decision system to estimate date fruits type, maturity level, and weight using computer vision (CV) and deep learning (DL) techniques. The proposed system consists of three sub-systems: Dates maturity estimation system (DMES), type estimation system (DTES), and dates weight estimation system (DWES). We utilized four DL architectures, including ResNet, VGG-19, Inception-V3, and NASNet for both DMES and DTES and support vector machine (SVM) (regression and linear) for DWES. We evaluated the performance of the proposed system using the dataset collected by the Center of Smart Robotics Research. Using multiple performance metrics, DTES achieved maximum performance of 99.175% accuracy, an F1 score of 99.225%, 99.8% average precision, and 99.05% average recall. The maximum performance of DMES was 99.058% accuracy, F1 score of 99.34%, 99.64% average precision, and 99.08% average Recall. DWES achieved a maximum performance of 84.27% using SVM-Linear."
9423426,EDEN: Multimodal Synthetic Dataset of Enclosed GarDEN Scenes,"Multimodal large-scale datasets for outdoor scenes are mostly designed for urban driving problems. The scenes are highly structured and semantically different from scenarios seen in nature-centered scenes such as gardens or parks. To promote machine learning methods for nature-oriented applications, such as agriculture and gardening, we propose the multimodal synthetic dataset for Enclosed garDEN scenes (EDEN). The dataset features more than 300K images captured from more than 100 garden models. Each image is annotated with various low/high-level vision modalities, including semantic segmentation, depth, surface normals, intrinsic colors, and optical flow. Experimental results on the state-of-the-art methods for semantic segmentation and monocular depth prediction, two important tasks in computer vision, show positive impact of pre-training deep networks on our dataset for unstructured natural scenes. The dataset and related materials will be available at https://lhoangan.github.io/eden."
8707957,Automatic Counting and Individual Size and Mass Estimation of Olive-Fruits Through Computer Vision Techniques,"Fruit grading is an essential post-harvest task in the olive industry, where size-and-mass based fruit classification is especially important when processing high-quality table olives. Within this context, this research presents a new methodology aimed at supporting accurate automatic olive-fruit grading by using computer vision techniques and feature modeling. For its development, a total of 3600 olive-fruits from nine varieties were photographed, stochastically distributing the individuals on the scene, using an ad-hoc designed an imaging chamber. Then, an image analysis algorithm, based on mathematical morphology, was designed to individually segment olives and extract descriptive features to estimate their major and minor axes and their mass. Regarding its accuracy for the individual segmentation of olive-fruits, the algorithm was proven through 117 captures containing 11 606 fruits, producing only six fruit-segmentation mistakes. Furthermore, by linearly correlating the data obtained by image analysis and the corresponding reference measurements, models for estimating the three features were computed. Then, the models were tested on 2700 external validation samples, giving relative errors below 0.80% and 1.05% for the estimation of the major and minor axis length for all varieties, respectively. In the case of estimating olive-fruit mass, the models provided relative errors never exceeding 1.16%. The ability of the developed algorithm to individually segment olives stochastically positioned, along with the low error rates of around 1% reported by the estimation models for the three features, makes the methodology a promising alternative to be integrated into a new generation of improved and non-invasive olive classification machines. The new developed system has been registered in the Spanish Patent and Trademark Office with the number P201930297."
9332217,Visibility Enhancement in Low Light Images with Weighted Fusion of Robust Retinex Model and Dark Channel Prior,"Images captured under poor illumination or at night time doesn’t have significant details as compared to images captured under proper lighting conditions. These images, when used for computer vision applications might be the reason for undesirable output. So, these kinds of images are not suitable for observation and analysis is case of any computer vision application. To solve this problem, visibility enhancement in low light images with weighted fusion of robust retinex model and dark channel prior based enhancement method have been proposed in the literature. The paper proposes visibility enhancement in low light images with weighted fusion of robust retinex model and dark channel prior based enhancement. The validation of proposed method is judged based on entropy. The performance based on the entropy as measure, is evaluated for proposed system and compared with the other existing popular low light image enhancement techniques. For rigorous validation, different weights combinations are explored in the proposed fusion based image enhancement method."
9544100,AI-based content filtering system using an age prediction algorithm,"Computer vision mainly focuses on the automatic extraction, analysis, and understanding of useful information from a single image or video. On the other hand, authenticity is emerging as one of the primary requirements in today's world by developing a system for computer vision complexity. Generally, two robust techniques such as age estimation and face recognition are required to maintain authenticity. In reality, fraud and scams are getting increased, so here this paper has proposed a new combined model for face recognition and age prediction. Face recognition has been implemented and presented in this paper by using a Deep Neural Network. The authenticity problem can be handled by using either facial recognition or age prediction alone; this study has presented a method that employs both of them together to enhance the system's robustness. So, first, this model detects the person's face, and then it predicts the person's age. If the individual is eligible to view the information or perform a task, their access will be limited; otherwise, their access will be restricted. So it helps to solve two difficulties in this case: the person's identification cannot be faked, and their age is also confirmed by the system. (CNN for the face, and mention technique for the age.)"
9054129,Unsupervised Image-to-Image Translation Via Fair Representation of Gender Bias,"Fairness becomes a critical issue of computer vision to reduce discriminative factors in various systems. Among computer vision tasks, Image-to-Image translation for facial attributes editing can yield discriminative results. The unexpected gender changed results can be generated instead of editing target attributes due to the dataset imbalance problem. In this work, we propose a framework of unsupervised Image-to-Image translation that learns a fair representation by separating the latent space of our model into two purposes: 1) Target Attribute Editing, 2) Gender Preserving. We evaluate the proposed framework on CelebA dataset. Both quantitive and qualitative results demonstrate that our method improves image quality and fairness than the prior Image-to-Image translation method."
9062586,Embedded Night-Vision System for Pedestrian Detection,"Assistive vision-based solutions for the driver extend the capabilities of human vision and support safe travel. Unfortunately, their widespread usage is generally limited to expensive cars. Interestingly, a high price is most likely a derivative of the costs incurred in the research instead of the value of hardware components. In the article we show that a mobile system for pedestrian detection in severe lighting conditions can be build using state of the art algorithms and widely available hardware. The proposed night-vision system for pedestrian detection processes thermal images using a proprietary ODROID XU4 microcomputer under Ubuntu MATE operating system. We applied a cascade object detector for the task of human silhouette detection in context of thermal imagery and contrasted the results with the state of the art deep learning approach. The experiments conducted prove effectiveness of the proposed solution."
9262407,Computer Vision on Identifying Persons under Real Time Surveillance using IOT,"Real time suspicious activities have been taken place every day at public places which threatens people, for that identification of Criminal person or missing person is necessary for police to take necessary action immediately so the crime rate will be reduced. This Project helps police by identifying of persons from datasets provided and trained and matches using face recognition using Deep neural networks which gives better accuracy. After the identification it detects and sends message to police with respective details and by sharing location currently person spotted. This done by recognizing and identifying the persons from database using face recognition Algorithm to detect faces of person accurately. It can also able to detect the person from 14 feet distance and tells the person is spotted or not. this system really useful when we try to solve real time problems occurred immediately and all actions can be taken place by IOT."
9150675,Multi-Granularity Tracking with Modularlized Components for Unsupervised Vehicles Anomaly Detection,"Anomaly detection on road traffic is a fundamental computer vision task and plays a critical role in video structure analysis and urban traffic analysis. Although it has attracted intense attention in recent years, it remains a very challenging problem due to the complexity of the traffic scene, the dense chaos of traffic flow and the lack of fine-grained abnormal labeled data. In this paper, we propose a multi-granularity tracking approach with modularized components to analyze traffic anomaly detection. The modularized framework consists of a detection module, a background modeling module, a mask extraction module, and a multi-granularity tracking algorithm. Concretely, a box-level tracking branch and a pixel-level tracking branch is employed respectively to make abnormal predictions. Each tracking branch helps to capture abnormal abstractions at different granularity levels and provide rich and complementary information for the concept learning of abnormal behaviors. Finally, a novel fusion and backtracking optimization is further performed to refine the abnormal predictions. The experimental results reveal that our framework is superior in the Track4 test set of the NVIDIA AI CITY 2020 CHALLENGE, which ranked first in this competition, with a 98.5% F1-score and 4.8737 root mean square error."
8953722,Events-To-Video: Bringing Modern Computer Vision to Event Cameras,"Event cameras are novel sensors that report brightness changes in the form of asynchronous “events” instead of intensity frames. They have significant advantages over conventional cameras: high temporal resolution, high dynamic range, and no motion blur. Since the output of event cameras is fundamentally different from conventional cameras, it is commonly accepted that they require the development of specialized algorithms to accommodate the particular nature of events. In this work, we take a different view and propose to apply existing, mature computer vision techniques to videos reconstructed from event data. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. Our experiments show that our approach surpasses state-of-the-art reconstruction methods by a large margin (> 20%) in terms of image quality. We further apply off-the-shelf computer vision algorithms to videos reconstructed from event data on tasks such as object classification and visual-inertial odometry, and show that this strategy consistently outperforms algorithms that were specifically designed for event data. We believe that our approach opens the door to bringing the outstanding properties of event cameras to an entirely new range of tasks. A video of the experiments is available at https://youtu.be/IdYrC4cUO0I."
9729083,Obstacles Detection for Electric Wheelchair with Computer Vision,"This research aims to present the detection system of an obstacle for electric wheelchair using computer vision in order to facilitate for disabled persons and reduce the possibilities of accidents. In this system, the distance threshold is set to alert when a wheelchair is approaching an obstacle. The alert system consists of the smartphone's camera attached to the back of a wheelchair. The YOLOv3 model was used for object detection. The researcher has developed an algorithm to detect obstacles such as pillars, doors, or edge of the wall with edge detection method to enhance the detection efficiency of the system. Therefore, the usage of two algorithms enables the system to choose the obstacle detection between objects and edge detection. The research found that the system can choose the algorithm to detect obstacles with an accuracy of up to 80%. Moreover, the experiment revealed that the system can alert warnings before collisions with an accuracy of up to 90%. Further, this system can also calculate the approximate time prior to the collision."
8646254,Development of Computer Vision-Based Movement Controlling in Mecanum Wheel Robotic Car,"The purpose of this research paper is how to control the mecanum wheel robotic car to approach the target via wireless by using the computer vision. The mecanum wheel robotic car includes four mecanum wheels and the main microcontroller. This robotic car is controlled by the vision-based control system using the openCV through the wireless communication. The openCV is used for the pattern recognition which is a method of image processing. The openCV detects the robotic car and the target pattern to compare the location. After comparing the locations, the data is sent to the robotic car to go or stop. To send data from the openCV to the robotic car, Pyserial method is used before transmitting that data wirelessly. In such way, the location of the robotic car is detected continuously, and the data is sent to the robotic car to approach the target pattern."
8807654,Migration Versus Management: the Global Distribution of Computer Vision Engineering Work,"Computer vision professionals develop systems that monitor endangered fish species, alert car dealerships of potential theft, and track inventory on retailers' shelves, to name a few examples. While their products vary, their day-to-day work practices rarely do. Most work in small, co-located, multi-disciplinary teams and rapidly iterate systems built with algorithms, products, and services provided by similar teams but at different companies. Their examples challenge global software engineering research to look beyond the social and technical coordination work of large, internal software development teams. Their stories, culled from ethnographic interviews with eighty computer vision engineers and research scientists, echo the heady days of 1980s Silicon Valley when Bay Area social networks and the global migration of professional talent fueled the rapid growth of the high technology industry. These engineers, then and now, migrate from task to task, versus the task from engineer to engineer."
9633454,A Computer Vision Based on Vehicle Detection and Counting System Using Sensor Security,"Unique—Vehicle identification and counting framework, which assumes a significant part in keen transportation framework, and the administration of the progression of traffic. In this real, we refuse a video metric technique for the place and whole of vehicles self-reliant on PC vision innovations. The proposed lack employs the foundation deduction strategy is to see closer view objects into the video. For a more precise recognition of moving vehicles, and afterward there are some PC vision strategies, including tear an opening to fill, and keeping in mind that the morphology of the exercises. At long last, the vehicle, the statistics will be done with the assistance of a virtual discovery zone. The trial results show ordinarily the exactness and precision of the deliberate vehicle counting framework, it is around 96%."
9418001,Image Captioning using Convolutional Neural Networks and Recurrent Neural Network,Image Caption is a concept of gathering the right description of the given image on the internet use Computer Vision and natural language processing. The following is achieved using the Deep learning techniques called as convolution neural network and recurrent neural network. The dataset used for implementation is called as the Flickr8_k Dataset. The model uses the combination of convolution neural network which helps in extraction whereas the recurrent neural network helps in generation of the right text.
9574030,Quantum Convolutional Neural Networks (QCNN) Using Deep Learning for Computer Vision Applications,"Deep learning algorithms and models have made an impact in the area of AI and machine learning, one among them is CNN. CNN is extensively used in the area of image recognition and object detection for classification purposes. CNN is composed of several layers of filters to get feature maps of input data, yet foremost and crucial one is convolutional layer, hence the name Convolutional neural networks. However, the growth of quantum computing and quantum neural network in deep learning is limited. Three main obstacles that limit the growth of these are, first is due to the lack of real-time quantum computers to experiment with. Second is the improper training algorithms and at last, non-linearity nature of the neural networks. This paper introduces a novel approach to begin one's journey in quantum computing, along with solutions and developments. This work provides a detailed description of architectures, frameworks and algorithms used for implementing a QCNN model. The research was made regarding image recognition and object detection using QCNN and found that QCNN can increase the computational speeds with better performance metrics compared to classical computational methods. This paper also debates about applications of QCNN in computer vision, signal and image processing, Pharmaceuticals, Cryptography and various other fields. This study also explains Key players and future work in developing quantum computers, quantum computing algorithms, software and hardware support to implement QCNN in various applications."
9148099,"Method of Electronic Component Location, Grasping and Inserting Based on Machine Vision","Aiming at the problems of low efficiency, high cost, missing or wrong insertion of labor in PCB production, a method of electronic component location, grabbing and inserting based on machine vision is proposed. By building the machine vision system, three industrial cameras cooperate with dome light sources to take images of the whole components, the pin features of components and the jack features of PCB board respectively. After image acquisition, image processing is carried out on VisionPro 9.0 software platform, which mainly completes feature extraction of component pins and PCB board jacks, and coordinates positioning. Then, coordinate transformation is carried out by nine-point calibration method to guide the manipulator to complete the work of location, grabbing and inserting. Through the running test, the system has a fast grabbing speed and inserting speed. The error inserting rate, missing inserting rate, bad product rate and inserting precision all reach the standard, and the working efficiency is three times that of the manual."
9307242,Requirements of API Documentation: A Case Study into Computer Vision Services,"Using cloud-based computer vision services is gaining traction, where developers access AI-powered components through familiar RESTful APIs, not needing to orchestrate large training and inference infrastructures or curate/label training datasets. However, while these APIs seem familiar to use, their non-deterministic run-time behaviour and evolution is not adequately communicated to developers. Therefore, improving these services' API documentation is paramount-more extensive documentation facilitates the development process of intelligent software. In a prior study, we extracted 34 API documentation artefacts from 21 seminal works, devising a taxonomy of five key requirements to produce quality API documentation. We extend this study in two ways. Firstly, by surveying 104 developers of varying experience to understand what API documentation artefacts are of most value to practitioners. Secondly, identifying which of these highly-valued artefacts are or are not well-documented through a case study in the emerging computer vision service domain. We identify: (i) several gaps in the software engineering literature, where aspects of API documentation understanding is/is not extensively investigated; and (ii) where industry vendors (in contrast) document artefacts to better serve their end-developers. We provide a set of recommendations to enhance intelligent software documentation for both vendors and the wider research community."
8757928,PCB-METAL: A PCB Image Dataset for Advanced Computer Vision Machine Learning Component Analysis,"We introduce PCB-METAL, a printed circuit board (PCB) high resolution image dataset that can be utilized for computer vision and machine learning based component analysis. The dataset consists of 984 high resolution images of 123 unique PCBs with bounding box annotations for ICs(5844), Capacitors(3175), Re-sistors(2670), and Inductors(542). The dataset is useful for image-based PCB analysis such as component detection, PCB classification, circuit design extraction, etc. We also provide baseline evaluations for IC detection and localization on state-of-the-art deep learning object detection algorithms."
9578351,VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation,"Accuracy of many visiolinguistic tasks has benefited significantly from the application of vision-and-language (V&L) BERT. However, its application for the task of vision-and-language navigation (VLN) remains limited. One reason for this is the difficulty adapting the BERT architecture to the partially observable Markov decision process present in VLN, requiring history-dependent attention and decision making. In this paper we propose a recurrent BERT model that is time-aware for use in VLN. Specifically, we equip the BERT model with a recurrent function that maintains cross-modal state information for the agent. Through extensive experiments on R2R and REVERIE we demonstrate that our model can replace more complex encoder-decoder models to achieve state-of-the-art results. Moreover, our approach can be generalised to other transformer-based architectures, supports pre-training, and is capable of solving navigation and referring expression tasks simultaneously."
8658866,"""Keep Me In, Coach!"": A Computer Vision Perspective on Assessing ACL Injury Risk in Female Athletes","We present and share a foundational dataset of multi-angle video recordings of scripted athletic movements to enable the development of computer vision research applications that evaluate and identify lower-body injury risk. The focus of the dataset is female athletes, who are at a substantially increased risk of anterior cruciate ligament (ACL) injury and are therefore a top priority for sports science. In our study, varsity and club sport athletes perform two assessment movements (the countermovement jump and the drop jump). These jump tasks are used ubiquitously in sports medicine research to characterize athleticism and to identify risk factors that indicate ACL injury propensity. The novelty of the dataset centers on (i) the type of movement data (purposeful, evaluative movements that need to be tracked with a high degree of precision), (ii) our generalized collection method that can be replicated with ease by non-experts, and (iii) the amount of data collected (we collected data from 55 division one (D1) female athletes performing 3-5 iterations of each jumps, for a total of 480 jumps). Data from each camera was manually aligned and a fully automated pipeline was built to extract knee information from athletes. Ideally, any athlete or researcher will be able to easily replicate our setup and assemble a compatible and complementary dataset to propel the development and assessment of injury propensity models."
8819803,VQAR: Review on Information Retrieval Techniques based on Computer Vision and Natural Language Processing,"Recently Computer vision and Natural language processing paradigm contains enormous research progress in their respective areas. Despite the progress in both areas, still it remains as a challenging task for machines to extract image semantics and then communicate this extracted information with the desired users. These problems will be solved by Visual Question Answering (VQA) system by connecting both computer vision and natural language processing paradigms. In VQA, system is presented with an image and textual question related to that image. The system will generate the answer by processing on both image and textual features. Answer generated by VQA is in one word, phrase or in sentence. Various datasets are available for training and evaluating VQA system which contains real or abstract images and question-answer pairs related to the semantics available in the image. VQA is being used in many areas such as for blind and visually impaired users, robotics, art gallery and many more areas. This paper discusses VQA techniques, VQA datasets and highlights the parametric evaluation of these techniques along with generic issues in VQA system."
8867236,Research and Experiment of an Underwater Stereo Vision System,"Underwater computer vision is a challenging task due to the severe underwater condition. The underwater optical system can provide more detailed information than the acoustic system. This paper describes a new stereo vision system for underwater object detection and 3D reconstruction. The Stereolabs ZED 2K Stereo Camera and NVIDIA Jetson TX2 are the core components of the system. A USB3.0 optical cable can transmit the high-definition video at the maximum speed of 5.0 GB/s within a distance of 300 meters. All software modules are developed based on ROS (Robot Operating System) middleware. The underwater camera calibration has been carried out to correct the underwater distortion of stereo vision, and underwater 3D reconstruction experiments have proven the validity of the system."
9462996,A framework for the automation of testing computer vision systems,"Vision systems, i.e., systems that enable the detection and tracking of objects in images, have gained substantial importance over the past decades. They are used in quality assurance applications, e.g., for finding surface defects in products during manufacturing, surveillance, but also automated driving, requiring reliable behavior. Interestingly, there is only little work on quality assurance and especially testing of vision systems in general. In this paper, we contribute to the area of testing vision software, and present a framework for the automated generation of tests for systems based on vision and image recognition with the focus on easy usage, uniform usability and expandability. The framework makes use of existing libraries for modifying the original images and to obtain similarities between the original and modified images. We show how such a framework can be used for testing a particular industrial application on identifying defects on riblet surfaces and present preliminary results from the image classification domain."
9526270,3D Object Localization Using 2D Estimates for Computer Vision Applications,"Computer vision based applications have received notable attention globally due to the interaction with the physical world. In this paper, a novel method for object localization based on camera calibration and pose estimation is discussed. The 3-dimensional (3D) coordinates are computed by taking multiple 2-dimensional (2D) images from different view that meets the requirement in camera calibration process. Several number of steps are involved in camera calibration including estimation of intrinsic and extrinsic parameters for the removal of lens distortion, estimation of object’s size and camera location. Besides, a technique to estimate the 3D pose using 2D images is proposed and the results of camera parameters and localization are applied for the 3D reconstruction. The hardware implementation of the proposed approach is implemented on HP core i5 with the MATLAB support packages and experimental results are validated for both camera calibration and pose estimation."
9461389,Gesture-Controlled Robotic Arm Utilizing OpenCV,"In this paper, a low-cost, 3D printed robotic arm that uses a system of gesture recognition controlled via computer vision is presented. The developed approach of Human-Computer Interaction (HCI) employs a single commonly available USB2.0 High-Definition camera to capture hand movements and gestures, implementing the OpenCV library. This enables the translation of those movements and gestures into commands that can be transferred using internet protocols to a Raspberry Pi; then interfaces with the robotic arm. Using computer vision as a method of HCI reduces components and overall cost. The Raspberry Pi uses individual commands to control the robotic arm's servo motors while monitoring multiple pressure sensors and limit switches to ensure the robotic arm operates within a certain limit of load. This prevents the arm from overextending itself while protecting the integrity of items within its grasp."
9022188,"Compact and Efficient Multitask Learning in Vision, Language and Speech","Across-domain multitask learning is a challenging area of computer vision and machine learning due to the intra-similarities among class distributions. Addressing this problem to cope with the human cognition system by considering inter and intra-class categorization and recognition complicates the problem even further. We propose in this work an effective holistic and hierarchical learning by using a text embedding layer on top of a deep learning model. We also propose a novel sensory discriminator approach to resolve the collisions between different tasks and domains. We then train the model concurrently on textual sentiment analysis, speech recognition, image classification, action recognition from video, and handwriting word spotting of two different scripts (Arabic and English). The model we propose successfully learned different tasks across multiple domains."
8453869,Computer Vision Analysis for Quantification of Autism Risk Behaviors,"Observational behavior analysis plays a key role for the discovery and evaluation of risk markers for many neurodevelopmental disorders. Research on autism spectrum disorder (ASD) suggests that behavioral risk markers can be observed at 12 months of age or earlier, with diagnosis possible at 18 months. To date, these studies and evaluations involving observational analysis tend to rely heavily on clinical practitioners and specialists who have undergone intensive training to be able to reliably administer carefully designed behavioral-eliciting tasks, code the resulting behaviors, and interpret such behaviors. These methods are therefore extremely expensive, time-intensive, and are not easily scalable for large population or longitudinal observational analysis. We developed a self-contained, closed-loop, mobile application with movie stimuli designed to engage the child's attention and elicit specific behavioral and social responses, which are recorded with a mobile device camera and then analyzed via computer vision algorithms. Here, in addition to presenting this paradigm, we validate the system to measure engagement, name-call responses, and emotional responses of toddlers with and without ASD who were presented with the application. Additionally, we show examples of how the proposed framework can further risk marker research with fine-grained quantification of behaviors. The results suggest these objective and automatic methods can be considered to aid behavioral analysis, and can be suited for objective automatic analysis for future studies."
9175713,Computer Vision Challenges for Chronic Wounds Assessment,"Chronic wound assessment and wound healing are important for diagnostic, follow up and wound treatment. However, this growing disease affecting nearly 2 thousand million and 5.7 million people in the USA and Europe, costing around 20 billion and
8 thousand million USD per year, still relies on subjective human assessment of wounds. A scoping review allowed us to identify 109 articles that map the literature on the topic of computer vision for chronic wound assessment and healing. These results were carefully analyzed and mapped into relevant clinical challenges associated with this field, identifying the maturity of each different computer vision challenge that needs addressing. Results show that wound size and tissue type classification already have interesting work, but various other clinical areas are in need of larger datasets and computer vision research efforts for achieving a relevant impact in today's clinical routine."
9250075,Computational Tool for Analysis of Strains Based on Optical Flow Approach,"At present, there is an increase in the quantity of computer vision method applications in the industry, robotics, in systems of virtual and augmented reality, geoinformation systems and so on. This leads to the need for the creation and development of efficient methods and algorithms for solving the problems of computer vision. In the paper, we present an approach to solve the problem of the finding of displacement vectors in the two consecutive video frames of the strain test process video. To analyse video we use the optical flow approach and propose improvements to feature extraction method of establishing of robust point correspondences between pair of key points in video frames."
9586891,Convolutional Neural Network and Adversarial Autoencoder in EEG images classification,"In this paper, we consider applying computer vision algorithms for the classification problem one faces in neuro-science during EEG data analysis. Our approach is to apply a combination of computer vision and neural network methods to solve human brain activity classification problems during hand movement. We pre-processed raw EEG signals and generated 2D EEG topograms. Later, we developed supervised and semi-supervised neural networks to classify different motor cortex activities."
9396140,The Restaurant Guest Tracking System,This article provides the description of the development of a restaurant guest tracking system and an overview of already existing realization methods. The system works due to computer vision algorithms and neural network. The article contains a description of the main principles of neural network with the face recognition functions and research of different ways to implement the tasks. The purpose of the investigation is finding the most rational solution of the problem. The article consists of comparison of various types of face identification algorithms and computer vision tools and data sets management tools. The result of this research is a developed application that maintains restaurant visitors database. As a conclusion research contains a cons and pros of the provided solution.
9476331,The Segmentation of Plants on RGB Images with Index Based Color Analysis,"Computer vision is an advanced technology and is widely used in aspects such as IoT, agricultural robotics and machine learning. The detection and extraction of plants are the primary tasks of this application. This article shows several index-based colour analysis computer vision methods and relative improvement to extract the plants from backgrounds which contain complex information. The sample images are from the Plant Phenotyping Dataset which contains arabidopsis as desired plants and interference information (metal, moss, white table, etc.). A comparison of the three methods is provided. These methods are Excess Green Index (ExG), Excess Red Index (ExR), and the combination of ExG and ExR (ExG-ExR). Meanwhile, a mask to remove the non-green object (BR mask), Gaussian filter, and erosion operation are added as an improvement to remove the remains. With the evaluation method such as Intersection over Union (IoU) and Sorensen-Dice Similarity Coefficient (DSC), the combination of ExG-ExR, BR mask, Gaussian filter, and erosion shows the best performance with IoU and DSC rate 0.8818 and 0.9368 respectively. However, one setback is that green information like moss is hard to be distinguished from desired plants with these methods. This problem will be researched in the future."
9614158,Advances in Adversarial Attacks and Defenses in Computer Vision: A Survey,"Deep Learning is the most widely used tool in the contemporary field of computer vision. Its ability to accurately solve complex problems is employed in vision research to learn deep neural models for a variety of tasks, including security critical applications. However, it is now known that deep learning is vulnerable to adversarial attacks that can manipulate its predictions by introducing visually imperceptible perturbations in images and videos. Since the discovery of this phenomenon in 2013, it has attracted significant attention of researchers from multiple sub-fields of machine intelligence. In 2018, we published the first-ever review of the contributions made by the computer vision community in adversarial attacks on deep learning (and their defenses). Many of those contributions have inspired new directions in this area, which has matured significantly since witnessing the first generation methods. Hence, as a legacy sequel of our first literature survey, this review article focuses on the advances in this area since 2018. We thoroughly discuss the first generation attacks and comprehensively cover the modern attacks and their defenses appearing in the prestigious sources of computer vision and machine learning research. Besides offering the most comprehensive literature review of adversarial attacks and defenses to date, the article also provides concise definitions of technical terminologies for the non-experts. Finally, it discusses challenges and future outlook of this direction based on the literature since the advent of this research direction."
9630963,Assessing Vision Quality in Retinal Prosthesis Implantees through Deep Learning: Current Progress and Improvements by Optimizing Hardware Design Parameters and Rehabilitation,"Retinal prosthesis (RP) is used to partially restore vision in patients with degenerative retinal diseases. Assessing the quality of RP-acquired (i.e., prosthetic) vision is needed to evaluate RP impact and prospects. Spatial distortions caused by electrical stimulation of the retina in RP, and the low number of electrodes, have limited the prosthetic vision: patients mostly localize shapes and shadows rather than recognizing objects. We simulate prosthetic vision and evaluate vision on image classification tasks, varying critical hardware parameters: total number and size of electrodes. We also simulate rehabilitation by re-training our models on prosthetic vision images. We find that electrode size has little impact on vision while at least 400 electrodes are needed to sufficiently restore vision (more than 65% classification accuracy on a complex visual task after rehabilitation). Argus II, a currently available implant, produces a low-resolution vision leading to low accuracy (21.3% score after rehabilitation) in complex vision tasks. Rehabilitation produces significant improvements (accuracy improvement of up to 30% on complex tasks, depending on the number of electrodes) in the attained vision, boosting our expectations for RP interventions and motivating the establishment of rehabilitation procedures for RP implantees."
9701685,A Novel Assessment Method of Cracks Width Quantitative Model Based on Computer Vision,"Cracks width quantitative models based on computer vision are used to obtain both cracks pixel width and real width. According to the location of candidate point in pixel level and judging rules, these quantitative models are commonly classified into four basic types which are edge-normal type, edge-minimum type, skeleton-normal type and skeleton-minimum. Existing assessment tools, such as crack width error analysis method, care much on the errors between computing value and measuring value. It is limited to the comparison of error value itself by ignoring the angel difference. A novel assessment method is presented, which use dual tuples of width value and measure angle to evaluate the accuracy of crack width quantitative models. This method outputs an assessment factor to enlarge the deviation of the results calculated by these crack width quantitative models, which makes the assessment more reasonable and credible. The comparison is made between traditional assessment method and proposed dual tuples assessment method. It is proved that proposed method could find more measure problems under some specific circumstances."
9527760,Computer Vision and Deep Learning to Manage Safety in Construction: Matching Images of Unsafe Behavior and Semantic Rules,"The determination of people.s unsafe behavior from images in construction has been typically based on hand-made rule approaches, which renders it difficult to identify multiple acts of unsafe behavior within an image and accordingly apply safety rules. This article aims to develop a computer vision and deep learning method that can match images of people's unsafe behavior with semantic safety rules. Our proposed method consists of: 1) image feature representation; 2) safety rule feature representation; and 3) feature fusion similarity whereby unsafe behavior extracted from an image is matched with safety rules. We validate the effectiveness of our method using an image database of people's unsafe behavior from different sites associated with the construction of the Wuhan Metro Project (China). The results of our research explicitly demonstrate that our method is robust and can accurately recognize people's unsafe behavior and the corresponding safety rule that has been contravened. To this end, we suggest that construction organizations can use our method to manage safety better as part of a behavior-based safety strategy and thus prevent accidents."
9120934,"Driver Alertness System using Deep Learning, MQ3 and Computer Vision","Being alert at the wheel, is a pressing problem concerning road safety, that many people face in their daily life. This project proposes developing technologies that can help to detect how alert the driver is. It addresses seven different use cases, to provide better feedback to the driver and therefore helps in reducing the number of accidents. The prototype performs Emotion Analysis through facial expressions, Visual Analysis of traffic signboards, calculates the driver’s Eye Aspect Ratio to detect drowsiness and mobile distractions and also calculates object proximity from the car using techniques of Computer Vision and Neural Networks. Drunk driving is detected using an alcohol sensor module MQ3. It also provides monthly alerts to the car owner about his vehicle maintenance and service. Every use case has a unique response which aims at alerting the driver to tackle the vital issue of safety at the wheel."
9342946,Irrigation Detection by Car: Computer Vision and Sensing for the Detection and Geolocation of Irrigated and Non-irrigated Farmland,"Irrigation can greatly increase the income of smallholder farmers in sub-Saharan Africa. By providing information about current irrigation utilization, or lack thereof, we seek to encourage investment in irrigation systems and their supporting infrastructure. In this paper, we describe the design, prototyping, and testing of a novel, cost-effective, and reliable computer vision system that is capable of locating irrigated plots at scale. Our system will be mounted to a vehicle and record the depth of objects in the camera's view while the vehicle is in motion. The GPS coordinates of objects are computed based on estimated depth, vehicle coordinates, and orientation, available from included sensors. We tested our prototype on objects at various distances from the system and achieved feasible accuracy with acceptable error in the estimated depth. In the future, we hope to deploy the system in parts of sub-Saharan Africa, to detect and geolocate irrigated agricultural plots during the dry season. Then we plan to use that collected data to inform and train machine learning models that use remote sensing and satellite imagery."
9420085,Tensor Methods in Computer Vision and Deep Learning,"Tensors, or multidimensional arrays, are data structures that can naturally represent visual data of multiple dimensions. Inherently able to efficiently capture structured, latent semantic spaces and high-order interactions, tensors have a long history of applications in a wide span of computer vision problems. With the advent of the deep learning paradigm shift in computer vision, tensors have become even more fundamental. Indeed, essential ingredients in modern deep learning architectures, such as convolutions and attention mechanisms, can readily be considered as tensor mappings. In effect, tensor methods are increasingly finding significant applications in deep learning, including the design of memory and compute efficient network architectures, improving robustness to random noise and adversarial attacks, and aiding the theoretical understanding of deep networks. This article provides an in-depth and practical review of tensors and tensor methods in the context of representation learning and deep learning, with a particular focus on visual data analysis and computer vision applications. Concretely, besides fundamental work in tensor-based visual data analysis methods, we focus on recent developments that have brought on a gradual increase in tensor methods, especially in deep learning architectures and their implications in computer vision applications. To further enable the newcomer to grasp such concepts quickly, we provide companion Python notebooks, covering key aspects of this article and implementing them, step-by-step with TensorLy."
9155637,Value and Strategy of Anime Elements in the Propaganda of COVID-19 Epidemic Situation based on Computer Vision Assisted Systems,"Value and strategy of anime elements in the smart propaganda of the COVID-19 epidemic situation based on the computer vision assisted systems is analyzed in this paper. During the COVID-19 epidemic, the in-depth research on the popular cartoons and animation epidemic and the content of Internet has found that the acceptance of the people is high, and there are also many people who respond to the comics and the content of the illustrations is difficult to understand. After an in-depth investigation and analysis of this phenomenon, it was found that the inconsistent style of the picture, the vagueness of the picture-oriented and indicative, and the messy text layout are the key factors that cause the audience to not understand. Hence, this paper combines the computer vision and computer systems to construct the intelligent propaganda strategy. The computer processing of graphic images has a certain connection. In actual operation, the combination of graphics and image technology can reduce pressure of data operations in image operations and improve the visual effects of pictures. This property is used to show the performance of the proposed method."
8754082,Computer Vision Based Systems for Human Pupillary Behavior Evaluation: A Systematic Review of the Literature,"Analyzing human pupillary behavior is a noninvasive and alternative method for assessing neurological activity. Changes in this behavior are correlated with various health conditions, such as Parkinson's, Alzheimer's, autism and diabetes. Examining pupil behavior is a simple, low-cost method that can be used as a complementary diagnosis in comparison with other neurological evaluation methods. This approach is made by recording the pupillary behavior against light stimuli and measuring the pupil diameter through the video. The relation of pupillometry with digital image processing creates a dependency for computer vision based systems. Therefore, this paper presents a systematic review of the literature (SRL) conducted in order to analyze the progress of pupillometry systems based on computer vision. The main goal was to establish the state of art and identify possible gaps."
9326651,Mobile phone label online detection system based on machine vision,"Each mobile phone has its own information, such as the manufacturer, model, IMEI code, S/N code, network access permission, etc., which are implanted in the mobile phone at the factory or pasted on the mobile phone shell, generally The position of the back cover. This article studies whether the information is accurately pasted to the designated location before leaving the factory, and whether the pasting quality meets the requirements. The research is based on machine vision technology, and the system hardware design mainly includes industrial computer, PLC control system, mobile phone transmission platform, and vision acquisition system. The software design of the detection and recognition system is based on the C++ programming language and the Halcon vision library. It uses median filtering and Gaussian pyramid algorithm to filter to eliminate the effects of electronic noise, camera shake and temperature rise; use Hough transform, 8-neighbor tracking algorithm and least square Multiplication for rectangular area detection and recognition; use of gray scale transformation and binarization to complete character segmentation and recognition."
8735546,"Analyzing the Energy-Efficiency of Vision Kernels on Embedded CPU, GPU and FPGA Platforms","This paper presents a benchmark of the energy efficiency of a wide range of vision kernels on three commonly used hardware accelerators for embedded vision applications: ARM57 CPU, Jetson TX2 GPU and ZCU102 FPGA, using their vendor optimized vision libraries: OpenCV, VisionWorks and xfOpenCV. Our results show that the GPU achieves an energy/frame reduction ratio of 1.1-3.2× compared to CPU and FPGA for simple kernels. While for more complicated kernels, the FPGA outperforms the others with energy/frame reduction ratios of 1.2-22.3×. It is also observed that the FPGA performs increasingly better as a vision kernel's complexity grows."
9577955,The Affective Growth of Computer Vision,"The success of deep learning has led to intense growth and interest in computer vision, along with concerns about its potential impact on society. Yet we know little about how these changes have affected the people that research and practice computer vision: we as a community spend so much effort trying to replicate the abilities of humans, but so little time considering the impact of this work on ourselves. In this paper, we report on a study in which we asked computer vision researchers and practitioners to write stories about emotionally-salient events that happened to them. Our analysis of over 50 responses found tremendous affective (emotional) strain in the computer vision community. While many describe excitement and success, we found strikingly frequent feelings of isolation, cynicism, apathy, and exasperation over the state of the field. This is especially true among people who do not share the unbridled enthusiasm for normative standards for computer vision research and who do not see themselves as part of the ""incrowd."" Our findings suggest that these feelings are closely tied to the kinds of research and professional practices now expected in computer vision. We argue that as a community with significant stature, we need to work towards an inclusive culture that makes transparent and addresses the real emotional toil of its members."
9151022,An Analytical Framework for Trusted Machine Learning and Computer Vision Running with Blockchain,"Machine learning algorithms often use data from databases that are mutable; therefore, the data and the results of machine learning cannot be fully trusted. Also, the learning process is often difficult to automate. A unified analytical framework for trusted machine learning has been presented in the literature to address both issues. It is proposed building a trusted machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts on blockchain are used to automate the machine learning process. However, in such a blockchain framework, data efficiency is a big concern, because it is very expensive to store a large amount of data on blockchain. On the other hand, machine learning-based computer vision systems often rely on a lot of data. Therefore, to fully leverage a blockchain-based machine learning framework for computer vision systems, data efficiency issues must be addressed. This paper investigates how to enhance data efficiency in such a framework to bring computer vision systems to the edge. It presents a three-step approach. First, a lightweight machine learning model is trained on the server layer. Second, the trained model is saved in a special binary data format for data efficiency. Finally, the streaming layer takes these binary data as input and scores incoming new data in an online fashion. Real-time semantic segmentation for autonomous driving is used as an example to demonstrate how this approach works. This paper makes the following contributions. First, it improves the analytical framework for trusted computer vision systems based on blockchain. Second, the real-time semantic segmentation example shows how data-efficient learning for computer vision can be performed on the edge."
9160169,Vision Maker: An Audio Visual And Navigation Aid For Visually Impaired Person,"People with low vision or complete loss of vision face challenging task to meet their daily demand. The barrier low vision hinders them participating in the society. The evolution of computer vision, artificial intelligence and machine learning proved to effective tool in revitalizing the situation of blind. The propounded design represents implementation of an assistive device that can aid them in recognizing the object. The low-cost device runs a pre-trained model (ssdlite_mobilenet_v2_coco) and can identify up to 80 classes. The user can read any text (English) from images. Ultrasonic sensors have been used in the custom-made device to continuously alert the user of obstacles from all directions. Additionally, it can help navigate them in outdoor using Google map API. The user can read news, listen to music and mail to the member of choice. The command is passed and received through earphone, which is connected in the audio jack of raspberry pi."
9370642,Robot for Ball Fetch-and-Carry with Computer Vision in Deep Learning,"A robot which functioned as fetch-and carry ball sever was implemented. The structure of this robot are (1) Raspberry Pi for processor; (2) a camera for computer vision; (3) a 4-wheel model car for carry; and (4) robot arms for the mechanism of fetching ball. The computer vision in deep learning technology makes the robot pick the balls quickly. For the safety, the navigation and anti-collision modules were embedded in the robot. The results presented that the number of successful fetch-and-carry is in the range of 69 to 88 times in the competition of working in 100 minutes to pick the table tennis balls. Therefore, the technology can be developed to be a fetch-and-carry server for other balls in the near future."
9674699,A Robot 3D Grasping Application Based on Binocular Vision System,"3D vision system plays an essential role in robot grasping applications in complex environments. Six-degree-of-freedom pose estimation for weakly textured targets is a current research hotspot in the field of machine vision. This paper proposes a method based on feature region distribution to optimize the feature matching point pairs. This paper presents a global feature constraint-based target pose estimation method, enabling the visual system to filter the target to grasp complex backgrounds and stacking situations. Finally, we validate the effectiveness of the proposed approach through robot grasping experiments."
9419920,Automatic Multiple Choice Test Grader using Computer Vision,"This paper contains the procedure for the implementation of an application for exam test grading in a fully automatic way, putting into practice the resources that artificial vision makes available. This application allows you to grade an exam that has been designed in a pre-established format and that has subsequently been solved by the evaluated person. The qualification process is possible through the comparison between the test already solved with the correct answers and the test that the evaluated person has completed. Python and OpenCV were used for the development of the application, the latter was necessary in the image analysis and processing stage, where the objective of the application was based on the recognition of contours and marks detected in the sample photographs. This project proposes a viable and practical option in the optimization of the exam test qualification process, saving time, and making teacher performance more efficient."
9574286,Design and Implementation of Human Computer Interactive Gesture Recognition System,"With the continuous development of science and technology, computer technology is also further improved, more and more operating instructions, and its functions are more comprehensive. Human computer interaction is the process that people adapt to the computer from the past to the computer constantly adapt to people. For example, human-computer interaction through graphics is attracting people to research. In this paper, raspberry PI combined with camera is used to collect the two-dimensional model data of gesture, and the feature processing of gesture information is completed. Hand gesture recognition is carried out through OpenCV image preprocessing, the boundary of input image is selected, the existence of hand is scanned, and the contour of hand is drawn to recognize and display finger. Using Spyder integrated development environment to complete the design and implementation of human-computer interactive gesture application system, which can effectively identify different types of gestures, so that it can be applied to real life to solve the needs of special environment."
8958020,Advanced Machine Vision for Space Application,The paper presents a block diagram of machine vision for space application. Space factors influencing machine vision systems are considered. Examples of space systems developed at JSC “Television Scientific Research Institute” are given. Trends in the development of machine vision for space application are outlined.
9208332,Computer Vision Techniques for Improving Structured Light Vision Systems,"In this paper, we propose computer vision techniques for 3D reconstruction and object height measurement using a single camera and multi-laser emitters which have an intersection on the projected image plane. Time-division and color division methods are first investigated. Although the color division method offers better accuracy for height measurement, it requires the laser emitters equipped with different color light, and the color division method is also sensitive to light exposure in the measurement environment. Next, a new multi-level random sample consensus (MLRANSAC) algorithm has been developed. The proposed MLRANSAC method not only offers high accuracy for height measurement but also eliminates the requirement for the laser emitters with different colors. Our experiment results have validated the effectiveness of the MLRANSAC algorithm."
9587490,Customized Human Mask-Face Recognition using Computer Vision,"looking at the present problem of coronavirus disease, it has been advised by World Health Organization to take preventive measures to safeguard ourselves. One of the main preventive measures taken by the government and WHO is to wear a face mask while traveling outside along with social distancing. On the other hand, identifying a person with a facemask is also challenging. To overcome such situations there has been an urgent requirement for the technology to step forward. The proposed work has been implemented to overcome the situation and able to recognize a person behind the mask as well. In this work, haar cascade is used for locating the coordinates of pixels of face & mouth and abstracting the pixels of face & mouth from an image. The process of identifying starts by taking an input of the image from the default camera in the RGB format and is converted to a greyscale for fast processing of the image. The proposed work with still images and also with live video streaming with an accuracy of 90%."
9418312,Social Distancing Detection Using Computer Vision,"Covid-19 is a highly contagious disease that is caused by a new coronavirus called the SARS-CoV-2. To control the spread of this disease it is imperative to maintain distance between people because it can't be possible to know at all times if a person is infected or not. Social distancing while maintaining a minimum 6-ft distance is recommended by WHO. The project aims at using Artificial Intelligence enforcing this social distancing in public places by constantly monitoring the distance between people by a video feed and alerting the responsible person so as the required actions can be taken. This video feed can be very easily collected by pre-existing infrastructure across the public places such as CCTV Cameras. This would allow us to constantly check the distance between any two individuals in the public place. The scalability of this solution is very high, as cameras are installed at almost all public places."
8725196,Learning Computer Vision using a Humanoid Robot,"This paper presents an innovative and motivating methodology to learn vision systems using a humanoid robot, NAO robot. Vision systems are an area of growing development and interest of engineering students. This approach to learning was applied in students of Master of Electrical Engineering. The goal is to introduce students the main approaches of visual object recognition and human face recognition using computer vision techniques to be embedded in a social robot and therefore he is able to interact with human beings. NAO robot as an educational platform easy to learn how to program, and it has a high sensory ability and two cameras that can capture the images for processing."
8730239,Cluster CV2: a Computer Vision Approach to Spatial Identification of Data Clusters,"This work shows a novel application based on techniques of Computer Vision and Machine Learning to identify k clusters into a data set with overlapping issue. Used in area of unsupervised data clustering, where separation between groups is tricky. Through pair-to-pair distance calculations upon original data, is gotten a Distances Matrix as representative information of data. This matrix contains visual information, then using morphological operators extract relevant features for individual identification of groups in data set. Next, matrix decomposition performed to covariance matrix, being calculation of data elements for each cluster in order to project data into a new linear space. So, overlapping and separation distances among clusters are corrected without loss information. Results present correct identification of k clusters, without loss information, and eliminating data overlap. Clustering validation metrics such as Silhouette and Precision was used to test the methodology."
9406541,Design and simulation robotic arm with computer vision for inspection process,"Different companies compete for control of the size of the commercial markets and to offer quality products. Companies are keen to examine the products and ensure that they are free of any defects or distortions before distributing them in markets in order to preserve the confidence of customers, but manual inspection is expensive and takes a lot of time. Investors tended to use modern technologies to implement the examination process. In this paper, an approach based on the association of robots with a computer vision system is proposed. A robot arm with 4 degrees of freedom is designed by SOLIDWORKS software that takes the cans into the conveyor belt then passes it to the examination room so that an image of the product is taken via camera attached to the computer and the image is processed by the LABVIEW program. The model was simulated using MATLAB, and Arduino microcontroller has been used for controlling the processes perform by the prototype. When a defective product passes a conveyor belt, the system changes the path to remove the product from the production line. The simulation and experimental results proved that the prototype is capable of grasping cans, then detecting the cans finally, taking the defective ones out of the production line. By using this technology in the product sorting process, the productivity will be increased and the quality will be enhanced in a short time. An accuracy of 96% has been accomplished by the proposed system, where of 50 samples only two samples haven't detected."
9022232,Scene Graph Contextualization in Visual Commonsense Reasoning,"Leveraging structured visual representations such as scene graphs is beneficial to high-level computer vision tasks such as captioning or visual question answering. The recent Visual Commonsense Reasoning challenge focuses on the cognition aspects of question answering. The paper also introduces a Reasoning to Cognition (R2C) architecture tailored for this problem. We propose a modification to the R2C network that takes into account the knowledge encoded in the image by extracting the scene graph, embedding the facts and attending to them."
9150972,Enabling monocular depth perception at the very edge,"Depth estimation is crucial in several computer vision applications, and a recent trend aims at inferring such a cue from a single camera through computationally demanding CNNs - precluding their practical deployment in several application contexts characterized by low-power constraints. Purposely, we develop a tiny network tailored to microcontrollers, processing low-resolution images to obtain a coarse depth map of the observed scene. Our solution enables depth perception with minimal power requirements (a few hundreds of mW), accurately enough to pave the way to several high-level applications at-the-edge."
9098277,Attention and Multi-layer Fusion for Real-time Semantic Segmentation,"Semantic segmentation is a challenging task in the field of computer vision. One of the significant difficulties is that the pixel loss caused by the downsampling process cannot be restored during the upsampling process. So now the focus of semantic segmentation research is to solve the problem of pixel loss and pixel complementation. For the pixel complementation and prediction output edge smoothing problem, this paper proposes a Gated Multi-layer Fusion Network. The structure enhances the precise restoration of pixels from two aspects: First, the parallel layer before fusion is optimized by the gating structure, so that the parallel layer provides finer pixel semantics, and its parameter can be adjusted to optimize downsampling;Second, the different layers contain different pixel semantics, the pixel semantics of different layers can be combined by multiple layers of fusion, which makes the pixel restoration be more accurate, and the edge smoothing problem of prediction results be solved better. Considering the real-time of the model, this paper adopts the small model as the basic model, and in the fusion process, the paper uses 1 × 1 convolution to reduce the dimension. For the data set with images of 512 × 512, the Mean IOU is up to 72.9%, and the speed is 43.1frames per second."
9356233,Car Detection Based Algorithm For Automatic Parking Space Detection,"In this paper, we present an object detection based algorithm to automatically map the parking spaces in a parking lot, instead of manually mapping them. The work addresses an important gap in the recent computer vision based artificial intelligence techniques to build smart parking systems. We test our approach with two of the most popular object detectors, Faster R-CNN and YOLOv4. Our results show that our approach decreases the human effort needed by up to a compelling 90%. We show that the percentage of the available parking spots that are automatically detected through our approach accumulates over time and, in theory, can approach a 100%, on a day when all the parking spots are fully occupied. In other words, the approach is designed to have its highest performance over a busy parking lot during the busiest time."
9298064,Deep Convolutional Neural Network for Decoding EMG for Human Computer Interaction,"sEMG is a promising human computer interaction approach, which has been widely used in myriads of areas. To perform sEMG classification, more and more sophisticated machine learning strategies have been developed. However, the deep neural network still has limited applications on sEMG decoding, though it has got a great success in the computer vision area. In this study, we propose a new deep learning framework to classify hand gestures based on sEMG, especially we perform convolutional neural network (CNN) on multiple-session sEMG, which is more challenging because of the time-varying biodynamics of the subjects. So we also investigate the topologies of CNN, expecting to get an optimized architecture to effectively detect the hidden features in the signals. It is shown that the proposed CNN framework in this study has a high classification accuracy for sEMG-based hand gesture recognition, and the difference of topologies has great impact on the performance of CNN. This study lays a promising foundation for multiple-session sEMG signal pattern recognition by CNN."
9390889,Review of Human Gesture Recognition Based on Computer Vision Technology,"Over the years, with continuous expansion of the application fields of intelligent video surveillance, technologies related to human gesture recognition have received more attention and become a research hotspot. This paper first shows the architecture of the intelligent surveillance system, and the corresponding computer vision task of human gesture recognition, such as target detection, feature fusion and scene understanding. Then, in order to better understand related technologies, typical methods based on statistics, template and deep learning are summarized, the characteristics and implementation of convolutional neural networks are introduced in detail. Third, some data sets are listed, and we compare existing models in different projects of human body pose estimation from performance indicators. Finally, the flow chart of crowd posture estimation is designed, with the key modules being explained, which helps better understand the mechanism of gesture recognition. Therefore, this paper has certain theoretical value and application significance."
9607559,A Computer Vision-Based Attention Generator using DQN,A significant obstacle to achieving autonomous driving (AD) and advanced driver-assistance systems (ADAS) functionality in passenger vehicles is high-fidelity perception at a sufficiently low cost of computation and sensors. An area of research that aims to address this challenge takes inspiration from human foveal vision by using attention-based sensing. This work presents an end-to-end computer vision-based Deep Q-Network (DQN) technique that intelligently selects a priority region of an image to place greater attention to achieve better perception performance. This method is evaluated on the Berkeley Deep Drive (BDD) dataset. Results demonstrate that a substantial improvement in perception performance can be attained – compared to a baseline method – at a minimal cost in terms of time and processing.
9199968,From Simulation to Reality: CNN Transfer Learning for Scene Classification,"In this work, we show that both fine-tune learning and cross-domain sim-to-real transfer learning from virtual to real-world environments improve the starting and final scene classification abilities of a computer vision model. A 6-class computer vision problem of scene classification is presented from both videogame environments and photographs of the real world, where both datasets have the same classes. 12 networks are trained from 2, 4, 8, , 4096 hidden interpretation neurons following a fine-tuned VGG16 Convolutional Neural Network for a dataset of virtual data gathered from the Unity game engine and for a photographic dataset gathered from an online image search engine. 12 Transfer Learning networks are then benchmarked using the trained networks on virtual data as a starting weight distribution for a neural network to classify the real-world dataset. Results show that all of the transfer networks have a higher starting accuracy pre-training, with the best showing an improvement of +48.34% image classification ability and an average increase of +38.33% for the starting abilities of all hyperparameter sets benchmarked. Of the 12 experiments, nine transfer experiments showed an improvement over non-transfer learning, two showed a slightly lower ability, and one did not change. The best accuracy overall was obtained by a transfer learning model with a layer of 64 interpretation neurons scoring 89.16% compared to the non-transfer counterpart of 88.27%. An average increase of +7.15% was observed over all experiments. The main finding is that not only can a higher final classification accuracy be achieved, but strong classification abilities prior to any training whatsoever are also encountered when transferring knowledge from simulation to real-world data, proving useful domain knowledge transfer between the datasets"
9711506,End-to-End Urban Driving by Imitating a Reinforcement Learning Coach,"End-to-end approaches to autonomous driving commonly rely on expert demonstrations. Although humans are good drivers, they are not good coaches for end-to-end algorithms that demand dense on-policy supervision. On the contrary, automated experts that leverage privileged information can efficiently generate large scale on-policy and off-policy demonstrations. However, existing automated experts for urban driving make heavy use of hand-crafted rules and perform suboptimally even on driving simulators, where ground-truth information is available. To ad-dress these issues, we train a reinforcement learning expert that maps bird’s-eye view images to continuous low-level actions. While setting a new performance upper-bound on CARLA, our expert is also a better coach that provides in-formative supervision signals for imitation learning agents to learn from. Supervised by our reinforcement learning coach, a baseline end-to-end agent with monocular camera-input achieves expert-level performance. Our end-to-end agent achieves a 78% success rate while generalizing to a new town and new weather on the NoCrash-dense bench-mark and state-of-the-art performance on the more challenging CARLA LeaderBoard."
9243806,A Novel Approach based on Microservices Architectures and Computer Vision to improve access to Culture Heritage,"The combination of technologies enabling the Internet of Things and advanced computer vision techniques find application in several real contexts. Among these, the tourism sector and in particular the access to artistic and cultural heritage represents a context where the interest in the use of innovative technologies is very high. In these contexts, the interest is mainly focused on improving the accessibility of artistic and cultural places and guarantee personalized contents in order to make the tourist experience unique during a cultural visit. The use of innovative technologies such as mobile devices, BLE beacons, and Cloud services is essential for achieving these objectives. Among the computer vision techniques, a particularly useful and interesting technique in the aforementioned contexts is based on the extraction of keypoints. This work summarizes the main scientific results obtained within a research project called INSTANT. The description of the system architecture has been reported and the first functional validation of the entire proposed system has been discussed through some demo labs."
9585834,Vision-based Acoustic Information Retrieval for Interactive Sound Rendering,"The planned thesis work involves adopting computer vision techniques in the process of decomposing complex scenes to recognise acoustic characteristics of space, determining physical and structural features of complex scenes. The experiments presented demonstrate applications of scene understanding techniques to game scenes and virtual reconstructions of real space to determine acoustic properties of scene geometry for automating realistic sound rendering, identifying the current state of automatic acoustic material recognition for virtual environments and proposing a novel evaluation framework to test objective and subjective accuracy against measurements from real environments. Proof-of-concept systems have been tested on state-of-the-art acoustic renderers to demonstrate their efficiency in offline procedures. Current directions are aimed at designing end-to-end pipelines for interactive, real-time applications, with the ambition of adopting computer vision to understand the acoustic space, even in contexts of dynamic geometry typical of Augmented Reality platforms, where the acoustic space is constantly updating based on the surrounding, real world."
9534447,UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments,"Synthetic data generation has become essential in last years for feeding data-driven algorithms, which surpassed traditional techniques performance in almost every computer vision problem. Gathering and labelling the amount of data needed for these data-hungry models in the real world may become unfeasible and error-prone, while synthetic data give us the possibility of generating huge amounts of data with pixel-perfect annotations. However, most synthetic datasets lack from enough realism in their rendered images. In that context UnrealROX generation tool was presented in 2019, allowing to generate highly realistic data, at high resolutions and framerates, with an efficient pipeline based on Unreal Engine, a cutting-edge videogame engine. UnrealROX enabled robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation. Nevertheless, its workflow was very tied to generate image sequences from a robotic on-board camera, making hard to generate data for other purposes. In this work, we present UnrealROX+, an improved version of UnrealROX where its decoupled and easy-to-use data acquisition system allows to quickly design and generate data in a much more flexible and customizable way. Moreover, it is packaged as an Unreal plug-in, which makes it more comfortable to use with already existing Unreal projects, and it also includes new features such as generating albedo or a Python API for interacting with the virtual environment from Deep Learning frameworks."
9613865,LightNet: A Lightweight Neural Network for Image Classification,"Image Classification is widely used in the field of computer vision that focuses on classifying the object in a given image. Lately, image classification techniques are not only restricted to computer applications but are also famous for edge devices. Convolutional Neural Networks play a crucial role in building a good image classifier. However, in order to achieve high accuracy, CNN algorithms lead to use a large number of layers that results in increasing the number of parameters and makes it difficult to implement on edge devices. To overcome this problem, a lightweight image classifier ""LightNet"" is proposed in this paper that makes use of different scales of receptive fields to extract more feature maps with fewer parameters. To examine the efficacy of the proposed lightweight classifier, it is tested on the CIFAR-10 dataset and got 90% accuracy by using only .26M parameters, which shows that the proposed Lightnet is very effective to be implemented on edge devices."
9204052,Tracking a human being via the gray local dissimilarity map,Tracking human being from real scenes has attracted great interest in the computer vision community. We aim in this paper to provide a visual tracking system that is based on a dissimilarity measure. The proposed method includes the gray Local Dissimilarity Map and the Kalman Filter. Experimental results on several image sequences illustrate that the proposed method performs well in several challenging aspects of real world scenes.
9116893,Improving Persian Digit Recognition by Combining Deep Neural Networks and SVM and Using PCA,"One of the machine vision tasks is optical character recognition (OCR) that researchers in this field are trying to achieve a high performance and accuracy in the classification task. In this paper, we have used a fine tuned deep Neural networks for Hoda dataset, which is the largest dataset for Persian handwritten digit classification, to extract valuable discriminative features. then, these features are fed to a linear support vector machine (SVM) for classification part. In the next experiment, In order to improve the accuracy and computational load, we applied the Principal component analysis (PCA) to reduce the extracted features dimensions then we fed it to SVM. To the best of our knowledge the proposed method was better than other methods in terms of accuracy measure"
9423022,Multi-frame Recurrent Adversarial Network for Moving Object Segmentation,"Moving object segmentation (MOS) in different practical scenarios like weather degraded, dynamic background, etc. videos is a challenging and high demanding task for various computer vision applications. Existing supervised approaches achieve remarkable performance with complicated training or extensive fine-tuning or inappropriate training-testing data distribution. Also, the generalized effect of existing works with completely unseen data is difficult to identify. In this work, the recurrent feature sharing based generative adversarial network is proposed with unseen video analysis. The proposed network comprises of dilated convolution to extract the spatial features at multiple scales. Along with the temporally sampled multiple frames, previous frame output is considered as input to the network. As the motion is very minute between the two consecutive frames, the previous frame decoder features are shared with encoder features recurrently for current frame foreground segmentation. This recurrent feature sharing of different layers helps the encoder network to learn the hierarchical interactions between the motion and appearance-based features. Also, the learning of the proposed network is concentrated in different ways, like disjoint and global training-testing for MOS. An extensive experimental analysis of the proposed network is carried out on two benchmark video datasets with seen and unseen MOS video. Qualitative and quantitative experimental study shows that the proposed network outperforms the existing methods."
9537054,Android Application for Posture Analysis using Tensorflow and Computer Vision,"Human posture is an important body part that indicates the fundamental structure of the human body. Hence, the aim of this project is to develop an android application that can analyze on the human body posture by capturing an image of the body and provide guide to improve it. It is decided that using Microsoft Azure AI: Custom Vision and Tensorflow as the machine learning approaches for this project is reasonable. The development of this project used the Rapid Application Development (RAD) model which consists of the requirement planning that required the collection requirements for the project from the users. The user design that needed to build prototype for deciding system design, the construction phases that focus on coding and testing, while lastly the cutover phase that last test will be conducted, and the project will be published. The expected outcome of this project is that the application can conveniently help the people that needed to improve their body posture in a more simple and effective way. In the future, this project is hoped to be enhanced with more features and reliable functionalities that improve the effectiveness of improving human body posture."
9139138,A Computer Vision Algorithm for the Digitalization of Colorimetric Lateral Flow Assay Readouts,"Lateral flow assays (LFAs) are low-cost testing tools widely used for home, point-of-care, or laboratory medical diagnostics. These tests typically use colorimetry to report the presence and the concentration of a certain physical/ biological quantity, showing the result as a color marker. This work presents a computer vision algorithm for the digitalization of LFA readouts, enabling precise and reliable results at low-cost. The algorithm receives as input an image of a sample, identifies the color marker, and computes its average color intensity. In contrast to existing algorithms, the proposed one can detect color markers that are not characterized by a predetermined precise shape, size, and position, since the topology is identified and analyzed by the algorithm itself. The evaluation of the proposed algorithm on a set of LFA strips shows correct functionality and execution time of less than a second."
8913967,Computer Vision-based Algae Removal Planner for Multi-robot Teams,"Water pollution has caused increased incidence of algal growth around the globe. Harmful algae blooms result in massive economic losses. In this paper, a multi-robot based task planner is designed to remove excessive algae from water bodies and to identify algae build-up so that prompt action can be taken against its accumulation. Computer vision is incorporated to enable algae detection and area estimation based on training, comparing, and evaluating various advanced deep learning models using our custom algae dataset. We further propose a novel algorithm for robot resource allocation between bounding boxes of detected algae based on multivariable optimization. This systematic solution is evaluated in a simulated environment, demonstrating how the robots are optimally assigned to the detected algae patches for algae removal."
9418195,Social Distancing Detection and Analysis through Computer Vision,"The widespread COronaVIrus Disease 2019 or COVID-19 infectious disease has caused global deaths of 1.5+ million with more than 66.5 million worldwide cases as of 5 December 2020. As per WHO and various studies based on mathematical models concluded that social distancing is the most potent preventive measure against covid-19 which can help bring down the rising cases and mortality rate. With more than 50 candidate vaccines under development but none being 100% effective to cure covid-19, social distancing is the most effective approach to fight this global pandemic. Inspired by this we propose a computer vision-based social distancing detector based on the Detection Transformer object detection model and applied camera calibration and perspective transformation to make the complete system independent of the camera view-point and distortion which attains balanced mean average precision (mAP) and fps score suitable for real-time environments. Extensive and meticulous experiments were done using SOTA models; EfficientDet with EfficentNet B0 and B5 backbone and DETR with resnet-50 backbone. DETR outperformed both variants of EfficientDet and was used for real-time social distancing monitoring and analysis."
9198588,A Computer Vision Based Approach for Automated Traffic Management as a Smart City Solution,"This study aims to provide a solution to the incessant land acquisition to surmount growing traffic by promoting the application of adaptable lane dividers meant to be implemented in smart cities. A flexible lane span manipulates the width of the road as a whole, avoiding the need for road expansion. Video data is obtained from cameras placed along a single stretch and is analyzed in real-time. The model uses Computer Vision, ROI (Region of Interest) based execution, exploiting both traffic speed and occupied lane area to determine traffic density. Each camera is assigned a priority value with cameras down the lane possessing higher priority. The decision of each camera constitutes the final decision. The design also adopts pattern recognition based on learning, besides real-time analysis for more conclusive results."
9396893,Septic Fruit's Maturity Inspection and Grade Evaluation Adopting Computer Vision - A Review,"A sensory quality token-like appearance of septic fruits and vegetables not only influences the market value, consumer's inclination and preference but also the interior quality to some extent. Inspection of the external quality of fruits which is performed manually is very time consuming, tedious, labor-intensive, inconsistent and error-prone. Hence, the necessity of an astute system for fruit grading and classification is perceived. For having technological advancement over the past few decades, a lot of algorithms and technical methods, powerful and scientific instruments have been revealed using a wide variety of image analysis techniques in the field of maturity classification. This paper narrates an elaborated and comparative overview of applications and latest developments of computer vision systems, various algorithms, methods and techniques for sorting and grading."
9573704,Computer Vision Based Local Vegetables Recognition,"We often travel to different countries, but we are not aware of the local vegetables of many countries. If we are aware of the local vegetables of different countries, then it is very easy for us to recognize the specific local vegetables. Since it is very important to recognize the local vegetables in our daily life. But it's challenging to identify easily & within short time. Because most of the vegetables are almost similar color, similar size and similar shapes. So through my paper I have shown the recognition process of five (5) types of local vegetables. Which will be helpful to our farmer as well as general people. In this work I proposed some machine learning methods with computer vision approach for local vegetables recognition and identification. Segmentation of local vegetable pixels is performed using K-means clustering, SVM classifier is used for image classification and feature is extracted according to two steps explicitly: Gray level Co-occurrence matrix and statistical features. For each local vegetables confusion matrix in binary format had been performed. My proposed system is much better than any other research work. Because my paper have achieved 97.97% average accuracy. Where the individual highest accuracy is 98.20% (Bottle Ground) and lowest accuracy is 97.69% (Potato). It also shows the better performance in sensitivity, precision and specificity where their values are respectively 94.90%, 94.37% and 98.77%."
9528269,CNN Based Automatic Computer Vision System for Strain Detection and Quality Identification of Banana,"Banana is amongst the most appealing and nutritious fruits worldwide, cultivated almost every part of the world round the year. Bangladesh ranks 14 th worldwide in producing this appealing fruit putting a substantial mark on the national economic growth. Classification and recognition of specific strains and identifying the quality of different agricultural products has been a challenge for mass production. With the continuous evolution of technological advancements now it has become a beneficial machine vision task to classify different strains and also determine the quality of the fruit to trash the affected ones that will minimize the loss to a great extent. In this paper, we have proposed a Convolutional Neural Network (CNN) based model that classifies five strains of different bananas namely cavendish, lady finger, shabri, green and the red banana and also identifies the rotten ones with great accuracy. We have successfully deployed the two deep learning models to find significant accuracy varying different parameters. We have also utilized the widely accepted precision, recall, F1-score and ROC evaluation metrics. The second model has outperformed the other in terms of accuracy with 93.4±0.8% and identifying the rotten bananas with an accuracy of 98.3±.8%."
9607698,The First Vision For Vitals (V4V) Challenge for Non-Contact Video-Based Physiological Estimation,"Telehealth has the potential to offset the high demand for help during public health emergencies, such as the COVID-19 pandemic. Remote Photoplethysmography (rPPG) - the problem of non-invasively estimating blood volume variations in the microvascular tissue from video - would be well suited for these situations. Over the past few years a number of research groups have made rapid advances in remote PPG methods for estimating heart rate from digital video and obtained impressive results. How these various methods compare in naturalistic conditions, where spontaneous behavior, facial expressions, and illumination changes are present, is relatively unknown. To enable comparisons among alternative methods, the 1st Vision for Vitals Challenge (V4V) presented a novel dataset containing high-resolution videos time-locked with varied physiological signals from a diverse population. In this paper, we outline the evaluation protocol, the data used, and the results. V4V is to be held in conjunction with the 2021 International Conference on Computer Vision 1 ."
8821634,Machine Vision-Based Dried Danggit Sorter,"This paper presents a machine vision system capable of sorting dried danggit as to size based on the Philippine National Standard/Bureau of Agriculture and Fisheries Producer Standards (PNS/BAFPS 68:2008 ICS 67.120.30). The sorter has two sub-systems: the size identifier and the sorting device. The size identifier uses image processing techniques to extract the length of the fish, and use this length as a basis in determining its size. The sorting device, on the other hand, has four arms that are controlled by a microcontroller. The microcontroller triggers one of the arms to place the dried danggit to the appropriate bin. Based on software quality characteristic of ISO 25010, the proposed system is perceived to be “Outstanding” by both IT and domain experts. Furthermore, the result of the experiment shows that the proposed system achieved a high level of sorting accuracy."
9022613,Layer-Wise Invertibility for Extreme Memory Cost Reduction of CNN Training,"Convolutional Neural Networks (CNN) have demonstrated state-of-the-art results on various computer vision problems. However, training CNNs require specialized GPU with large memory. GPU memory has been a major bottleneck of the CNN training procedure, limiting the size of both inputs and model architectures. Given the ubiquity of CNN in computer vision, optimizing the memory consumption of CNN training would have wide spread practical benefits. Recently, reversible neural networks have been proposed to alleviate this memory bottleneck by recomputing hidden activations through inverse operations during the backward pass of the backpropagation algorithm. In this paper, we push this idea to extreme and design a reversible neural network with minimal training memory consumption. The result demonstrated that we can train CIFAR10 dataset on Nvidia GTX750 GPU only with 1GB memory and achieve 93% accuracy within 67 minutes."
9264289,Adapting Computer Vision Algorithms to Smartphone-based Robot for Education,"Robobo is a smartphone-based educational robot used on multiple education stages. The following paper describes the development and testing of three computer vision algorithms that run natively in Robobo. They perform real-time object identification, ArUcO marker detection, and lane detection. With them, the students that use the Robobo robot can develop highly realistic projects on mobile robotics. This brings robotics teaching closer to real robotics, where advanced computer vision algorithms are widely used. The characterization of the algorithms that are presented in this document shows that their performance is very high in comparison to implementations that run on a standard computer."
9236821,Research on Application of Computer Vision Assist Technology in High-precision UAV Navigation and Positioning,"Traditional UAV human-computer interaction requires specialized equipment and professional training. Convenient and novel interaction methods are often more popular. Using ordinary cameras, the UAV gesture control system based on computer vision and deep learning is studied. Under this background, the paper proposes a new method for the navigation information extraction of UAV autonomous landing terminal based on monocular vision/INS integrated navigation; it only needs to detect two feature points on the runway and then combine the attitude of the inertial navigation system Angle information, you can directly calculate the position vector of the aircraft relative to the airport; because the straight line has obvious observability, for the feature points that are difficult to extract, you can detect the two runway edge lines and a landing threshold line, and then take the intersection of the threshold line and the edge of the runway is used as the feature point; the algorithm is deduced in detail based on the geometric principle of imaging. Finally, the landing experiment shows that the extraction and conversion of the method is feasible."
9297511,A multi-disciplinary vision-based fire and smoke detection system,"Fire is the result of chemical reactions taking place between the oxygen and the rest of the atmosphere, which on large scales causes an ecological imbalance by giving out bi-products such as smoke. More than 90 Million cases of fire incidents have been reported since 1990 causing loss of life as well as endangered and vulnerable resources. The reason most of the time is unknown, but global warming is cited as one of the factors in the case of forest fires. Through this work, we develop a multi-disciplinary system that reports the traces of fire and smoke under any isolation. The system uses Computer Vision for analyzing and detecting fire or smoke in real-time through a Deep Learning algorithm and alerts back upon found"
9442238,Grain Surface Simulator to Averiguate the Overlapping and Noise Problems on Computer Vision Granullometry of Fertilizers,"The production of food for all the population in the world became the biggest concern. The population continues to grow and the number of farmable lands has been decreasing. To make the lands more productive, fertilizers are used on a larger scale. To guarantee the quality of the product, particle size analysis are made by mechanical sieving. With the time, the wear-out of the sieving in the fertilizer industry the results of the particle size analysis will be erroneous. So the computer vision appears as an alternative that is non-invasive and less time-consuming. In this context, this paper has the objective to develop a grain surface simulator capable of generating virtual images with overlapping grains, since there is a difficulty to obtain annotated data of images of fertilizers. In order to validate the proposed simulator using a DIP algorithm, noises are added in the virtual images to compare with the reality in the industry, to show how well the particle size analysis with computer vision were handled towards adversities. The results of the overlapping analysis show that when the virtual image has a fewer number of grains, the DIP algorithm can identify the majority of grains, consequently with less error in the particle size analysis. Different noises, at different intensities, have their effects analyzed on the algorithm. As the analyzes in this study match with the reality showing the consequences, tendencies, and errors of the overlapping of grains and noises in the images, the simulator developed here matches with reality and is extremely useful to facilitate the study of complex cases of application of visual computing and digital image processing in particle size analysis of fertilizers."
9088898,Application of the computer vision system for evaluation of pathomorphological images,"Work of medical personal with images is one of sought-after skill with insufficient amount of specialists that realized in their overloading and possible. In that connection purpose of our work was implementation of the computer vision system for evaluation of pathomorphological images as pathologists are most scarce specialist in modern medicine. We performed programmed and manual study of pathomorpological slides (immunohistochemical and cytological) with application of machine vision systems for counting of selected objects and comparison with previously manual estimation. The software was written in the Python 2.7 programming language using the OpenCV library for other purposes was modified. Two features were used to determine the nuclei and cells: the characteristic color range and the ratio of the area of the object to the square of its perimeter. We obtain average relative error of the suggested soft version about 9.2%, so accuracy of detection of cancer markers is 90.8% that is sufficient for the initial examination of a patient with screening examination of large number of patients even in so difficult images as immunohistochemistry."
9619228,A Faster R-CNN Implementation of Presence Inspection for Parts on Industrial Produce,"Presence inspection has been conducted as a matter of course during acceptance/shipment and on production lines. Due to increased factory automation (FA) in recent years, vision systems and image processing technologies have been actively introduced. This paper describes the advantages of using image processing for presence inspection, and the basic principles and practical applications of presence inspection using image processing. This paper demonstrates the use of Faster R-CNN, TensorFlow object detection API, and Computer Vision for the detection of the presence or absence of an industrial part. This solution would present itself as a cost-effective method for the quality assessment phase of a production line in a factory."
9595935,Parking Space Occupancy Monitoring System Using Computer Vision and IoT,"The process of organizing and managing parking spaces presents challenges for both the public and private sectors. Many of these challenges are caused by the inability to generate new parking spaces at the same pace as population adherence to new vehicles. Constantly improving technology in the fields of sensing and machine vision allow us to create systems that offer functionalities that are impractical to be performed with low human resources in large parking operations. An example of this type of functionality is real-time parking space occupancy monitoring. The objective of this work is the development of a system to monitor the occupancy of parking spaces. To achieve this goal, computer vision related technology integrated by an IoT platform will be used. The system was validated using different scenarios with different lighting intensities. The results are very promising even in cases where there was low ambient light"
9108039,Designing of an Amphibian Hexapod with Computer Vision for Rescue Operations,"Nowadays robots have been inching towards living beings in terms of performance, agility, accuracy and performance. But there is one domain which is yet to come to limelight i.e. autonomous rescue robots. This involves a parallel use of robot design and control systems to get it executed. Basically, in simpler terms, there is a risk of life is involved in such scenarios and it's always hoped that a robot deployed for such operations be working with high precision and recall. So, in this paper we have described the design and control of an autonomous amphibian all terrain six-legged robot with adaptive gait for rescue operations. To fulfill this, we have used LiDAR and camera in parallel to assist bot in locomotion and detect the victim. Achieving all of this needs a significant amount of on-board processing power, but at the same time has to be compact enough. Keeping this in mind, NVIDIA Jetson Nano seems to be promising choice to get all the processing done onboard."
9607618,SwinIR: Image Restoration Using Swin Transformer,"Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.14∼0.45dB, while the total number of parameters can be reduced by up to 67%."
9156337,A Sparse Resultant Based Method for Efficient Minimal Solvers,"Many computer vision applications require robust and efficient estimation of camera geometry. The robust estimation is usually based on solving camera geometry problems from a minimal number of input data measurements, i.e. solving minimal problems in a RANSAC framework. Minimal problems often result in complex systems of polynomial equations. Many state-of-the-art efficient polynomial solvers to these problems are based on Gröbner basis and the action-matrix method that has been automatized and highly optimized in recent years. In this paper we study an alternative algebraic method for solving systems of polynomial equations, i.e., the sparse resultant-based method and propose a novel approach to convert the resultant constraint to an eigenvalue problem. This technique can significantly improve the efficiency and stability of existing resultant-based solvers. We applied our new resultant-based method to a large variety of computer vision problems and show that for most of the considered problems, the new method leads to solvers that are the same size as the the best available Gröbner basis solvers and of similar accuracy. For some problems the new sparse-resultant based method leads to even smaller and more stable solvers than the state-of-the-art Gröbner basis solvers. Our new method can be fully automatized and incorporated into existing tools for automatic generation of efficient polynomial solvers and as such it represents a competitive alternative to popular Gröbner basis methods for minimal problems in computer vision."
9048698,Adaptive Video Subsampling For Energy-Efficient Object Detection,"Energy-efficient computer vision is vitally important for embedded and mobile platforms where a longer battery life can allow increased deployment in the field. In image sensors, one of the primary causes of energy expenditure is the sampling and digitization process. Smart subsampling of the image-array in a manner that is task-specific, can result in significant savings of energy. We present an adaptive algorithm for video subsampling, which is aimed at enabling accurate object detection, while saving sampling energy. The approach utilizes objectness measures, which we show can be accurately estimated even from sub-sampled frames, and then uses that information to determine the adaptive sampling for the subsequent frame. We show energy savings of 18 - 67% with only a slight degradation in object detection accuracy in experiments. These results motivated us to further explore energy-efficient subsampling using advanced techniques such as, reinforcement learning and Kalman filtering. The experiments using these techniques are underway and provide ample support for adaptive subsampling as a promising avenue for embedded computer vision in the future."
9115770,Accelerated Computer Vision Inference with AI on the Edge,"Computer vision is not just about breaking down images or videos into constituent pixels, but also about making sense of those pixels and comprehending what they represent. Researchers have developed some brilliant neural networks and algorithms for modern computer vision. Tremendous developments have been observed in deep learning as computational power is getting cheaper. But data-driven deep learning and cloud computing based systems face some serious limitations at edge devices in real-world scenarios. Since we cannot bring edge devices to the data-centers, so we bring AI to the edge devices with AI on the Edge. OpenVINO toolkit is a powerful tool that facilitates deployment of high-performance computer vision applications to the edge devices. It converts existing applications into hardwarefriendly and inference-optimized deployable runtime packages that operate seamlessly at the edge. The goals of this paper are to describe an in-depth survey of problems faced in existing computer vision applications and to present AI on the Edge along with OpenVINO toolkit as the solution to those problems. We redefine the workflow for deploying computer vision systems and provide an efficient approach for development and deployment of edge applications. Furthermore, we summarize the possible works and applications of AI on the Edge in future in regard to security and privacy."
8695970,Multi-objects detection and classification using Vision Builder for autonomous assembly,"in this paper, we proposed the methods of object detection and object classification to obtain the location information of each objects on the placement mat through the state diagram process using Vision Builder for Automated inspection (AI). By using the state diagram design detect and classify object on placement mat found that the state diagram can detect and classify almost it objects, both objects with similar surface pattern and objects with similar size. The location of the objects data can be detected and classified have the accuracy is about ±0.5 millimeter. And after using this object's location data with the automation system, it was found that the robot moved to the position of the object correctly and was able to pick the object for assembly."
9010272,A Cognitive Radar for Classification of Resident Space Objects (RSO) operating on Polarimetric Retina Vision Sensors and Deep Learning,"A novel cognitive radar, operating on Polarimetric Dynamic Vision Sensor (pDVS) and deep learning principles, aimed at discriminating moving targets, based on their motion patterns, is presented. The system consists of an asynchronous event-based neuromorphic imaging sensor coupled with polarization filters which enable better discrimination; a spinning light modulating wheel, operating at varying angular frequency, is placed in front of a static object. A pipeline has been designed and implemented in order to train a neural network for motion pattern classification using event data. This pipeline first extracts features using a pre-trained convolutional neural network and then feeds these features into a single-layer long short-term memory recurrent neural network. The outcome of this study indicates that deep learning combined with pDVS principles is well suited to classify accurately motion pattern-based targets using limited set of data; thus opening the way to many innovative bioinspired-based vision applications where feature extraction is complex or precognitive vision-based applications for the detection of salient features. The proposed cognitive radar would be able to operate at high speeds and low bandwidth, while maintaining low storage capabilities, low power consumption, and high-processing speed."
8710175,Needle Entry Angle & Force: Vision-enabled Force-based Metrics to Assess Surgical Suturing Skill,"This paper describes a new set of vision-enabled force-based metrics to be used in the assessment of open surgery suturing skill. A computer vision algorithm, along with an instrumented suturing platform, enables computing metrics of mean needle entry angle, maximum needle entry force, and mean needle entry force. A preliminary experimental study including 12 subjects (5 Attendings surgeons and 7 surgical residents) is presented. Preliminary results suggest that the metrics explained in this paper might be useful in the assessment and training of open surgery suturing skill."
8819850,Automated Criminal Identification by Face Recognition using Open Computer Vision Classifiers,"This paper presents a real time face recognition using a automated surveillance camera. The proposed system consists of 4 steps, including (1)training of real time images (2)face detection using Haar-classifier (3) comparison of trained real time images with images from the surveillance camera (4)result based on the comparison. An important application of interest is automated surveillance, where the objective is to recognize people who are on a watch list. The aspiration of this paper is to compare an image with several images which has been already trained. In this paper, we represent a methodology for face detection robustly in real time environment. Haar cascading is one of the algorithm for face detection. Here we use Haar like classifiers to track faces on OpenCV platform. The accuracy of the face recognition is very high. The proposed system can successfully recognize more than one face which is useful for quickly searching suspected persons as the computation time is very low. In India, we have a system for recognizing citizen called Aadhaar. If we use this as a citizenship database we can differentiate between citizen and foreigner and further investigate whether the identified person is criminal or not."
9342226,Computer Vision based Accident Detection for Autonomous Vehicles,"Numerous Deep Learning and sensor-based models have been developed to detect potential accidents with an autonomous vehicle. However, a self-driving car needs to be able to detect accidents between other vehicles in its path and take appropriate actions such as to slow down or stop and inform the concerned authorities. In this paper, we propose a novel support system for self-driving cars that detects vehicular accidents through a dashboard camera. The system leverages the Mask R-CNN framework for vehicle detection and a centroid tracking algorithm to track the detected vehicle. Additionally, the framework calculates various parameters such as speed, acceleration, and trajectory to determine whether an accident has occurred between any of the tracked vehicles. The framework has been tested on a custom dataset of dashcam footage and achieves a high accident detection rate while maintaining a low false alarm rate."
9231673,Product Stock Management Using Computer Vision,"Good management of the supply of products at a supermarket is crucial to help the staff working effectively. The information about product availability in real-time is needed to know when a product needs to be updated, either layout or refill So that the product is always available on the shelf when customers require it This research focuses on the display management of the product in a supermarket, which is to find out which goods are nearly empty and misplaced. We used a camera, installed in front of the rack, to capture all displayed product on the rack. Deep learning was employed to detect and recognize each product All detected products were then compared with a preconfigured product mapping that was previously prepared by the supermarket's manager. The results of the products detection and recognition are then informed to the responsible staff. The product's existence is correct if the product matches the place of mapping. With this system, refilling products can be easier for staff and customers can easily find the items they seek. The progress in collecting image datasets, labeling them, taking pictures on shelves, and recognizing products using tiny YOLOv3 has been made. The shelf segmentation process uses virtual lines that are used horizontally to recalculate the number of product lines. The accuracy of 97.61% for product recognition and almost empty detection, 76.67% accuracy for misplacement detection."
9297610,Computer Vision based use of Robotics for Teaching,"Innovative Students analyzing the applications of robots want to have each experimental and theoretical understanding in a selection of regions. Those regions embody perception sensors, three-d area, transforms among organize frames, and deciphering factor-cloud data. An outstanding device for them to acquire the critical factor is a collaborating show display screen. The human-system interface makes the challenge rely greater on the motion. At present, 3-D sensor is used typically via the robotics neighborhood (kinectv2). This sensor is positioned above the display and turned around with a purpose to experience human workout in the front of the display. A design proposal is detected in the nearest object interior to the front of the screen. To strive this, college students need to compute the extensively alternate of the coordinate body of the sensor and body of the display. Several strategies are applied to execute this process successfully. One in each and every usage of the aruco markers, they favor to devise and verify imperative mathematical technique and take a look at their answer. Subsequently, they look at to work with factor-cloud information. They choose to flip out to be conscious of the nearest man or female reputation subsequent in the front of the screen. They may additionally be capable to gain this through calculating the plane equation of the ground in mixture with inferring the records from the component of view of the coordinate body of the screen. The thresholding operations which are interior to the scanned factor-cloud are studied. Finally, an algorithm is developed for the interplay between the segmented character and the show screen. Completing this challenge represents university and college students can analyze data for higher modern robotics' programs, on the whole inside the concern of the Human-Robotic Interaction (HRD."
9070545,IDNet-A: Variant of DenseNet with Inception-Family,"A lot of interest in deep learning and advances in computer hardware (especially GPU) has recently led to many studies on network architecture in various fields. With these studies, the technology using machine learning in various fields shows good performance. Especially in the field of computer vision, solutions using convolutional neural networks (CNNs) are becoming overwhelming, with much better performance than solutions using image processing algorithms. In image recognition of computer vision, various network architectures using CNNs have been introduced and developed. In reference to previous studies, we introduce IDNet-A in this paper, which combines two impressive and powerful networks (DenseNet and Inceptionfamily). We studied how to increase the size of network to get good performance with increased representational power. We applied the Inception Module concept of Inception-family and Dense Connectivity of DenseNet to IDNet-A to efficiently increase the size of the network at a reasonable cost. As a result, we can train well in deep network architecture. We constructed several models with different hyperparameters and experimented with CIFAR datasets. Finally, IDNet-A, which we introduce in this paper, increases the size of the network by increasing the depth and width appropriately and achieves good performance with fewer parameters compared to other networks."
8754189,A Disparity Computation Framework,"A disparity map is a key component of stereo vision systems. Autonomous navigation, 3D reconstruction and mobility are examples of areas that use disparity maps as an important element. Although much work has been done in the stereo vision field, it is not easy to build stereo systems with concepts such as reuse and extensible scope. In the present paper, we contribute to reducing this gap by presenting a software architecture that can accommodate different stereo methods through a new standard structure. Firstly, we introduce scenarios that illustrate use cases of disparity maps, and we show a novel architecture that foments code reuse. A Disparity Computation Framework (DCF) is presented and how its components are structured regarding compartmentalization are discussed. Then, we introduce a prototype that closely follows our proposal, and we describe some test cases that were performed. We conclude that the DCF can satisfy different on-demand scenarios and that it can support new stereo methods, functions, and evaluations for different applications without much effort."
9270445,A simple calibration method for line-structured light vision sensor based on planar target of different positions,"Accurate, efficient and robust calibration for the line-structured light vision sensor in 3D vision measurement is a significant procedure for that it is the foundation of the subsequent 3D-reconstruction. A simple and high-precision calibration method for the line-structured light vision sensor based on the planar target is proposed in this paper. The presented approach consists of three main steps. In the first step, the camera is calibrated to obtain the intrinsic parameters. In the second step, the camera coordinate of the light plane feature point is acquired through combining the constraint condition between the camera coordinate and pixel coordinate on the perspective projection model of the camera with the special equation of the planar target. In the third step, the light plane is fitted with the obtained light plane feature points based on the least square fitting. Compared with the cross-ratio invariability method, the proposed method achieve calibration with simply extracting the centerline of the light stripe. A calibration accuracy of 0. 080mm is achieved using the proposed method, which is comparable to that of the cross-ratio invariability method. This calibration method satisfies the requirement of high-accuracy for 3D vision measurement."
9249100,Recognizing Fractal Behavior in Jackson Pollock Artwork through Computer Vision,"This project uses Computer Vision to verify that Jackson Pollock's drip-and-pour style paintings exhibit fractal behavior. Pseudocode provided from a different study of fractals in his artwork is converted into a working program that is easily replicable and runnable from a web browser. The Detrended Fluctuation Analysis (DFA) Algorithm is applied to a dataset of Pollock's artwork using Python and Jupytr Notebooks to verify that the output of the algorithm mirrors a power law distribution. The program converts a dataset of Pollock's drip-and-pour style paintings into black-and-white images, and saves off the luminance values into two-dimensional matrices. The algorithm is run on these matrices, and the matrices are then segmented and evaluated at increasingly magnified scales. The final results are then displayed in graphs, which are assessed against a power law distribution graph. The resulting graphs created in this project do verify that fractal behavior is observed. The program is housed in a github repository that can be uploaded and run in Google Colab. The final result is a code base that is broken into multiple sections to help any user gain an understanding of the Computer Vision algorithm."
9272188,Classification of Fresh Cocoa Beans with Pulp Based on Computer Vision,"Precision farming is one of the most trending topics nowadays and computer vision techniques are increasingly gaining momentum on this subject. On the other hand, in the cocoa industry, particularly in small farms, farmers still perform the classification of fresh cocoa beans with pulp in a traditional way, i.e. through their senses. In this work, we explain a new approach for cocoa beans with pulp classification, in order to aid in the process of removing cocoa beans pulp to efficiently estimate the quality of these beans. Our approach used morphological operations, k-means clustering, a bag of visual words as a feature extractor, and finally a support vector machine classifier. We achieved an AUC of 97.757% and an accuracy of 97.57% with a low false-positive rate of 2.46%, which demonstrates the viability of using computer vision for this task. We used a real-world dataset of 247 fresh cocoa beans images, that we collected and labeled with experienced cocoa farmers."
9626604,Near-Field Perception for Low-Speed Vehicle Automation Using Surround-View Fisheye Cameras,"Cameras are the primary sensor in automated driving systems. They provide high information density and are optimal for detecting road infrastructure cues laid out for human vision. Surround-view camera systems typically comprise of four fisheye cameras with 190°+ field of view covering the entire 360° around the vehicle focused on near-field sensing. They are the principal sensors for low-speed, high accuracy, and close-range sensing applications, such as automated parking, traffic jam assistance, and low-speed emergency braking. In this work, we provide a detailed survey of such vision systems, setting up the survey in the context of an architecture that can be decomposed into four modular components namely Recognition, Reconstruction, Relocalization, and Reorganization. We jointly call this the 4R Architecture. We discuss how each component accomplishes a specific aspect and provide a positional argument that they can be synergized to form a complete perception system for low-speed automation. We support this argument by presenting results from previous works and by presenting architecture proposals for such a system. Qualitative results are presented in the video at https://youtu.be/ae8bCOF77uY."
9249762,A Mathematical Overview Of Machine Vision,"The work is intended to introduce Machine Vision and Image Processing from a mathematical standpoint. Machine visions origins lie within image capturing, digital image processing and machine learning, however its now becoming geared into a much more powerful tool. This work will give description to mathematical methods used in the current algorithms in machine vision and image processing techniques such as digital watermarking, image compression, noise reduction, etc. In this research, we will explore how machine vision is dependent on image processing algorithms in order to get important information from an image, as well as describe some of the most reliable image processing methods such as Singular Value Decomposition (SVD). Alongside SVD, mathematical models of neural networks and how they are used in the field of machine learning will be discussed. Furthermore, we will discuss some of the applications and limitations of machine vision."
9600549,Visual Data Acquisition for Measuring Devices using Deep Learning,"Most of the measurement devices in the university labs are not computerized. Hence, unsteady measurements are difficult to capture. In order to retrieve the measured data to computers, expensive data acquisition systems are needed to link these devices to computers. To overcome this issue a cost-efficient solution is proposed. This article proposes a methodology to convert the readings of LCDs of the various measuring devices into a digital form using computer vision. The procedure is successfully implemented and the results are presented."
9577963,Uncalibrated Neural Inverse Rendering for Photometric Stereo of General Surfaces,"This paper presents an uncalibrated deep neural network framework for the photometric stereo problem. For training models to solve the problem, existing neural network-based methods either require exact light directions or ground-truth surface normals of the object or both. However, in practice, it is challenging to procure both of this information precisely, which restricts the broader adoption of photometric stereo algorithms for vision application. To bypass this difficulty, we propose an uncalibrated neural inverse rendering approach to this problem. Our method first estimates the light directions from the input images and then optimizes an image reconstruction loss to calculate the surface normals, bidirectional reflectance distribution function value, and depth. Additionally, our formulation explicitly models the concave and convex parts of a complex surface to consider the effects of interreflections in the image formation process. Extensive evaluation of the proposed method on the challenging subjects generally shows comparable or better results than the supervised and classical approaches."
9038593,Offline optical character recognition (OCR) method: An effective method for scanned documents,"Optical Character Recognition (OCR) is a major computer vision task by which characters of image are detected and recognized by comparing to training set images. Process of detecting character is one of the perplexing tasks in computer vision. This is because of input image often not correctly aligned or because of noise. This paper presents a complete Optical Character Recognition (OCR) system which is worked for English character mostly for Calibri font. This system first corrects skew of image if input image is not correctly aligned followed by noise reduction from input image. This process is passed through line and character segmentation that are passed into the recognition module and recognize characters. By experimenting with a set of 50 images, average achievement is 92%, 98% is for Calibri font. Moreover, the developed technique is computationally efficient and requires less time than other Optical character recognition system."
9019914,Offline Kannada Handwritten Character Recognition Using Convolutional Neural Networks,"Handwritten characters are still far from being replaced with the digital form. The occurrence of handwritten text is abundant. With a wide scope, the problem of handwritten letter recognition using computer vision and machine learning techniques has been a well pondered upon topic. The field has undergone phenomenal development, since the emergence of machine learning techniques. This work on a major scale devises to bridge the gap between the state-of-the-art technology, of deep learning, to automate the solution to handwritten character recognition, using convolutional neural networks. Convolutional neural networks have been known to have performed extremely well, on the vintage classification problem in the field of computer vision. Using the advantages of the architecture and leveraging on the preprocessing free deep learning techniques, we present a robust, dynamic and swift method to solve the problem of handwritten character recognition, for Kannada language. We discuss the performance of the network on two different approaches with the dataset. The obtained accuracy measured upto 93.2 % and 78.73 % for the two different types of datasets used in the work. The results obtained have been plotted and presented in the relevant sections."
9208920,Adaptive Immunohistochemical Image Pre-processing Method,"In this research study, the authors analyzed immunohistochemical images used to diagnose breast cancer pathologies. The authors developed an adaptive method of immunohistochemical image pre-processing. This method allows improving the quality of immunohistochemical image processing at the following stage of segmentation and calculating the quantitative characteristics of micro-objects. The method is based on computer vision algorithms."
9474694,Can computer vision be used for anthropometry? A feasibility study of a smart mobile application,"Anthropometry is a method for measuring physical characteristics of the human body, particularly dealing with measures of size and shape of the body. These measurements can be performed using a tape measure, but new devices and software solutions already been employed in digital anthropometry. However, such tools do not enable the anthropometric evaluation to be performed automatically. In this paper, we present the NLMeasurer application for anthropometry, a mobile tool based on computer vision for identifying anatomical reference points (ARPs) and assessing the size of body segments. To evaluate the performance of the NLMeasurer, four participants were photographed and their images processed. The anthropometric measures calculated by the application, using different settings, were compared with those obtained using a tape measure. Results indicate no statistically significant difference (p > 0.05) between the methods, except in one configuration. This initial experiment was promising to reveal the feasibility of using NLMeasurer for anthropometry."
8962465,A Computer Vision Based Beamforming Scheme for Millimeter Wave Communication in LOS Scenarios,"A novel location-aware beamforming scheme for millimeter wave communication is proposed for line of sight (LOS) and low mobility scenarios, in which computer vision is introduced to derive the required position or spatial angular information from the image or video captured by camera(s) colocated with mmWave antenna array at base stations. A wireless coverage model is built to investigate the coverage performance and influence of positioning accuracy achieved by convolutional neural network (CNN) for image processing. In addition, videos could be intentionally blurred, or even low-resolution videos could be directly applied, to protect users' privacy with acceptable positioning precision, lower computation complexity and lower camera cost. It is proved by simulations that the beamforming scheme is practicable and the mainstream CNN we employed is sufficient in both aspects of beam directivity accuracy and processing speed in frame per second."
9153977,A Survey on Applications of Siamese Neural Networks in Computer Vision,"Computer Vision nowadays uses many Deep Learning techniques in order to make the computer learn data representations from images and image sequences (as in videos). One of the important tasks in this respect is learning the similarity between two given images, which can be readily accomplished by learning a similarity criterion between the images. This can be readily accomplished using Siamese Convolutional Neural Networks (Siamese CNNs). Siamese CNNs can learn a similarity criterion between various kinds of image pairs. The paper presents a survey, which deals with the study of some remarkable papers which have used Siamese CNNs and triplet nets (which are a variation of the Siamese CNNs) in order to learn how similar are two images to one another."
9522811,I Find Your Lack of Uncertainty in Computer Vision Disturbing,"Neural networks are used for many real world applications, but often they have problems estimating their own confidence. This is particularly problematic for computer vision applications aimed at making high stakes decisions with humans and their lives. In this paper we make a meta-analysis of the literature, showing that most if not all computer vision applications do not use proper epistemic uncertainty quantification, which means that these models ignore their own limitations. We describe the consequences of using models without proper uncertainty quantification, and motivate the community to adopt versions of the models they use that have proper calibrated epistemic uncertainty, in order to enable out of distribution detection. We close the paper with a summary of challenges on estimating uncertainty for computer vision applications and recommendations."
9577286,The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models,"The computer vision world has been re-gaining enthusiasm in various pre-trained models, including both classical ImageNet supervised pre-training and recently emerged self-supervised pre-training such as simCLR [10] and MoCo [40]. Pre-trained weights often boost a wide range of downstream tasks including classification, detection, and segmentation. Latest studies suggest that pre-training benefits from gigantic model capacity [11]. We are hereby curious and ask: after pre-training, does a pre-trained model indeed have to stay large for its downstream transferability? In this paper, we examine supervised and self-supervised pre-trained models through the lens of the lottery ticket hypothesis (LTH) [31]. LTH identifies highly sparse matching subnetworks that can be trained in isolation from (nearly) scratch yet still reach the full models' performance. We extend the scope of LTH and question whether matching subnetworks still exist in pre-trained computer vision models, that enjoy the same downstream transfer performance. Our extensive experiments convey an overall positive message: from all pre-trained weights obtained by ImageNet classification, simCLR, and MoCo, we are consistently able to locate such matching subnetworks at 59.04% to 96.48% sparsity that transfer universally to multiple downstream tasks, whose performance see no degradation compared to using full pre-trained weights. Further analyses reveal that subnetworks found from different pre-training tend to yield diverse mask structures and perturbation sensitivities. We conclude that the core LTH observations remain generally relevant in the pre-training paradigm of computer vision, but more delicate discussions are needed in some cases. Codes and pre-trained models will be made available at: https://github.com/VITA-Group/CV_LTH_Pre-training."
9259998,DermaDetect: A novel computer vision model for an accurate diagnosis of skin conditions and rashes,"Millions of people across the globe contract skin conditions and rashes that may irritate, clog, or inflame skin, subsequently leading to redness, swelling, burning, and itching if not treated immediately. In developing countries, there is a great need for an automated diagnosis system that would reduce manual efforts and the time consumption of dermatologists and patients. A computer-automated detection of such skin diseases is possible by taking a computer vision approach. The ensemble deep learning pipeline utilizes a 34- layer ResNet, a popular image classification convolutional neural network, to detect 11 major skin conditions with only pixels and labels as inputs. With no previously made training datasets available for the research of skin diseases, the dataset for the model was designed from scratch and involved gathering 587 total images of 11 classes of skin rashes and conditions, while also accounting for healthy skin in a miscellaneous class. The calculated average precision for my ResNet model was 0.917 with precision (strength against false positives) as 0.925 and with recall (strength against false negatives) being 0.681. My implementation of an adjusted, optimized ResNet model proved successful in that it was possible to beat state-of-the-art online architectures such as Google AutoML's API which had a slightly lower average precision of 0.887."
9027658,An Implementation of the System on Chip Control System for a FPGA-Based Computer Vision Accelerator,"In this paper, we propose a System on Chip (SoC) system to integrate a computer vision acceleration IP using Field Programmable Gate Arrays (FPGAs) and the open source autopilot system called the ArduPilot. Since our goal is to build a low cost, high efficiency and compact system, we used MicroBlaze micro controller unit (MCU) which is provided by Xilinx. Also, for the flexibility and convenience of machine control, we use the commercial Flight Control Computer (FCC) Pixhawk4 (PX4). We implemented the object tracking IP and the path planner IP for controlling the motion of the machine, and designed the MicroBlaze embedded software for Mavlink packet generation and SoC system control. The entire system was tested on the drones and confirmed that real-time flight control is possible based on the tracking results of moving objects such as vehicles and human."
9573996,Comparative Analysis of Edge Computing and Edge Devices: Key Technology in IoT and Computer Vision Applications,"Edge computing is a new emerging technology which focuses on finding computation power near Internet of Things (IoT) devices such as sensors, smart phones, and many more advanced devices. The process of creating, collecting and analysing real-time resources, in IoT gadgets that provide information. In case of edge analytics, the data is moved to the next step, where more data is obtained and analysed before non-delaying steps are taken. Computing in edge is known for its performance at moment or in software applications. Edge computing focuses on eliminating challenges of IoT devices such as congestion of networks, latency and data management. The area of computer vision also uses the edge computing technique. Edge computing processes the computer vision in order to reduce the cost of transmission of data. Edge computing also processes sample of video without sending it to the cloud or any other storage unit. This paper describes use of edge computing technology in IoT and computer vision applications. Construction of computers on edge and also provides insight into the most prominent key players in this domain. And types of peripheral devices, research challenges endured by researchers in IoT and Computer vision area."
9574462,Transformer in Computer Vision,"Transformer is widely used in Natural Language Processing (NLP), in which numerous papers have been proposed. Recently, the transformer has been borrowed for many computer vision tasks. However, there are few papers to give a comprehensive survey on the vision-based transformer. To this end, we give an in-depth review of the vision-based transformer. We conclude 15 articles covering transformers on image object detection, multiple object tracking, action classification, and visual segmentation. Furthermore, we summarize 6 related datasets for corresponding tasks as well as their metrics. We also provide a comprehensive experimental comparison to validate the strength of transformer-based methods. We provide a brief introduction to the transformer and its applications on computer vision tasks, which can help beginners to recognize it."
9498483,Age Estimation in Juveniles using Convolution Neural Network,Age estimation models have been developed by many but no one has been able to build it with much accuracy and precision. This becomes even more challenging while estimating age of juveniles. This project aims to find a solution to this challenging problem. Juvenile age estimation can be used in multiple domains. It can be used majorly to stop criminal activities affecting children and teenagers. Faces of the subjects are captured and then the images are analysed using neural networks. The project is based mainly on concepts of machine learning like computer vision and convolution neural networks. This project has enormous scope as the algorithm applied can be continuously optimised as per requirements. Convolution Neural networks are highly used for tasks involving image analysis and identification. Computer vision deals with how the computer perceives and visualises the input provided to it. The fundamental concepts used in any machine learning based image analysis are Computer vision and Convolution Neural networks. These fundamentals have been developed by researchers across the world but still have some improvements to be done. With accurate dataset and proper optimisation of algorithms it is possible to create an age estimation model more precise and accurate than the existing ones.
9498024,Sign Language Recognition Based on Computer Vision,"The study of sign language involves the intersection of many fields and disciplines. At present, the two mainstream research directions of sign language recognition are data gloves [1] and visual sign language recognition [2] . The former uses the data collected by the sensor for sign language recognition and translation, while the latter uses the camera to capture the user's hand characteristics for sign language recognition and translation. In this paper, an improved convolutional neural network (CNN) [3] and long short-term memory(LSTM) [4] neural network combined sign language recognition system, which is different from the current only for sign language recognition and translation, but also for sign language generation function is designed. For the first time, this system uses a PyQt designed GUI interface. Once in the system, users can select sign language recognition and translation capabilities, capture images via OpenCV, and then use the trained CNN neural network for special processing. The model can then identify American sign language through LSTM decisions. The user can also click the voice button, the system will be based on the user's voice to convert the corresponding gesture image into the same pixels, and write to the video file. Experimental results show that sign language recognition rate is 95.52% compared with similar algorithms [5], and sign language [6] (American sign language and Arabic numerals) is 90.3%."
9309300,"A Review on Computer Vision Technology for Monitoring Poultry Farm—Application, Hardware, and Software","The productivity and profitability of poultry farming are crucial to support its affordability issues in food security. Criteria in productivity measurement, including Feed Conversion Ratio (FCR) calculation, whereas economic management is essential for profitability. Hence, best management practices need to be implemented throughout the growth period for optimizing the poultry performance. This review provides a comprehensive overview of computer vision technology for poultry industry research. This review relies on the use of several online databases to identify key works in the area of computer vision in a poultry farm. We recommend our search by focusing on four keywords, ‘computer vision’ and ‘poultry’ or ‘chicken’ or ‘broiler’ that had been published between 2010 and early 2020 with open access provided by University Teknologi Malaysia only. All the selected papers were manually examined and sorted to determine their relevance to computer vision in a poultry farm. We focus on the latest developments by focusing on the hardware and software parts used to analyze the poultry data with some examples of various representative studies on poultry farming. Notably, hardware parts can be classified into camera types, lighting units and camera position, whereas software parts can be categorized into data acquisition and analysis software types as well as data processing and analysis methods that can be implemented into the software types. This paper concludes by highlighting the future works and key challenges that needed to be addressed to assure the quality of this technology prior to the successful implementation of the poultry industry."
9522786,CLIP-Art: Contrastive Pre-training for Fine-Grained Art Classification,"Existing computer vision research in artwork struggles with artwork’s fine-grained attributes recognition and lack of curated annotated datasets due to their costly creation. In this work, we use CLIP (Contrastive Language-Image Pre-Training) [12] for training a neural network on a variety of art images and text pairs, being able to learn directly from raw descriptions about images, or if available, curated labels. Model’s zero-shot capability allows predicting the most relevant natural language description for a given image, without directly optimizing for the task. Our approach aims to solve 2 challenges: instance retrieval and fine-grained artwork attribute recognition. We use the iMet Dataset [20], which we consider the largest annotated artwork dataset. Our code and models will be available at https://github.com/KeremTurgutlu/clip_art"
9607398,U-Net based skeletonization and bag of tricks,"Skeletonization is a process focused on providing a compact and simple representation of an object by extracting the skeleton pixels from the given shape in a binary image. This method has been widely applied in various image processing and computer vision applications. In addition to traditional approaches which are not robust and provide low accuracy results, many efforts have been made for creating deep learning based methods to overcome these disadvantages. However, skeletonization is still a new topic in the deep learning world. In this paper, we propose our solution for the Pixel SkelNetOn challenge in the third edition of the ""Deep Learning for Geometric Computing"" workshop at ICCV 2021, which includes (1) modification of U-Net architecture using the attention mechanism, (2) implementation of auxiliary task learning for a more effective training process and (3) application of several tricks for improving the skeletonization model’s performance. Our method achieved 0.8000 on the Pixel SkelNetOn validation set and second place in the leaderboard. We also release our code to facilitate future research at https://github.com/namdvt/skeletonization."
9025588,Anticipation of Human Actions With Pose-Based Fine-Grained Representations,"Anticipating an action that is about to happen allows us to be more efficient in interacting with our environment. However, prediction is a challenging task in computer vision, because videos are only partially available when a decision is to be made. Complicating the issue is that it is not always clear which of the visible activities in the scene are relevant to the action, and which ones are not. We suggest that the key to recognizing an action lies with the human actors, and that it is therefore necessary for the prediction process to attend to persons in a scene. In our work, we extract fine-grained features on visible human actors and predict the future via an L2-regression in feature space. This allows the regressed future feature to focus on the actor. Using this, the future action is classified. More specifically, the fine-grained extraction is guided by a pose prediction system that models current and future human poses in the scene. We run qualitative and quantitative experiments on the Charades dataset, and initial results show that our system improves action prediction."
9546980,Body Motion Detection Technology in Video,"The detection and recognition of human body movement involves the research direction of multi-disciplinary knowledge, including digital image processing, computer vision and other professional fields. This paper first introduces three common moving object detection algorithms: background subtraction, inter frame difference and optical flow. On the basis of frame difference method and image edge detection algorithm, a dynamic edge point preserving method is proposed to realize moving object detection. This method combines the motion information and edge information of the picture, and can obtain the edge information of moving object in video sequence more accurately, which is more suitable for human body’s body motion detection."
9581091,Analysis of Light Source Selection and Lighting Technology in Machine Vision,"With the development of the technology, more and more application based on machine vision was used in actual production. The selection of light source and lighting method is the key step in the application of machine vision, which directly affects the quality of imaging and the difficulty of subsequent visual processing. Therefore, it is very important to select the proper light source and lighting method. The selection of common light source and lighting method are discussed in this paper."
9025652,A Novel Algorithm for Skeleton Extraction From Images Using Topological Graph Analysis,"Skeletonization, also called thinning, is an important pre-processing step in computer vision and image processing tasks such as shape analysis and vectorization. It is a morphological process that generates a skeleton from an input image. Many thinning algorithms have been proposed, but accurate and fast algorithms are still in demand. In this paper, we propose a novel algorithm using embedded topological graphs and computational geometry that can extract skeletons from input binary images. We compare three well-known thinning algorithms with our method, with the experimental results showing effectiveness of the proposed method and algorithms."
9022357,Evaluating Text-to-Image Matching using Binary Image Selection (BISON),"Providing systems the ability to relate linguistic and visual content is one of the hallmarks of computer vision. Tasks such as text-based image retrieval and image captioning were designed to test this ability, but come with evaluation measures that have high variance or are difficult to interpret. We study an alternative task for systems that match text and images: given a text query, the system is asked to select the image that best matches the query from a pair of semantically similar images. The system's accuracy on this Binary Image SelectiON (BISON) task provides a robust and interpretable measure of its ability to match linguistic content with fine-grained visual structure. We gather a BISON dataset that complements the COCO dataset and use it to evaluate modern text-based image retrieval systems."
9454697,Height Estimation Clamp by using Vision System,"Automated visual inspection is one of these revolutions of industrial technologies. In today's world of technology, a manufacturer requires higher inspection speeds which can be replaced as human inspection. Therefore, the machine vision system plays an important role to develop the machine in the manufacturing process. Thus, it became necessary to develop a low-cost machine for sorting these products in an automated, accurate manner, and to make the system user friendly as possible. This paper presents a machine vision system designed to identify the height estimation to obtain the size of clamps real-time."
9427080,Drone-Based Ceramic Insulators Condition Monitoring,"This article develops a prototype quadcopter drone-based system for inspection of power-line ceramic insulators. The drone uses its onboard cameras and Raspberry Pi single-board computer to monitor the health condition of outdoor ceramic insulators. The main contribution of this article is the development of a complete quadcopter-based system prototype for overhead power-line ceramic insulators inspection. The system is capable of performing the required computer vision routines for insulator health monitoring, either onboard or on an onshore ground station computer. In the onshore mode of operation, the drone captures images as it flies and simultaneously sends them to the onshore ground station. The developed system is tested in real life on a small-scale model frame, on which insulators are mounted. The results presented in this article show that quadcopter-based insulator inspection can be carried out successfully using both onshore and onboard computer vision techniques, with acceptable quality in terms of precision and computer vision time."
9491843,Autonomous Aerial Vehicle Vision and Sensor Guided Landing,"The use of autonomous landing of aerial vehicles is increasing in demand. Applications of this ability can range from simple drone delivery to unmanned military missions. To be able to land at a spot identified by local information, such as a visual marker, creates an efficient and versatile solution. This allows for a more user/consumer friendly device overall. To achieve this goal the use of computer vision and an array of ranging sensors will be explored. In our approach we utilized an April Tag as our location identifier and point of reference. MATLAB/Simulink interface was used to develop the platform environment."
9339254,Defect detection of energized grid based on Machine Vision,"In recent years, automatic detection methods based on Machine Vision have been widely used in industrial field which not only save manpower effectively but also improve product quality and detection efficiency. The energized grids produced in the industry are large in size and have a great number of connection points. Due to the poor reliability and low efficiency of traditional manual detection, a method for defect detection of energized grids based on Machine Vision is proposed. At first, the images obtained by the network camera are corrected and cropped. Then enhance the image features and extract the region of interest by image processing technology. On this basis, detect defects through the Rhombus-based Point Correction (RSPC) algorithm and the Points Rearrangement (PR) algorithm. Experimental results show that the solution proposed can detect the defects of energized grid with an accuracy rate of about 97.5% and meet the needs of industrial production."
9526949,Combing Color Index and Region Growing with Simple Non-iterative Clustering for Plant Segmentation,"Plant segmentation is an important application of computer vision processing technology in agriculture. Relying on plant segmentation technology, many crop problems can be significantly discovered and prevented. At the same time, plant conditions can be quickly and accurately understood, thereby increasing yield. This paper proposes a plant segmentation method based on the combination of color index and region growing. Threshold on the color index image produces object mask and background mask. The rest of pixels are segmented by region growing with the mask as seed points. The region growing is implemented with a simple non-iterative clustering method. The experimental results on four sets of data sets show that the accuracy has been improved with our method compared to index-based method."
9182453,A Novel Approach to Tennis Game Analysis and Emulation using Computer Vision,"Continuous practice, analysis and game experience are essential to tennis players hoping to improve their game. While practice for tennis players is being made available through thousands of academies around the world, game analysis and game experience are lacking. In this paper, we propose a method of obtaining a set of statistics quantifying a tennis player's game from a video using an optimized version of a state-of-the-art computer vision algorithm to detect the position of the ball in a frame in real-time and infer the state of the game through a series of such detections. We also propose a system that is capable of virtually simulating a player's game in real-time using a graphics processing unit. This virtual simulation can be extended to real life with the use of three ball shooting machines, a camera, and a processing unit. We are able to obtain the statistics with an accuracy of 70% on a test set of two videos consisting of tennis rallies."
8606938,Recent Advances in 3D Data Acquisition and Processing by Time-of-Flight Camera,"Three-dimensional (3D) data acquisition and real-time processing is a critical issue in an artificial vision system. The developing time-of-flight (TOF) camera as a real-time vision sensor for obtaining depth images has now received wide attention, due to its great potential in many areas, such as 3D perception, computer vision, robot navigation, human-machine interaction, augmented reality, and so on. This paper survey advances in TOF imaging technology mainly from the last decade. We focus only on recent progress of overcoming limitations such as systematic errors, object boundary ambiguity, multipath error, phase wrapping, and motion blur, and address the theoretical principles and future research trends as well."
9611778,Wearable Sensing System to perform Realtime 3D posture estimation for lower back healthcare,"Wearable sensing devices have a huge potential in developing innovative solutions for healthcare problems. Lower back or lumbar-pelvic movement monitoring is proven to be an effective method in tackling back pain problems. Significant applications of wearables in postural control therapies are crucial in the prevention of low back pain. These methods reduce costly hospitalization and allow the self-rehabilitation process. This research paper introduces a novel wearable sensing system to perform real-time 3D posture estimation for lower back healthcare. The real-time 3D system is divided into three components: a wearable device at the lumbar region; a wearable watch device; and a computer vision unit. These devices use BLE (Bluetooth Low Energy) technology for wireless communication. The wearable units utilise inertial measurement unit (IMU) sensors to perform position and orientation measurements of the body. The motion sensors are compact, cost-effective, and low power, which makes them more accessible and useful. The sensing system is developed and calibrated using practical data and cross-verified using real-world datasets. The computer vision system effectively estimates the human posture using enhanced machine learning algorithms with >97% accuracy. The research focuses on estimating the correct postures using dual-IMU and computer vision data. The paper conclusively proposes the development of a DNN based approach in analysing and improving body posture accuracy."
9522930,Adaptive Spatial-Temporal Fusion of Multi-Objective Networks for Compressed Video Perceptual Enhancement,"Perceptual quality enhancement of heavily compressed videos is a difficult, unsolved problem because there still not exists a suitable perceptual similarity loss function between two video pairs. Motivated by the fact that it is hard to design unified training objectives which are perceptual-friendly for enhancing regions with smooth content and regions with rich textures simultaneously, in this paper, we propose a simple yet effective novel solution dubbed ""Adaptive Spatial-Temporal Fusion of Two-Stage Multi-Objective Networks"" (ASTF) to adaptive fuse the enhancement results from networks trained with two different optimization objectives. Specifically, the proposed ASTF takes an enhancement frame along with its neighboring frames as input to jointly predict a mask to indicate regions with high-frequency textual details. Then we use the mask to fuse two enhancement results which can retain both smooth content and rich textures. Extensive experiments show that our method achieves a promising performance of compressed video perceptual quality enhancement."
9573964,3D Object Detection and Tracking Methods using Deep Learning for Computer Vision Applications,"3D multi-object detection and tracking is an essential constituent for many applications in today's world. Object detection is a technology related to computer vision and image processing that allows us to detect instances of certain classes. There are numerous applications like robotics, autonomous driving and augmented reality. A bounding box often defines the region of interest and then is used to classify into respective categories. Due to identical appearance and shape of various objects and the interference of lighting and shielding, object detection has always been a challenging problem in computer vision. Conventional 2D object detection yields four degrees of freedom axis-aligned bounding boxes with centre (x, y) and 2D size (w, h), the 3D bounding boxes generally have 6 Degrees of freedom: 3D physical size (w, h, l), 3D centre location (x, y, z). 2D object detection and tracking methods do not provide depth information to perform essential tasks in various computer vision applications. One among them is the Autonomous driving. 3D object detection includes depth information that provides more information on the structure of detected object. More information is required to make decisions accurately in different fields where 3D object detection and tracking can be applied. In this paper various 3D object detection and tracking methods are elaborated for various computer vision applications, this includes various fields such as robotics, driving, space field and also in the military."
8576508,Recent Advances of Generative Adversarial Networks in Computer Vision,"The appearance of generative adversarial networks (GAN) provides a new approach and framework for computer vision. Compared with traditional machine learning algorithms, GAN works via adversarial training concept and is more powerful in both feature learning and representation. GAN also exhibits some problems, such as non-convergence, model collapse, and uncontrollability due to high degree of freedom. How to improve the theory of GAN and apply it to computer-vision-related tasks have now attracted much research efforts. In this paper, recently proposed GAN models and their applications in computer vision are systematically reviewed. In particular, we firstly survey the history and development of generative algorithms, the mechanism of GAN, its fundamental network structures, and theoretical analysis of the original GAN. Classical GAN algorithms are then compared comprehensively in terms of the mechanism, visual results of generated samples, and Frechet Inception Distance. These networks are further evaluated from network construction, performance, and applicability aspects by extensive experiments conducted over public datasets. After that, several typical applications of GAN in computer vision, including high-quality samples generation, style transfer, and image translation, are examined. Finally, some existing problems of GAN are summarized and discussed and potential future research topics are forecasted."
9039191,Moving Objects Segmentation Method for Flight Vision Systems,"In matters of ensuring the safety of the aircraft operations, it is especially important to provide the crew with the information about the runway condition and obstacles presence. The paper covers the objects segmentation method proposed for use on moving obstacles on the runway. The results of this work are relevant to the tasks of avionics, computer vision and image processing."
9038598,A Computer Vision System for Guava Disease Detection and Recommend Curative Solution Using Deep Learning Approach,"Guava disease has become a tremendous problem for the production of guava which has undeviating effect on the socioeconomic development of the farmers. This phenomenon leads to initiate an automated computer vision based guava disease detection system that may detect malicious guava and guide to early cure approaches, resulting reduction of relative economic loss. Considering the fact, in this paper we have proposed a convolutional neural network (CNN) based guava disease detection and curative suggestion providing system. We have collected the images of guava affected by anthracnose, fruit rot and fruit canker along with disease free guava from different districts of Bangladesh. In this paper, we have applied three CNN models and experimentally found that the third model has outperformed the other two with an accuracy of 95.61%. For meticulous experimentation, performance metrics like precision, recall and F1 score is evaluated and found to yield great results."
9451771,Computer Vision System for Railway Track Crack Detection using Deep Learning Neural Network,"For better inspections and security, we need an efficient railway track crack detection system. In this research, we present a computer vision-based technique to detect the railway track cracks automatically. This system uses images captured by a rolling camera attached just below a self-moving vehicle in the railway department. The source images considered are the cracked and crack-free images. The first step is pre-processing scheme and then apply Gabor transform. In this paper, first order statistical features are extracted from the Gabor magnitude image. These extracted features are given as input to the deep learning neural network for differentiate the cracked track image from the non-cracked track image. Accuracy of the proposed algorithm on the procured images is 94.9 % and an overall error rate of 1.5%."
8806511,Utilization and Comparision of Convolutional Neural Networks in Malware Recognition,"Advances in Industry 4.0, IoT and mobile systems have led to an increase in the number of malware threats that target these systems. The research shows that classification via the use of computer vision and machine learning methods over byte-level images extracted from malware files could be an effective static solution. In this study, in order to detect malware, we have employed various contemporary convolutional neural networks (Resnet, Inception, DenseNet, VGG, AlexNet) that have proven success in image classification problem and compared their predictive performance along with duration of model production and inference. In addition, a novel malware data set involving 8750 training and 3644 test instances over 25 different classes was proposed and used. As a result of the experiments carried out with 3-channel (RGB) images obtained, the highest success in terms of accuracy was determined as 97.48% by using DenseNet networks."
9513940,AI & multi-resolution Temporal Processing for Accurate Counting of Exercises Repetitions,"Artificial intelligence has its importance in the field of objects' movement tracking, which is extensively applied in the fitness field. Applying machine learning technology in fitness field could introduce an auto judge by counting accurately the repetitions of any exercise. On the other hand, camera resolution depends heavily on number of frames per second (fps) which may affect the AI decision accuracy due to the possibility of losing some frames. In this paper a new algorithm relay on temporal processing for the combination of Computer Vision and deep machine learning techniques are introduced to analyze frames data accurately and report a feedback on the repetitions of exercises."
9710271,Task Switching Network for Multi-task Learning,"We introduce Task Switching Networks (TSNs), a task-conditioned architecture with a single unified encoder/decoder for efficient multi-task learning. Multiple tasks are performed by switching between them, performing one task at a time. TSNs have a constant number of parameters irrespective of the number of tasks. This scalable yet conceptually simple approach circumvents the overhead and intricacy of task-specific network components in existing works. In fact, we demonstrate for the first time that multi-tasking can be performed with a single task-conditioned decoder. We achieve this by learning task-specific conditioning parameters through a jointly trained task embedding network, encouraging constructive interaction between tasks. Experiments validate the effectiveness of our approach, achieving state-of-the-art results on two challenging multi-task benchmarks, PASCAL-Context and NYUD. Our analysis of the learned task embeddings further indicates a connection to task relationships studied in the recent literature."
9360903,Training CNNs in Presence of JPEG Compression: Multimedia Forensics vs Computer Vision,"Convolutional Neural Networks (CNNs) have proved very accurate in multiple computer vision image classification tasks that required visual inspection in the past (e.g., object recognition, face detection, etc.). Motivated by these astonishing results, researchers have also started using CNNs to cope with image forensic problems (e.g., camera model identification, tampering detection, etc.). However, in computer vision, image classification methods typically rely on visual cues easily detectable by human eyes. Conversely, forensic solutions rely on almost invisible traces that are often very subtle and lie in the fine details of the image under analysis. For this reason, training a CNN to solve a forensic task requires some special care, as common processing operations (e.g., resampling, compression, etc.) can strongly hinder forensic traces. In this work, we focus on the effect that JPEG has on CNN training considering different computer vision and forensic image classification problems. Specifically, we consider the issues that rise from JPEG compression and misalignment of the JPEG grid. We show that it is necessary to consider these effects when generating a training dataset in order to properly train a forensic detector not losing generalization capability, whereas it is almost possible to ignore these effects for computer vision tasks."
9742860,ML based Implementation for Documents Forensic and Prediction of Forgery using Computer Vision Framework,"Facial Recognition, Biometric Verification, Internet of Things (IoT), Criminal Investigation, Signature Identification in banking are just a few of the fields benefiting from breakthroughs in Computer Vision and Optical Image Processing right now. These applications prepare for dealing with numerous applications for breaking down and determining through the use of images and live video. There are several workouts that make use of computer vision, including checking, face recognition, movement recognition, and object placement. Due to advances in long-distance informal communication stages such as Facebook and Instagram, the amount of picture data being generated has increased. Image and video manipulation software usage poses a huge risk to Facebook because of the manipulated images that users submit to their social network. These sorts of pictures are habitually referred to as phony and utilized in noxious ways like rousing viciousness and passing. Need to confirm the sketchy pictures before make a move. It is extremely difficult to guarantee photograph validness because of the force of photograph controls. Picture arrangement can be dictated by picture measurable strategies. The strategy of picture duplication is utilized to hide missing regions."
9435480,State-of-the-Art Analysis of Modern Drowsiness Detection Algorithms Based on Computer Vision,"The paper presents a state-of-the-art analysis of modern drowsiness detection algorithms based on computer vision technologies as well as considers the problem of yawning detection for the vehicle driver. Based on the analysis of the literature we classify drowsiness detection techniques into three groups: the driving pattern of the vehicle; psychophysiological characteristics of drivers; and computer vision techniques for driver monitoring. The computer vision methods look most promising since they are non-intrusive for the driver. The importance of the driver drowsiness monitoring system rises from the number of drowsiness-related accidents. Yawning is an important identifier of drowsiness, even though it is not the most reliable drowsiness indicator. Some of the methods that are based on computer vision are presented and discussed in the paper. We developed and evaluated a yawning detection model. We analyzed available datasets for yawning detection and conclude that the existing datasets have to be enhanced by pictures taken in real driving conditions. We propose yawning detection dataset-preparation as well as detection model development and evaluation."
9599160,Research on the Mechanical Zero Position Capture and Transfer of Steering Gear Based on Machine Vision,"The steering gear is a very critical component in the control system of the aircraft with steering gear. In order to achieve a high-precision rudder deviation command, it is necessary to detect the mechanical zero position of the steering gear. Based on machine vision, a non-contact method for capturing the mechanical zero position of the gas steering gear was proposed, which aimed to transmit the position of the mechanical zero position to the encoder, and saved it to the system. The method includes three parts: identification, capture and transmission of the center of the mechanical zero position. The edge detection method was used to identify the center of the mechanical zero position which was captured by overlapping the center of the mechanical zero position with the pixel reference point in the pixel coordinate system. To complete the transmission of the reference, according to the relative position of the pixel reference point and the mechanical zero position to be tested, the angle inverse method was used to calculate the encoder value corresponding to the mechanical zero position. This study provides a reliable basis for the calculation of rudder deflection angle and electrical compensation value."
9512953,Research on the Strategy of Automatic Needle Knitting Based on Machine Vision and Cross-stitch Embroidery,"With the development of cross stitch market demand, machine cross stitch instead of manual cross stitch has become an inevitable trend in the development of cross stitch industry. This paper studies a fully automatic stitch knitting method based on machine vision technology and cross stitch spectrum extraction, in order to improve the automation and intelligence level of cross stitch industry and improve the quality of embroidery products."
9441670,"Synchronous System for Driver Drowsiness Detection Using Convolutional Neural Network, Computer Vision and Android Technology","One of the major reasons behind car accidents is the drowsy nature acquired by a driver while driving any vehicle. Owing to the ongoing scenario, in this project, we aim to develop a real time driver drowsiness detection system in order to detect the drivers' fatigue status, such as dozing, flickering of eye lids and time span of eye closure without having to equip their bodies with devices. The objective of this project is to build a drowsiness detection system that will detect that a person's eyes are closed for a few seconds. This system will alert the driver when drowsiness is detected. This approach is based on the Convolutional Neural Network that can be implemented on Android applications with high accuracy. Apart from CNN, computer vision also plays a major role to detect the drowsiness pattern of the driver. Cloud architecture has also proved to be beneficial in case of capturing and analyzing real time video streams."
8870369,Vehicle Tracking through Vision-Millimeter Wave Doppler Shift Fusion,"Automated cars benefit greatly from millimeter wave broadband communication links. Also, computer vision is becoming more and more used in automotive applications. In this scenario, we propose a system capable to track the position of a given car on a road by fusing data from a camera system and a wireless radio link at 60 GHz. A Gaussian mixture model and epipolar geometry have been used for computer vision. From a V-band wireless link a Doppler shift estimate has been obtained. The effectiveness of our method is shown on measurement data."
8911242,Are Object Detection Assessment Criteria Ready for Maritime Computer Vision?,"Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime setting. Thus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sight. We discuss the problem of defining assessment metrics suitable for maritime computer vision. We consider new bottom edge proximity metrics as assessment metrics for maritime computer vision. These metrics indicate that existing computer vision approaches are indeed promising for maritime computer vision and can play a foundational role in the emerging field of maritime computer vision."
8760014,Automatic Image Enhancement from a Mobile Synthetic Vision System,"The work discusses basic image contrasting algorithms and noise compensation methods, an algorithm for estimating image quality based on an integral quality indicator, as well as approaches for estimating noise values in images. The results of contrasting algorithms work (with a numerical estimation) and the most prominent image filtering methods are presented. A description is given for an automatic image enhancement algorithm from a mobile synthetic vision system based on a choice of contrasting algorithms using an integral quality indicator, and a space-time filter using a pyramidal version of Lucas-Kanade optical flow algorithm is proposed."
8659114,Toward Computer Vision Systems That Understand Real-World Assembly Processes,"Many applications of computer vision require robust systems that can parse complex structures as they evolve in time. Using a block construction task as a case study, we illustrate the main components involved in building such systems. We evaluate performance at three increasingly-detailed levels of spatial granularity on two multimodal (RGBD + IMU) datasets. On the first, designed to match the assumptions of the model, we report better than 90% accuracy at the finest level of granularity. On the second, designed to test the robustness of our model under adverse, real-world conditions, we report 67% accuracy and 91% precision at the mid-level of granularity. We show that this seemingly simple process presents many opportunities to expand the frontiers of computer vision and action recognition."
9423276,Real-Time Uncertainty Estimation in Computer Vision via Uncertainty-Aware Distribution Distillation,"Calibrated estimates of uncertainty are critical for many real-world computer vision applications of deep learning. While there are several widely-used uncertainty estimation methods, dropout inference [11] stands out for its simplicity and efficacy. This technique, however, requires multiple forward passes through the network during inference and therefore can be too resource-intensive to be deployed in real-time applications. We propose a simple, easy-to-optimize distillation method for learning the conditional predictive distribution of a pre-trained dropout model for fast, sample-free uncertainty estimation in computer vision tasks. We empirically test the effectiveness of the proposed method on both semantic segmentation and depth estimation tasks, and demonstrate our method can significantly reduce the inference time, enabling real-time uncertainty quantification, while achieving improved quality of both the uncertainty estimates and predictive performance over the regular dropout model."
9522947,Towards Computer Vision and Deep Learning Facilitated Pollination Monitoring for Agriculture,"Globally, pollinators affect 35% of agricultural land and play a key role in food production. Consequently, monitoring is useful to understand the contribution insects make towards crop pollination. Traditional sampling techniques used in insect monitoring have several drawbacks, including that they are labour intensive and potentially unreliable. Some of these drawbacks may be overcome using computer vision and deep learning-based approaches to automate pollination monitoring. In this paper, we present a pipeline for computer vision-based pollination monitoring and propose a novel algorithm, Polytrack, that tracks multiple insects simultaneously in complex agricultural environments. Our algorithm uses deep learning and fore-ground/background segmentation to detect and track in-sects. We achieved precision and recall rates of 0.975 and 0.972 respectively when monitoring honeybees foraging in our test sites within the polytunnels of an industrial straw-berry farm. Polytrack includes a flower identification module to automate collection of insect-flower interaction data, and a low-resolution processing mode that reduces computational demands placed on the processor to bring the software towards the requirements of low-powered monitoring hardware."
9384588,An Introductory Survey on Attention Mechanisms in Computer Vision Problems,"Attention mechanism is a simple method derived from the research of human vision. With the development of artificial intelligence, it is gradually applied to the field of deep learning, especially computer vision. The weight of the pixels is redistributed in the form of a mask to compose attention mechanism. Various tasks in the field of computer vision, including image classification, object detection, image semantic segmentation, and fine-grained classification, have used attention mechanism widely and significantly improved. This paper presents an introductory summary of the mechanisms in computer vision problems through recent research. Provide readers with the basics of this widely used method, explore its manifestations under different tasks, evaluate the performance of attention mechanisms, and look forward to its development trends."
9211922,Efficient Integrated Circuits Characterization Through Computer Vision Assistance,"Industry tends to optimize accuracy and time efficiency of every process analyzing its constraints and limits. Several tasks requiring high precision and reproducibility must be automated. In the context of secure Integrated Circuits (ICs) characterization, tasks such as power analysis are commonly automated. However, very few automations exist for tools calibration, while recent characterization schemes encounter mechanical constraints. Computer vision, flexible tool used in various fields, gives opportunities to address these constraints. In the case of laser fault injections, several positioning adjustments are required to ensure a maximal energy transmission in a targeted point. An accurate focalization of the laser beam is reached by using an autofocus system. Such a system is obtained by analyzing the camera view of the IC in the time-frequency domain. Whatever the method to disturb an IC, every secure characterization should be reproducible. By exploiting computer vision assistance, fault injection can be automated by mixing vision techniques to build a full or partial view of an IC and automatically identify the targeted IC and focus the perturbation on a chosen pattern in the image. A reliable pattern detection is implemented by studying spatial consistency of image’s remarkable features represented by graphs. This paper presents several computer vision techniques to address above problems."
9572544,Single-Pixel Machine Vision Using Spectral Encoding Through Diffractive Optical Networks,We present and experimentally demonstrate a deep learning-driven machine-vision framework that trains diffractive surfaces to encode the spatial information objects into the output power spectrum for all-optical image classification using a single-pixel spectroscopic detector.
9653265,On De-Interlacing and Sub-Pixel Precision Tracking,"Video cameras with interlaced scan sensors find applications in a variety of tasks such as object tracking due to their lower overhead in terms of memory and the higher sensitivity in comparison to their counterparts that employ progressive scan sensors. Such cameras, however, suffer from noticeable interlacing artefacts that need to be corrected with appropriate de-interlacing methods before the target in the video can be accurately tracked. Despite this, the effect of de-interlacing methods on the object tracking accuracy has not yet been widely studied. In this work, the first comprehensive comparison of different de-interlacing methods is carried out in the context human computer interaction studies where precise finger tracking is required. Furthermore, we propose a semiautomatic sub-pixel annotation scheme to create precise ground truth for fingertip location, allowing the analysis of the impact of de-interlacing filters on tracking at sub-pixel level. The experimental part of the work showed that the de-interlacing filter by Pixop outperformed other filters that were evaluated. Moreover, the plausible benefits of sub-pixel precise tracking over pixel precise tracking in trajectory analysis were demonstrated."
9458003,StimulEye: A Computer Vision Based Concussion Detector,"Traumatic brain injuries (TBI) are highly prevalent in contact sports, affecting the physical safety of athletes of all ages. Despite this, there is currently no affordable and objective way to test for concussions on the sidelines. Current sideline concussion tests are either trivial and subjective or overly complex and financially unfeasible. This paper presents a method for concussion detection using a smartphone. The proposed smartphone TBI test uses the camera to analyze the reactions of players' pupils when subjected to the external stimulus provided by the smartphone's LED. The results of this research show that sufficient data and accuracy are obtained by the application, StimulEye, to efficiently and accurately detect a concussion."
9532440,A Vision System for Shot Tracking and Thrown Distance Measurement,"This paper presents a vision system for the tracking of the shot and the measure of the thrown distance in a shot put competition. The pan-tilt-zoom (PTZ) camera is used to capture images of the shot and track the shot to the landing position, and the lidar is used to measure the thrown distance once the landing position is determined. A new algorithm based on Hough gradient is developed in order to detect object quickly. The algorithm is further accelerated through GPU parallel computing and implemented on NVIDIA TX2. The control of the system is realized with an onboard arm processor. The system is able to handle video steam of 75 frame per second (FPS), where the image resolution is 640 \times 480
pixels."
8899571,Human-Computer text conversation through NLP in Tamil using Intent Recognition,"Chatbots which are also known as conversational interfaces, present a new way for individuals to interact with computer systems. A Chabot allows a user to ask questions in the same manner that they would address a human. Chatbots are currently being adopted in almost all fields. Our Chatbot aims at bridging the gap between parents and educational Institutions. Proportions of parent contribution ordinarily incorporate the quality and recurrence of correspondence with educators just as investment in organization capacities and exercises. Studies show that Parent's involvement and knowledge in their ward's performance, mainly in colleges is scanty. The main reason could be a communication i.e. either communication gap between teachers and parents, or even communication with their child could be an issue. Most parents are unaware of the happenings at college. This paper focuses on developing a Chatbot that leverages NLP by using the Dialog Flow API to answers parent's queries in the native language (Tamil) thus assisting parents in keeping track of their students' performance at college. The significant components of our Chabot incorporate intent detection, routines for conversation management, dialogue design and slot fulfilment. It also uses Flask micro framework and Pusher Channels."
9036659,ACENTRIST: An Augmented Census Transform Histogram for Aerial Image Classification,"Aerial image classification has become one of the most important topics to the computer vision researchers because of its numerous real world application. A great number of census transform based descriptors have been introduced in recent years to classify the aerial images. But the major drawback of these census transform based techniques is, most of these techniques works only with the center pixel information of an image with respect to their neighboring pixels. Hence, no information about the relationship among the neighboring pixels is obtained. To mitigate this problem, we introduce an Augmented Census Transform Histogram (ACENTRIST) for aerial image classification which encodes both the center pixel information and neighboring pixel information. The proposed technique augments two local binary pattern based descriptor which encodes the center pixel information with respect to the neighboring pixels and information of the angular difference of the neighboring pixels. We have conducted thorough experiments in two of the well-known aerial image dataset, UC Merced Land Use (Land Use 21) and In-House (Banja Luka), and the experimental result shows that the proposed methodology gains considerable higher accuracy over the state of the art methods."
9484583,DeepMix: A Real-time Adaptive Virtual Content Registration System with Intelligent Detection,"This demo proposes a novel virtual content registration system (DeepMix) for MR applications, which integrates state-of-the-art computer vision technology and allows real-time interaction between virtual contents and arbitrary real objects in the physical environment for MR devices. DeepMix effectively utilizes different sensors on MR devices to measure the dimension and spatial location of real objects in the physical environment and improves the quality of experience (QoE) of users adaptively under various situations. Compared with state-of-the-art virtual content registration methods, DeepMix is a light-weight registration system with more flexibility, higher intelligence, and stronger adaptability."
9731177,Research on Sports Image Recognition and Tracking Based on Computer Vision Technology,"Human behavior recognition technology is a sub-branch of computer vision technology, which can recognize and track human actions in videos through computers, so it is widely used in sports images. Aiming at the shortcomings of low recognition accuracy of current human body recognition technology, a hybrid algorithm (S-D) is proposed by combining semi-naive Bayes classification algorithm (SNBC) and dense trajectory algorithm (DT). The research results show that the number of training times for SNBC to achieve the target accuracy is 1762 less than that of the naive Bayes algorithm; the recognition accuracy of the S-D algorithm reaches 92%. The above results prove that the S-D algorithm can effectively improve the recognition accuracy of sports images."
8953695,CRAVES: Controlling Robotic Arm With a Vision-Based Economic System,"Training a robotic arm to accomplish real-world tasks has been attracting increasing attention in both academia and industry. This work discusses the role of computer vision algorithms in this field. We focus on low-cost arms on which no sensors are equipped and thus all decisions are made upon visual recognition, e.g., real-time 3D pose estimation. This requires annotating a lot of training data, which is not only time-consuming but also laborious. In this paper, we present an alternative solution, which uses a 3D model to create a large number of synthetic data, trains a vision model in this virtual domain, and applies it to real-world images after domain adaptation. To this end, we design a semi-supervised approach, which fully leverages the geometric constraints among keypoints. We apply an iterative algorithm for optimization. Without any annotations on real images, our algorithm generalizes well and produces satisfying results on 3D pose estimation, which is evaluated on two real-world datasets. We also construct a vision-based control system for task accomplishment, for which we train a reinforcement learning agent in a virtual environment and apply it to the real-world. Moreover, our approach, with merely a 3D model being required, has the potential to generalize to other types of multi-rigid-body dynamic systems."
8992972,Tabletop Human Computer Interface to Assist Elderly with Tasks of Daily Living,"Old age is associated with declines in vision, hearing, sensory function, motor function, and cognition. These declines make managing activities of daily living harder. Computer mobile devices could help, but many elderly people find the interface too small and navigation too complex. A larger computer interface in a natural setting, such as a tabletop, could provide a more usable interface and be able to provide better assistance for some elderly people. The purpose of this project was to develop a prototype human computer interface that projects images onto a tabletop, uses an imaging system to identify hand position over the tabletop in relation to the projected image, detects menu selections of the hand over projected buttons, and takes actions, such as displaying the selected next menu. A prototype was developed using custom LabView programs for generation of images of menu selections to be projected, image processing and control of the system. Hand recognition was simplified by having the user wear a white glove with a black square on the back for the imaging system to search for. Button selection was recognized by holding the hand over a projected menu button for several seconds. The prototype function showed promise. Further testing and development will be necessary toward wider implementation. Such a projected tabletop human computer interface may improve computer derive assistance for elderly people compared to mobile or computer display interfaces."
9095147,Baseline Length Estimation Method Based on Binocular Stereo Vision Perception,"With the emergence of various 3D stereo imaging devices, stereo viewing has become more and more popular. At the same time, since the technologies of such as WEBGL are used, the design and implementation of 3D scene becomes easier. But most of the WEBGL designs are for single-eye situations. For binocular stereo vision scenarios, we should reasonably design the baseline length of left and right cameras and the related viewing distance and angle. Binocular stereo vision related papers have done a lot of research on the design of stereo vision parameters, but the relationship between baseline length, viewing distance and viewing angle of two cameras for WEBGL scene rendering combined with human eye perception and stereo scene has not been deeply discussed. This paper presents a series of relationship functions with the baseline length of rendered scene, viewing distance and viewing angle combining the binocular stereo vision and imagining theory. The relationship functions are analyzed through experiments. The experimental results show that the method has a good theoretical guidance for setting the relevant parameters reasonably."
9489043,Binocular Stereo Vision Intelligent Control Framework for the Landing Position of Quadrotor UAV Based on Fuzzy Control,"Binocular stereo vision intelligent control common framework for the landing position of quadrotor UAV based on fuzzy control is studied in this paper. The simulation system can truly reflect the changes in the position parameters of the UAV during the flight, and has the good user display and control interface. With the consideration of the vision model, the accuracy landing function is achieved. During autonomous landing, the drone always flies in the fixed altitude mode to maintain its own altitude stability. The altitude controller adopts the cascade PID control form of altitude with speed. Based on the experiment regarding the landing accuracy, it is reflected that the designed system can ensure the UAV intelligent control."
9387993,An Ultrasonic Vision based on a Neural Network for Navigating Robots Through Obstacles,"The technologies used for robot navigation have their advantages and disadvantages. To improve the technology, specialists often turn to the biological world. In this article, we implemented machine vision based on ultrasound using a neural network, similar to the ultrasound vision of a bat. The ultrasound system consists of a software and hardware installation and an artificial neural network. The use of an artificial neural network increased the accuracy of positioning, as well as provided three-dimensional vision (in 3D). This article presents the learning outcomes of a fully connected neural network and the evaluation of learning outcomes."
9665000,Vision-based Robot Handwriting Skill Reproduction and Generalization,"Robot skills transfer can increase the availability and acceptability of robots. However, the physical contact between robots and humans when teaching is unsafe. Teaching based on teleoperation can avoid contact, but the requirement of the extra input device is inconvenient. In this paper, a possible solution is provided to address the problems by proposing a novel robot skill learning framework to realize handwriting skill transfer and generalization. Instead of teaching hand-to-hand, the computer vision technique is utilized to recognize the human motion in the demonstration stage, which makes skill transfer more intuitive and natural. This approach detects continuous curves by finding the connection point in pictures written by humans, and then the continuous curves are used for teaching so that robots can learn the human skills and further generalize the learned handwriting skills. Finally, an experiment that the robot writes the Chinese character ""Yang"" is conducted to validate the effectiveness of the presented approach."
9569114,COVID-Net MLSys: Designing COVID-Net for the Clinical Workflow,"As the COVID-19 pandemic continues to devastate globally, one promising field of research is machine learning-driven computer vision to streamline various parts of the COVID-19 clinical workflow. These machine learning methods are typically stand-alone models designed without consideration for the integration necessary for real-world application workflows. In this study, we take a machine learning and systems (MLSys) perspective to design a system for COVID-19 patient screening with the clinical workflow in mind. The COVID-Net system is comprised of the continuously evolving COVIDx dataset, COVID-Net deep neural network for COVID-19 patient detection, and COVID-Net S deep neural networks for disease severity scoring for COVID-19 positive patient cases. The deep neural networks within the COVID-Net system possess state-of-the-art performance, and are designed to be integrated within a user interface (UI) for clinical decision support with automatic report generation to assist clinicians in their treatment decisions."
9615453,Workpiece Intelligent Identification and Positioning System based on Binocular Machine Vision,"In this paper, a workpiece detection and positioning system has been studied. Firstly, data sets of workpieces with three different shapes (cube, cylinder and sphere) are established, and the YOLOV3 target detection algorithm is used for deep learning to realize intelligent recognition of different shapes of workpieces. Then by the use of binocular machine vision technology, the key points position of workpieces can be successfully detected with relative error less than 2 %. This workpiece detection and positioning system can be introduced in the mechanical arm grabbing control system to make it have the visual ability similar to human in order to realize mechanical arm intelligent grabbing in complex environment."
9698669,Automatic Recognition of Indian Sign Language using Image Processing and Computer Vision,"The main objective of this work is to develop an intelligent system which will be able to take hand gestures (sign language), recognize and interpret it automatically using image processing and computer vision techniques. Several techniques are available in literature, however, to improve the accuracy of the existing recognition system, an unique method has been adopted here that uses Histogram of Gradient based algorithm for feature extraction & SVM based feature classifier. It is seen that the average recognition rate is as high as 96.3 % for 10 alphabets (A-J) and 95.05% for 10 numerals (0–9) using this technique. The proposed system has been developed in Python-3.6 and OpenCV 3.0 platform."
9315787,Cricket Activity Detection Using Computer Vision,"Nowadays the most trending and bookmark game is cricket in the whole world in which various types of activities occur like a No-ball, Wide Ball, Boundaries, etc. Here we detect a composite feature combining computer vision Algorithm along with camera view analysis. Many human errors occur in cricket matches because a wide ball or no ball creates very crucial situations and these decisions create very contradictorily during a match. Today technology is playing the most important role in the present world. So we decided that detect the various activities using computer vision techniques that occur during a cricket match like crucial catches, LBW, No ball, wide ball, etc. Here we will discuss activity detection using computer vision. Technology has various dimensions. Today the technology available is not computed the data. The technology has many different applications and magnitudes/aspect at which the software is achieving higher accuracy and greater results when the software is precisely performed. Implementation in any sport is much beneficial. Then Games such as Tennis, Baseball, Rugby, Soccer, Hockey, Cricket, Football, Kabaddi, etc. and single-player games like Chess, Badminton, Shooting, etc. are also being considered well thought out as honor to their countries."
9144058,Computer Vision for Smart Farming and Sustainable Agriculture,"Developments in satellite technology, remote sensors and drone technologies are mushrooming. These developments yield volumes of high quality scene images that require effective processing for intelligent farming applications. The recent deep learning technologies can leverage these opportunities to fuse computer vision and artificial intelligence in farming. This encompasses the big data phenomena and huge volumes of data that are captured, processed and applied for decision-making. This paper aims to give insights on the integration of computer vision for smart farming in-order to attain sustainable agriculture. Using a structured approach, this research proposes a computer vision technique for crop image feature characterization that applies in the determination of the crop's health status. To achieve this, a deep convolutional network is applied for image feature extraction and representation, and then these features are fed to the support vector-learning machine for training and subsequent image interpretation. From the experimental results, it is evident that the proposed technique generates superior visual interpretation results of scene images as compared to other methods in literature. It follows that the Global food security and agricultural sustainability can be attained through ICT enabled solutions that are integrates and works together a phenomenon referred to as smart farming."
8803607,VisionISP: Repurposing the Image Signal Processor for Computer Vision Applications,"Traditional image signal processors (ISPs) are primarily designed and optimized to improve the image quality perceived by humans. However, optimal perceptual image quality does not always translate into optimal performance for computer vision applications. We propose a set of methods, which we collectively call VisionISP, to re-purpose the ISP for machine consumption. VisionISP significantly reduces data transmission needs by reducing the bit-depth and resolution while preserving the relevant information. The blocks in VisionISP are simple, content-aware, and trainable. Experimental results show that VisionISP boosts the performance of a subsequent computer vision system trained to detect objects in an autonomous driving setting. The results demonstrate the potential and the practicality of VisionISP for computer vision applications."
9511529,Fusion of Wireless Signal and Computer Vision for Identification and Tracking,"In public safety scenarios, target objects identification and tracking is an important application, and two positioning methods including wireless and computer vision are respectively used for applications. In this article, we combine the wireless signal and computer vision, and propose a novel object identification and tracking technology. The positioning method based on computer vision helps to improve the accuracy of positioning, and we can easily distinguish different users according to wireless device information. Based on our proposed trajectory association technology, the visual trajectory is accurately matched to the corresponding wireless trajectory, and the identity of the visual trajectory is confirmed. Combined with the analysis of the position change and appearance change of visual objects, wireless positioning results are fused to correct the affected visual trajectory to improve overall system performance. A tracking system was deployed in the real world. The fusion path is proved to be closer to the real path and 90% of the errors were less than 1m. We have also implemented large-scale simulation experiments to evaluate our approach. The results show that our association algorithm has a high matching success rate and is insensitive to synchronization errors."
9208758,COVID-19 Control by Computer Vision Approaches: A Survey,"The COVID-19 pandemic has triggered an urgent call to contribute to the fight against an immense threat to the human population. Computer Vision, as a subfield of artificial intelligence, has enjoyed recent success in solving various complex problems in health care and has the potential to contribute to the fight of controlling COVID-19. In response to this call, computer vision researchers are putting their knowledge base at test to devise effective ways to counter COVID-19 challenge and serve the global community. New contributions are being shared with every passing day. It motivated us to review the recent work, collect information about available research resources, and an indication of future research directions. We want to make it possible for computer vision researchers to find existing and future research directions. This survey article presents a preliminary review of the literature on research community efforts against COVID-19 pandemic."
9400070,Trend Forecasting of Computer Vision Application in Aquaponic Cropping Systems Industry,"Computer vision is a fast-growing technology that provides exceptional impact in the world of agriculture. As different problems affecting crop cultivation emerge, as well as the increase in world population, the threat to food security becomes more significant. This paper discussed the international and local agricultural advancements. Trends of computational intelligence (CI) applications in aquaponics systems were also reviewed. This is to see the growth of the industry in terms of technological advancements and determine where the industry is heading. Computer vision allows a non-destructive way of monitoring and detecting agricultural irregularities. Integration of CI in aquaponics made the system more cost-efficient, and beneficial both for the farmers and the environment. This integration has been gaining more attention through the years and the increasing technical publications about the applications it is the significant proofs. This expansion of CI application in the Philippine farm industry can shift old manual labor into a fully automated scheme of farming. Though the Philippine agricultural growth in the past five years has been slow with 1.3% between 2016 and 2019, the introduction of CI, as well as different technologies, can help boost the industry performance especially now that the whole world is now engaging in the fourth industrial reform (FIRe). Therefore, technology intensification of computer vision in Philippine agriculture is expected to crest in the coming two decades."
9601850,Vision-Based Robotic Manipulation of Intelligent Wheelchair with Human-Computer Shared Control,"Based on human-computer shared control, this paper introduces a novel robotic manipulation fashion combining computer vision and brain-computer interface (BCI). Designed for disabled groups, the intelligent wheelchair with our proposed method exhibits the precise robotic manipulation ability but also the human decision-making capabilities, which will bring better life quality for the disabled. The overall pipeline includes three parts: asynchronous brain-computer interface based on steady-state visual evoked potential (SSVEP), vision detection with deep network and robotic manipulation of UR5 robot. Particularly, first, the user receives the periodic visual stimulation with different frequencies and then electroencephalography (EEG) signals of the user are collected by EEG cap. Second, we preprocess the EEG signals and extract feature embedding. To judge the frequency of the stimulus signals received by the user, the canonical correlation analysis (CCA) algorithm is used to fit and compare it with the standard EEG signal. In our work, the signals with different frequencies corresponds to different types of objects item by item. Third, we apply the off-the-shelfvision detection algorithm, Mask-RCNN, to output the position of the object corresponding to the detected EEG in the image frame. UR5 robot arm plan manipulation path according to the position of objects transferred by robot operating system (ROS). Extensive experiments show that our method can achieves performance with more than 90% accuracy and the user can control the robot arm to grab the expected object accurately through BCI."
9710244,3D Human Pose Estimation with Spatial and Temporal Transformers,"Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classification, object detection, and semantic segmentation. However, in the field of human pose estimation, convolutional architectures still remain dominant. In this work, we present PoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional architectures involved. Inspired by recent developments in vision transformers, we design a spatial-temporal transformer structure to comprehensively model the human joint relations within each frame as well as the temporal correlations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at https://github.com/zczcwh/PoseFormer"
9004382,Analysing and Applying Captured Object with Machine Learning Techniques,Detecting an object either form a static or dynamic image is considered to be a fundamental objective in the area of a computer vision. Object detection techniques can be applied both to static images as well as for dynamic images. Presently there subsist implementations of applications that uses object detection; this work orients itself on object detection using artificial intelligence and machine learning techniques implementing varied computer vision characteristics. The aim is to integrate state-of-the-art practice for object recognition with the intension of attaining extraordinary accurateness with an instantaneous enactment. The basic working is to identify patterns while capturing objects using camera to identify different daily tasks-and implementing them using gesturing. The final result exhibits real-time performance and satisfactory toning outcomes.
9432326,A Deep Learning approach to solve sudoku puzzle,"Computer vision has been lionized in the recent trends of information technology world. Because of its enormous application and features, it has been gaining limelight for solving real world complex problems which are beyond human intelligence. Extending this artificial intelligence marvel to conventional life makes it useful and convenient. Sudoku has become a part of amusement in many lives which is ubiquitous in many articles news papers, magazine and apps. So imbibing computer vision techniques to solve the Sudoku puzzle is upshot of this paper. We have employed some of the prominent tools like Open CV, OCR, Tensorflow. The paper explains the methodology with which we have used these tools to solve the Sudoku puzzle. The methodology shown in the paper involves several stages such as image preprocessing and image extraction using OpenCV, OCRing the numerical data from the extracted image using Tesseract and finally feeding the numerical data extracted to the neural network(tensorflow) model to get the desired output. The main motivation of our research is to develop up a help device for Sudoku players, for example urging players to tackle the hard Sudoku puzzles or when looking for help. It is troublesome at various stages and simple to artful players, particularly for new players or individuals insufficient certainty or endurance. The input data, comes from the camera or as an image."
9166095,Developing Spherical Mobile Devices for Indoor Exploration,"Computer vision systems and algorithms are designed to process digital images and extract necessary information from it. In this research we propose computer vision system consisting of several spherical mobile devices with digital camera and microcomputer inside. Device's design, basic characteristics and current results, as well as steps for further actions to improve the system are described below in the paper. At the core of image processing OpenCV library and modern convolutional neural networks such as YOLOv3, MobileNETSSDv2 are used."
9397040,Implementation of Customized Adaptive Algorithms for Visualization of Stereoscopic Face Mask,"Computer vision and Image processing are some of the most important technical aspects in the domain of object detection and reconstruction. These concepts provide solutions not only restricted to the engineering domain but also across the various domains. In this article, the proposed framework mainly deals with the scenario of the generation of face masks in 3D view with the aid of selfie images using computer vision and image processing methodologies. The proposed framework consists of two kinds of algorithms that generate face mak with the aid of single and multiple images respectively. Those adaptive algorithms are discussed along with their implementation. The adaptive algorithm utilized for multiple image input names as MIIO3D and the adaptive algorithm for single image input named as SIIO3D implementation for the construction of an efficient 3D face mask. The input images considered are from real-time aspects. Data visualization and their comparisons made with the aid of Tableau."
9719261,Image processing technology based on computer vision algorithm,"Aiming at the problem of intelligent interactive 3D display image distortion, this paper designs a display computer vision system based on ARM processor. The paper realizes the 3D dimensional reconstruction by means of high-speed projector to high-speed projection of transmissive directional scattering screen. Then this paper establishes a mathemational model for distorted images and proposes a deep learning method based on CNN model to correct the distorted images in real time during projection. The experiment shows that the effect of correction is obvious. The system has the advantages of small size, low cost, wide viewing angle, high resolution and strong human-computer interaction."
9554355,A Novel Land Use Classifier with Convolutional Recurrent Structure,"Through the development of machine learning and computer vision, image scene classification has made immense progress over the last decade. Remote sensing land use analysis remains a topic of great interest. Using deep learning methods from computer vision, we develop a novel approach that combines a convolutional structure and gated recurrent unit layers with a fully connected neural network to solve land use classification tasks. Simulation studies confirm the proposed method can more accurately classify and recognize remote sensing images in the EuroSAT dataset than current state-of-the-art algorithms."
9314659,Computer Vision Controlling an Autonomous Unmanned Aerial Vehicle Flight over a Railway,"In this work, an algorithm to control the autonomous quadcopter flight along the railway, without an operator and without the use of satellite navigation systems has been developed. The quadcopter camera image processing determines the rails position and direction and the software generates signals to control flight characteristics, such as pitch, yaw, and roll, to fly over the railway at a given height. The algorithm does not require significant computing resources, therefore its hardware does not affect the quadrocopter weight and size parameters and power consumption, and therefore does not affect the range and duration of its flight. Quadrocopter tests have confirmed the algorithm feasibility and reliability."
8519337,Person Recognition in Personal Photo Collections,"People nowadays share large parts of their personal lives through social media. Being able to automatically recognise people in personal photos may greatly enhance user convenience by easing photo album organisation. For human identification task, however, traditional focus of computer vision has been face recognition and pedestrian re-identification. Person recognition in social media photos sets new challenges for computer vision, including non-cooperative subjects (e.g., backward viewpoints, unusual poses) and great changes in appearance. To tackle this problem, we build a simple person recognition framework that leverages convnet features from multiple image regions (head, body, etc.). We propose new recognition scenarios that focus on the time and appearance gap between training and testing samples. We present an in-depth analysis of the importance of different features according to time and viewpoint generalisability. In the process, we verify that our simple approach achieves the state of the art result on the PIPA [1] benchmark, arguably the largest social media based benchmark for person recognition to date with diverse poses, viewpoints, social groups, and events. Compared the conference version of the paper [2] , this paper additionally presents (1) analysis of a face recogniser (DeepID2+ [3] ), (2) new method naeil2 that combines the conference version method naeil and DeepID2+ to achieve state of the art results even compared to post-conference works, (3) discussion of related work since the conference version, (4) additional analysis including the head viewpoint-wise breakdown of performance, and (5) results on the open-world setup."
9376035,Performance of Single Board Computers for Vision Processing,"With the increasing complexity of machine vision algorithms and growing applications of image processing, how do computers without a dedicated graphics processor perform? This research discusses the computational abilities of two low-cost single board computers (SBCs) by subjecting them to various Visual Inertial Odometry (VIO) algorithms. The end goal of this research is to identify a SBC which meets the requirements of being employed on an Unmanned Aerial System for autonomous navigation."
9633464,Fast Vision Transformer via Query Vector Decoupling,"Although transformer structure has become the defacto standard of the natural language processing task, it still has limited application in computer vision. In vision, attention is either combined with or replaces certain modules in a convolutional network, while keeping its overall framework intact. It proves that this reliance on CNNs is unnecessary for the pure transformer directly applied to image patch sequence can successfully classify the images. When large amounts of data are pre-trained and transmitted to multiple small and medium-sized image recognition benchmark (ImageNet, CIFAR-100, VTAB, etc.), the Vision Transformer (ViT) achieves remarkable results with less computational resources required than existing convolutional networks. Unlike the previous calculation that weighs attention, the model in this paper has been optimized, reducing the time complexity from O(n 2 ) to O(n • logn), largely improving the model’s speed. Therefore, a new model is developed—Fast Vision Transformer (Fast VIT)."
9668906,Assistive Diagnostic Tool for Brain Tumor Detection using Computer Vision,"Today, over 700,000 people are living with brain tumors in the United States. Brain tumors can spread very quickly to other parts of the brain and the spinal cord unless necessary preventive action is taken. Thus, the survival rate for this disease is less than 40% for both men and women. A conclusive and early diagnosis of a brain tumor could be the difference between life and death for some. However, brain tumor detection and segmentation are tedious and time-consuming processes as it can only be done by radiologists and clinical experts. The use of computer vision techniques, such as Mask R Convolutional Neural Network (Mask R CNN), to detect and segment brain tumors can mitigate the possibility of human error while increasing prediction accuracy rates. The goal of this project is to create an assistive diagnostics tool for brain tumor detection and segmentation. Transfer learning was used with the Mask R CNN, and necessary parameters were accordingly altered, as a starting point. The model was trained with 20 epochs and later tested. The prediction segmentation matched 90% with the ground truth. This suggests that the model was able to perform at a high level. Once the model was finalized, the application running on Flask was created. The application will serve as a tool for medical professionals. It allows doctors to upload patient brain tumor MRI images in order to receive immediate results on the diagnosis and segmentation for each patient. (Abstract)"
9093334,Towards Good Practice for CNN-Based Monocular Depth Estimation,"Monocular depth estimation has gained increasing attention in recent years, and various techniques have been proposed to tackle this problem. In this work, we aim to provide a comprehensive study on the techniques widely used in monocular depth estimation, and examine their individual influence on the performance. More specifically, we provide a study on: 1) network architectures, including different combinations of encoders/decoders. 2) supervision losses, including fully supervised losses and self-supervised losses and 3) other practices such as input resolution. The experiments are conducted on two commonly used public datasets, KITTI and NYU Depth v2. We also provide an analysis on the errors produced by different models, to reveal the limitations of current methods. Furthermore, by a careful redesign, we present a model for depth estimation, which achieves competitive performance on KITTI and state-of-the-art performance on NYU Depth v2. Our code is publicly available at https://github.com/zenithfang/supervised_dispnet."
9484339,High Quality Facial Data Synthesis and Fusion for 3D Low-quality Face Recognition,"3D face recognition (FR) is a popular topic in computer vision, since 3D face data is invariant to pose and illumination condition changes which easily affect the performance of 2D FR. Though many 3D solutions have achieved impressive performances on public high-quality 3D face databases, few works concentrate on low-quality 3D FR. As the quality of 3D face acquired by widely used low-cost RGB-D sensors is really low, more robust methods are required to achieve satisfying performance on these 3D face data. To address this issue, we propose a novel two-stage pipeline to improve the performance of 3D FR. In the first stage, we utilize pix2pix network to restore the quality of low-quality face. In the second stage, we launch a multi-quality fusion network (MQFNet) to fuse the features from different qualities and enhance FR performance. Our proposed network achieves the state-of-the-art performance on the Lock3DFace database. Furthermore, extensive controlled experiments are conducted to demonstrate the effectiveness of each model of our network."
9526411,Automated Proctoring System using Computer Vision Techniques,"The arrival of COVID-19 has ushered in a new era of distance learning. Since schools and universities have closed, learning has transferred to apps like Google Meet, Microsoft Teams, Zoom, and others. Almost all colleges have changed their curricula to reflect the current reality. Students' practical knowledge deteriorated as a result of the virtual form of learning, and they began attending lectures only for the purpose of attending them. With all of this, their grades and scores should ideally be declining, but the findings came as a shock. It was a fantastic turnaround. Many students outperformed their average score. This is due to the fact that there has never been a way to perform an organized evaluation without using unequal means in the online-mode of education. To address the existing problem, a system that can assist in analysing unfair tactics used by students is required. The employment of proctoring procedures is a big difficulty for the research community when it comes to online examinations. In this paper, we present how to create a complete multi-model system utilising computer vision to prevent the presence of humans throughout the examination. We propose a system that includes a variety of features that students may exploit throughout the test, such as eye gaze tracking, mouth open or close detection, object identification, and head posture estimate using facial landmarks and face detection. Our system can also transform the student's voice into a text format, which might be useful for keeping track of the words said by the student. This might aid the examiner in determining whether or not the student is speaking with someone close. In summary, this research reveals how to prevent cheating in online tests using semi-automated proctoring based on vision and audio capabilities and monitor multiple students at a time."
8869034,"Optimal Selection and Identification of Defects in Chestnuts Processing, through Computer Vision, Taking Advantage of its Inherent Characteristics","In the agro-industry automation, computer vision has become very important to the product selection and classification process. The problem becomes more challenging when it is necessary to detect defects or diseases in the product images. In literature, it was observed that when the fruit or vegetable image is treated as only one problem, efficiency is lower than when dividing it into sub-problems considering regions with similar appearance. Thus, in this paper, the target is to automate the detection and identification of visual defects in Brazil nuts by dividing the problem into two sub-problems (pulp and epidermis defects recognition) and by using color, shape and texture descriptors. First, the original image is segmented into two regions (one dark and one light). Then, First Order Descriptor, is applied to detect the presence or absence of defects in each region through the texture descriptor. Next, color, size and texture descriptors are used to the identification of each defect. This approach improves results obtained in previous research (Álvarez-Valera et al. [1]). We obtained an efficiency rate of 98.03 % with a processing time of 75 ms at worst and 51 at the best for every 3 images processed, unlike the previous attempt that had an efficiency rate of 91.79 % with a processing time of 130 ms. Finally, this approach can be applied in different types of products with other characteristics, since its inherent characteristics allows us to divide the original problem in two or more sub-problems."
9514982,Computer Vision Based Prototype Model of Face Gesture Controlled Vehicle,"Vehicles has been an integral and progressive part of urban life for the past few decades, a lot of new advancement in the field of vehicle mobility and automation is evident. The development in the field of Image Processing and the need for strategic planning in the field of smart vehicle has laid the foundation of the current research. The current work focuses on minimizing driver’s manual control, thus enhancing the existing smart features in the cars. The vehicle motion is proposed to be controlled through face gestures or face movements of the driver. A novel approach using Image Processing and Computer Vision is proposed for developing a prototype model of face gesture controlled vehicle. An algorithm based on the feature extraction technique of the image is used which is capable of matching the real time face movements of the driver and further sending the command signal to the wheels of the vehicle."
9718800,Real-Time Recognition of Bangla Sign Language Characters: A Computer Vision Based Approach Using Convolutional Neural Network,"Sign Language is the elementary communication media for Deaf & Mute (D&M) people. On the other hand, it seems too tenacious for the general people to understand this language. In order to tear out this communication barrier, a real-time automated translator is essential. Through this research, a computer vision-based approach has been developed for the recognition of Bangla Sign Language (BdSL) characters. In this work, a deep learning-based recognition model has been developed. Adaptive thresholding has been integrated with 2D Convolutional Neural Network (CNN) to construct this model. Proposed model has been trained to build this real-time automated translator through our own created dataset (dataset containing 3600 different images for 36 distinct characters). The proposed model has been trained and tested with 2880 (80%) training images and 720 (20%) testing images respectively. Thirty-six unique characters of Bangla Sign Language can be recognized through this model with significant accuracy. The model delivers validation accuracy of 99.72% and validation loss of 0.73%. A significant result has been achieved for the recognition and translation of Bangla Sign Language characters with this dataset over other existing Bangla Sign Language Recognition model."
9656269,Application of Computer Vision on Displacement Monitoring System,"Monitoring accuracy and verifiability of displacement of structure is essential for project’s safety. This paper presents a kind of design method of displacement monitoring system based on computer vision, including scale acquisition module, visual recognition module and wireless communication network. Also taking static level as example to demonstrate the key points and realization methods of designing each module, an approach to recognize the scale value from picture is proposed based on the invariance of imaging range, along with optimized methods of improving picture’s pixel and intercepting specified match region to enhance accuracy of visual recognition module’s output. Test result shows the system is capable of monitoring displacement automatically at low cost, the optimized methods could upgrade accuracy, recognition precision achieves 0.16mm, meanwhile, the visualization feature of system can be assistant method of manual monitoring to ensure the verifiability of data."
9553481,Plant Counts in Dense Red Beet Crops: A Computer Vision Approach,"Yield assessment in broadacre crops is often base on time-consuming and labor-intensive approximations. However, the emergence of unmanned aerial systems (UAS) has allowed for rapid and cost-effective data acquisition. We evaluated red beet plant counts using multispectral UAS data via computer vision and regression analysis. Flight data were captured twice during summer 2019. Our preprocessing steps included i) vegetation detection, ii) feature generation, and iii) feature selection. Partial least squares regression was used as a statistical predictor. Results showed that plant count could be predicted with an acceptable coefficient of determination ( R^{2}=0.76
for calibration; R^{2}=0.54
for cross-validation) and a low root-mean-square-error ( \text{RMSE} =12.27
plants/plot for calibration, \text{RMSE} =17.45
plants/plot for cross-validation). These results are promising, since the error margin, relative to the average density (175 plants/plot), was below 10%. Future efforts should include different geographical locations, higher resolution imagery, and more advanced approaches such as deep learning algorithms with potential for improved accuracy and precision."
9649256,Reducing the labor intensity of technological operations using computer vision system,This article discusses a key part of the enterprise computer vision system – semantic segmentation of images. Segmentation is performed using convolutional neural networks. STL model projections of real parts are used to create training and test samples. The accuracy of three convolutional neural network architectures for facet segmentation of five-part types was compared.
9711733,Vehicle Distance and Speed Estimation Algorithm for Computer Vision Sensor System,"This study is devoted to solving the perception problem of autonomous transport system by Convolutional Neural Network (CNN) application. The goal of this paper is to develop the method for single camera based computer vision sensor for intelligent transport systems to detect and recognize objects in real-time, as well as estimate the speed of detected vehicle. This paper aims at evaluating the workability of this sensor in different weather conditions for autonomous vehicle applications. This sensor can be applied for different levels of abstraction like road safety, autonomous vehicle navigation, guiding system for disabled, animal survey, surveillance and other purposes."
9072808,Classified Counting and Tracking of Local Vehicles in Manila Using Computer Vision,"Many countries have improved their traffic surveillance system by using computer vision to classify and track different types of vehicles. Having this data can lessen management cost and help improve rules and regulations in route planning. In Manila, using the common vehicle type dataset for traffic management is inefficient. The city has at least 9 types of vehicles present in its main roads, and at least 21 types of vehicles in secondary and tertiary roads. By using digital image processing, an algorithm for classified counting and tracking was created. The algorithm utilizes machine learning methods to create a local dataset with 16 types of vehicles for the city. After successfully creating the dataset, the system can detect all the present vehicles on the selected footage accurately. In a recorded video containing 11 types of vehicles, 92.96% were correctly classified and 95% were counted. The location of the Region of Interest (ROI) for counting must be strategically placed to avoid misclassified counting. The local dataset can also be improved by collecting data from other roads, and by adding the other local classes such as tricycles and pedicabs."
9607839,Exploiting Egocentric Vision on Shopping Cart for Out-Of-Stock Detection in Retail Environments,"Continuous detection and efficient monitoring of Out-Of-Stock (OOS) of products in retail environments is a key factor to improve stores profits. Traditional methods require labour-intensive human work dedicated to checking for products to refill raising the requirement of automatic solutions to detect OOS. In this work, we focus on the problem of OOS detection from an egocentric perspective proposing a new weak annotation of the EgoCart dataset. We benchmark the considered challenge employing a deep learning approach for the detection of OOS areas. Specifically, we train a Convolutional Neural Network (CNN) to predict attention maps useful to find OOS in retail areas and hence suggest the retail employers where to intervene. We evaluate results with both objective measures and a subjective analysis provided by human which has reviewed the obtained OOS attention maps. The achieved performance demonstrates that the proposed pipeline is promising to help the refilling process in the retail domain."
9451736,"Computer Vision-based Survey on Human Activity Recognition System, Challenges and Applications","For surveillance purpose, lots of method were used by the researchers but computer vision based Human Activity Recognition (HAR) technologies/systems received the most interest because they automatically distinguish human behaviour and movements from video data utilizing recorded details from cameras. But the extraction of accurate and opportune information from video of human's activities and behaviours is most important and difficult task in pervasive computing environment. Due to lots of applications of HAR systems like in medical field, security, visual monitoring, video recovery, entertainment and irregular behaviour detection, the accuracy of system is most important factors for researchers. This review article presents a brief survey of the existing video or vision-based HAR system to find out their challenges and applications in three aspects such as recognition of activities, activity analysis, and decision from visual content representation. In many applications, system recognition time and accuracy is most important factor and it is affected due to an increase in the usage of simple or low quality type cameras for automated systems. So, to obtain a better accuracy and fast responses, the usage of demanding and computationally intelligent classification techniques such as deep learning and machine learning is a better option for researchers. In this survey, we addressed numerous computationally intelligent classification techniques-based research for HAR from 2010 to 2020 for a better analysis of the benefits and drawbacks of systems, the challenges faced and applications with future directions for HAR. We also present some accessible problems and ideas that should be discussed in future research for the HAR system utilizing machine learning and deep learning principles due to thPeir strong relevance."
9031123,Service Robot Navigation and Computer Vision Application in a Banquet Hall Setting,"This paper describes a low computation system for a surveying service robot in a banquet hall setting. Automating this process will allow staff to focus on other tasks which will improve the overall efficiency of the event setup. The robot platform used was FURO, a two-wheeled service robot with an on board computer as well as a camera. Image processing algorithms were developed using the open source Open CV library to detect a total item count and ellipse tracker program. Potential fields was the navigation algorithm implemented to generate a collision free path towards the edge of the table. The results were successful as the computer vision algorithms worked 90% of the time in a controlled environment with a normal setup. It experienced failure when items were overlapping or too close to each other. Differences in lighting also came into effect when detecting the contents on the table. Nonetheless, this experiment presents a service robot application for table monitoring in a ballroom."
9543692,Development and Testing on the European Space-Grade BRAVE FPGAs: Evaluation of NG-Large Using High-Performance DSP Benchmarks,"The advent of space applications with increased computational requirements has led the space industry to consider innovative chips and avionics architectures for high-performance on-board data processing. In a relatively limited market, the European BRAVE family of Field-Programmable Gate Arrays (FPGAs) offers such novel radiation-hardened solutions. Towards verification, the current work devises and applies a methodology to thoroughly assess the BRAVE FPGAs and their SW tools. The paper focuses on NG-Large, i.e., the largest FPGA of the 65nm Radiation-Hardened-By-Design (RHBD) technology of NanoXplore to date. The proposed approach comprises a number of customized steps to systematically evaluate the entire FPGA design flow. Initially, we carefully select and tune a set of high-performance Digital Signal Processing (DSP) & Computer Vision (CV) benchmarks, which were originally developed as Hardware Description Language (HDL) IPs in past projects of the European Space Agency (ESA). Subsequently, we perform exhaustive exploration of the Synthesis, Placement, and Routing stages of the SW tools, as well as testing on actual HW boards. At each step, we generate and analyze a variety of results, while we also compare them to 3rd-party solutions. The results show that NG-Large provides sufficient programmability and performance, e.g., classic CV IPs for feature detection on megapixel images can achieve a throughput of 5–10 frames per second, while the on-chip memory utilization is up to 56% better than that of 3rd-party FPGAs. As a highlight, at system-level, we successfully implement and execute an entire HW/SW algorithmic pipeline for Vision-Based Navigation (VBN) involving SpaceWire data transfers with LEON CPU & NG-Large co-processing."
9574383,Elements and construction of sports visual image action recognition system based on visual attention analysis,"With the rapid development of computer vision technology, human action recognition technology has occupied an important position in this field. It has important practical value and research value in security protection, advanced human-computer interaction, video search analysis and sports analysis. Due to the non rigid body characteristics of human body, the change of illumination, and the influence of the changeable surrounding environment, human action recognition is more challenging. Virtual reality technology is an important subject in computer field. It simulates real scenes by means of computer software and hardware technology. Observe students' learning situation dynamically according to the scene. At present, this technology is still in the research and development stage, and there are still many problems in the application process. However, the exchange of students' learning information and simulated scenes is of great help to teaching activities. Moreover, the data obtained by virtual technology also provides guidance for teaching and research."
9131266,Research on Visual Relation Detection Based on Computer Vision,"Extracting semantic information from unstructured data, such as images or text, is a critical challenge in artificial intelligence and a long-lasting research direction. In general, semantic information is captured in more meaningful ways by the relationship between objects. In particular, visual relationship can be represented by triples of this form (subject, predicate, object). Visual relationship detection needs to identify objects in the image and their location, as well as identify relationships between objects. In this paper, we review the methods about visual relation detection, analyses the algorithms at home and abroad, and compare the advanced and inadequacies between algorithms. Finally, we look forward to the development trend of visual relation detection."
9588001,A computer vision program that identifies and classifies fish species,"In today's era of seafood consumption, the best marketing method is to classify the fish according to the type and size. This fish recognition program compares and segments the fish image data. The data set includes nine different kinds of seafood collected by a supermarket in Izmir, Turkey. The photos are taken by Kodak Eshare z650 and Samsung ST60 cameras, and the image resolutions are 2832×2128. Before segmentation, feature extraction and classification, the vertical and horizontal scale is 590×445 and adjusting the data set to 590, Tags in all datasets have been enhanced (through inversion and rotation). At the end of the enhancement process, there are 2000 images of each type, including 1000 RGB fish images. 1000 is their consistent real tags on the ground. This program is a highly accurate tag for fish recognition. The experimental results prove the applicability of the program in fish recognition and classification."
9607565,Leveraging Batch Normalization for Vision Transformers,"Transformer-based vision architectures have attracted great attention because of the strong performance over the convolutional neural networks (CNNs). Inherited from the NLP tasks, the architectures take Layer Normalization (LN) as a default normalization technique. On the other side, previous vision models, i.e., CNNs, treat Batch Normalization (BN) as a de facto standard, with the merits of faster inference than other normalization layers due to an avoidance of calculating the mean and variance statistics during inference, as well as better regularization effects during training.In this paper, we aim to introduce Batch Normalization to Transformer-based vision architectures. Our initial exploration reveals frequent crashes in model training when directly replacing all LN layers with BN, contributing to the un-normalized feed forward network (FFN) blocks. We therefore propose to add a BN layer in-between the two linear layers in the FFN block where stabilized training statistics are observed, resulting in a pure BN-based architecture. Our experiments proved that our resulting approach is as effective as the LN-based counterpart and is about 20% faster."
9164476,Computer vision-based Road Crack Detection Using an Improved I-UNet Convolutional Networks,"Cracks are one of the common and important diseases on the pavement surface. The traditional crack detection method mainly relies on manual operation, which is time-consuming and laborious in practical treatment. Therefore, an improved road cracks algorithm based on the I-UNet is proposed in this paper. In the method, the dilated convolution is used to expand the receptive field of the convolution. The method likes ""Inception"" is used to extract different scales of image features and conduct multi scale feature fusion, the Elu activation function is used. After training, I-UNet can efficiently segment complete cracks in complex environments. Experiments show that the proposed I-UNet based road crack detection method is more robust and more accurate than the U-Net based method."
9093317,Going Much Wider with Deep Networks for Image Super-Resolution,"Divide and Conquer is a well-established approach in the literature that has efficiently solved a variety of problems. However, it is yet to be explored in full in solving image super-resolution. To predict a sharp up-sampled image, this work proposes a divide and conquer approach based wide and deep network (WDN) that divides the 4× up-sampling problem into 32 disjoint subproblems that can be solved simultaneously and independently of each other Half of these subproblems deal with predicting the overall features of the high-resolution image, while the remaining are exclusively for predicting the finer details. Additionally, a technique that is found to be more effective in calibrating the pixel intensities has been proposed. Results obtained on multiple datasets demonstrate the improved performance of the proposed wide and deep network over state-of-the-art methods."
9594764,Computer Vision in the Infrared Spectrum: Challenges and Approaches,"act:
Human visual perception is limited to the visual-optical spectrum. Machine vision is not. Cameras sensitive to the different infrared spectra can enhance the abilities of autonomous systems and visually perceive the environment in a holistic way. Relevant scene content can be made visible especially in situations, where sensors of other modalities face issues like a visual-optical camera that needs a source of illumination. As a consequence, not only human mistakes can be avoided by increasing the level of automation, but also machine-induced errors can be reduced that, for example, could make a self-driving car crash into a pedestrian under difficult illumination conditions. Furthermore, multi-spectral sensor systems with infrared imagery as one modality are a rich source of information and can provably increase the robustness of many autonomous systems. Applications that can benefit from utilizing infrared imagery range from robotics to automotive and from biometrics to surveillance. In this book, we provide a brief yet concise introduction to the current state-of-the-art of computer vision and machine learning in the infrared spectrum. Based on various popular computer vision tasks such as image enhancement, object detection, or object tracking, we first motivate each task starting from established literature in the visual-optical spectrum. Then, we discuss the differences between processing images and videos in the visual-optical spectrum and the various infrared spectra. An overview of the current literature is provided together with an outlook for each task. Furthermore, available and annotated public datasets and common evaluation methods and metrics are presented. In a separate chapter, popular applications that can greatly benefit from the use of infrared imagery as a data source are presented and discussed. Among them are automatic target recognition, video surveillance, or biometrics including face recognition. Finally, we conclude with recommendations fo...
(Show More)"
9272177,Plum selection system using computer vision,"The development of a computer vision system, allows the possibility to makes easier and specify the different techniques that the conventional sensorics make in some way tedious or expensive. A computer vision system for the plums selection can be as simple as a computer, a webcam and an actuator, this is why in the development of this paper will emphasize the different algorithms which show the huge advantage these systems have over the conventional ones implemented with sensors."
9036472,A Review on Computer Vision - Scene Classification Techniques,"In today's era, need for automatic response of machines on certain task has been prevalent. Humans want their life easier and automatic in every possible way. However, those tasks require better understanding by the machine to perform human like tasks. Tasks like classification, detection and localization are on high demand and dominant research area. These tasks fall into a domain called computer vision where computers by analyzing and understanding performs human like tasks. This domain provides the automatic inference by machines to make human life easier. In this paper, we focus on one of the difficult computer vision tasks called scene classification. Scene Classification deals with techniques that make machine intelligent and automated by processing given input say image. As machines are made automatic and intelligent to perform various tasks, Artificial Intelligence and Image processing comes into the picture. We study and analyze various approaches and methods by which such task can be handled easily and accurately. Furthermore, we compare all the approaches and find out the best approach to opt for this task."
8882438,iSVC – Digital Platform for Detection and Prevention of Computer Vision Syndrome,"This paper describes the research, development and evaluation process of a solution based on computer vision for the detection and prevention of Computer Vision Syndrome, a type of eye fatigue characterized by the appearance of ocular symptoms during or after prolonged periods watching digital screens. The system developed targets users of computers and mobile devices, detecting and warning users to the occurrence of eye fatigue situations and suggesting corrective behaviours in order to prevent more complicated health consequences. The implementation resorts to machine learning techniques, using eye images datasets for training the eye state detection algorithm. OpenCV Lib was used for eye's segmentation and subsequent fatigue analysis. The final goal of the system is to provide users and health professionals with quality data analysis of eye fatigue levels, in order to raise awareness over accumulated stress and promote behaviour change."
8850795,Recognition of Dandelion Weed via Computer Vision for a Weed Removal Robot,"This paper introduces novel methods for distinguishing dandelion weed leaves from grass blades and identifying the centroids of dandelion weeds by means of a webcam. Firstly, a method of isolating the colour green using a Hue-Saturation-Value (HSV) scheme is proposed. Next, a technique for filtering the high frequency grass blades is developed to isolate individual low frequency of weed leaves. Subsequently, two novel methods: the lengthwise intersection method and mean centroid method are proposed to detect the location of the weed center based on the individual weed leaves. The detected location of the dandelion weed centroid has been used as the desired destination of autonomous driving system of a weed removal robot for successful dandelion removal."
8978179,ICDAR 2019 Competition on Scene Text Visual Question Answering,"This paper presents final results of ICDAR 2019 Scene Text Visual Question Answering competition (ST-VQA). ST-VQA introduces an important aspect that is not addressed by any Visual Question Answering system up to date, namely the incorporation of scene text to answer questions asked about an image. The competition introduces a new dataset comprising 23,038 images annotated with 31,791 question / answer pairs where the answer is always grounded on text instances present in the image. The images are taken from 7 different public computer vision datasets, covering a wide range of scenarios. The competition was structured in three tasks of increasing difficulty, that require reading the text in a scene and understanding it in the context of the scene, to correctly answer a given question. A novel evaluation metric is presented, which elegantly assesses both key capabilities expected from an optimal model: text recognition and image understanding. A detailed analysis of results from different participants is showcased, which provides insight into the current capabilities of VQA systems that can read. We firmly believe the dataset proposed in this challenge will be an important milestone to consider towards a path of more robust and general models that can exploit scene text to achieve holistic image understanding."
9498117,Multi-stage network based on connected simple backbone interaction for human pose estimation,"In recent years, the task of human pose estimation has continuously made significant progress, and it has become one of the research hotspots in the field of computer vision. From a certain point of view, there are two types of human pose estimation methods: single-stage methods and multi-stage methods. Both of these two methods have good performance, but they also have shortcomings. Single-stage methods have encountered a common bottleneck that simply increasing the model capacity does not give rise to much improvement in performance. Because of the insufficiency in some design choices, the performance of multi-stage methods in current practice is not as robust as expected. Therefore, we balance the pros and cons and design a multi-stage human pose estimation network structure, each of which contains an excellent single-stage module. At the same time, we adopt strategies such as cross stage feature fusion and multi-scale supervision to effectively improve the accuracy of positioning key points of the human body. Finally, we applied this network to the COCO keypoint detection dataset, and proved the effectiveness of our network through analysis of related data."
8821790,3D Mesh Deformation Using Graph Convolution Network,"In the field of computer vision, generation of 3D data is a fundamental but important problem. The existing methods try to generate 3D objects in geometry representation such as voxel, or point clouds. Unlike previous methods, we propose an end-to-end deep graph convolution network, representing a 3D shape as triangle mesh, that produces a high-quality mesh from a coarse input mesh. In order to connect the information between low-level layers and high-level layers, we adapt the architecture similar to ""U-Net”. As we represent a shape as a mesh, we can define higher order loss functions between a vertex and its neighborhood, which is important to get an accurate geometry shape. We show that our method performs qualitative results on shape deformation and produces 3D mesh with great details, evaluating on a dataset mainly consisting of human shape."
9068128,Automatic detection for students behaviors in a group presentation,"This paper suggests a model for automatic detection of student behavior to be used in student presentations. The proposed approach is based on the combined use of computer vision libraries and machine learning algorithms to help and support in student assessment using video content. This paper is a part of a research study focusing on investigating and analysing, human behaviours and finding relations between human behaviours and their personal modalities using pattern recognition techniques. For the purpose of this study a group of specific behaviours expressed by students during group presentations in higher education level, are selected. The study proceeds with the detection of the occurrences of those behaviours and comparative analysis of the model's suggested behavioural patterns against those observed through the manual analysis of observations. Both approaches are based on the same set of video files."
8825099,A Virtual Image Accelerator for Graph Cuts Inference on FPGA,"Graph Cuts is a popular technique for Maximum A Posteriori inference in computer vision. It transforms a Markov Random Field problem into a network flow problem, solved via the Push-Relabel algorithm. While attractively simple, the large size of a typical image and the large number of necessary pixel-level iterations render the technique computationally expensive. Prior accelerator attempts have been reported with GPUs and FPGAs. In 2017, we demonstrated the first pixel-parallel architecture on FPGA, but limited to only 256-pixel images. This paper extends this pixel-parallel concept and proposed a Virtual-Image architecture which solves the size limitation. We demonstrate the first working virtual-image Graph Cuts accelerator, implemented on a state of the art FPGA, applied to standard benchmark images for a background segmentation task. The design is 11-13× faster than other FPGA designs, and slightly faster than a modern GPU benchmark by about 30%."
9501790,A Computer Vision System for the Categorization of Citrus Fruits Using Convolutional Neural Network,"The recognition of citrus fruit is one of the most challenging and crucial measures in citrus yield mapping. Several artificial vision systems have been proposed to solve the issue of fruits recognition problem with sundry effects. In this study, we developed an automated system to categorize citrus fruit images using Convolutional Neural Network. We categorized two different citrus fruits, Orange (Citrus Sinensis) and Kinnow (Citrus Reticulate). Firstly the images of Orange and Kinnow were collected and preprocessed. Secondly, the fruit images and their background were segmented by image segmentation and edge detection. Four main features of Orange and Kinnow fruit were extracted based on image segmentations such as fruit size, surface color fruit shape and fruit surface defects. These features were examined through Convolutional Neural Network. We implemented three separate Convolutional Neural Network models to further experiment and tested recognition rates for different parameters. We have used the classical measurements including precision, recall, F1 score, ROC and accuracy for performance evaluation. Among the three experimented models, the third model was outperformed by 92.25% percent accuracy."
9057077,Self-localization and positioning vehicle navigation system based on computer vision and PID control,"In this work a control system is presented for the autonomous navigation of vehicles using a monocular vision system to detect reference patterns in a controlled environment. This control system is based on the data extraction from the vision system camera feed to generate position vectors, which enable the localization of the vehicle in the environment, without having to depend purely on odometry avoiding the error that it generates. The infrastructure developed for this paper is a low cost 4-wheeled vehicle with a minimal quantity of sensors and a VGA camera for the computer vision system, all components commercially available."
9137854,Virtual Mouse Using Object Tracking,"With new changes seen in computer technology day by day, it has become quite essential for us to find specific new ways of interaction with computer systems as its need is increasing in society every day. Today, every device is making the use of touch screen technology on its systems, which isn't affordable to be used in all applications. A specific interactive module like a virtual mouse that makes use of Object Tracking and Gestures that will help us to interact can be an alternative way for the traditional touch screen and the physical mouse. The objective is to create an Object Tracking application that interacts with the system. This system proposed is a Computer Vision-based mouse cursor control system, which uses hand gestures that are being captured from a webcam through an HSV color detection technique. This system allows the user to navigate the system cursor using their hand bearing color caps or tapes that the computer webcam tracks and perform mouse operations like left-click, right-click, and double click using different hand gestures. Python and OpenCV library is used for realtime computer vision to implement the system. The camera output is displayed on the monitor."
8968590,Camera Exposure Control for Robust Robot Vision with Noise-Aware Image Quality Assessment,"In this paper, we propose a noise-aware exposure control algorithm for robust robot vision. Our method aims to capture best-exposed images, which can boost the performance of various computer vision and robotics tasks. For this purpose, we carefully design an image quality metric that captures complementary quality attributes and ensures light-weight computation. Specifically, our metric consists of a combination of image gradient, entropy, and noise metrics. The synergy of these measures allows the preservation of sharp edges and rich texture in the image while maintaining a low noise level. Using this novel metric, we propose a real-time and fully automatic exposure and gain control technique based on the Nelder-Mead method. To illustrate the effectiveness of our technique, a large set of experimental results demonstrates the higher qualitative and quantitative performance compared with conventional approaches."
9363770,Image Matching Algorithm based on ORB and K-Means Clustering,"With the rapid development of science and technology, image processing technology plays an important role in the field of computer vision. In order to improve the matching speed and real-time requirements, this paper proposes an image matching algorithm based on ORB and K-means clustering, which can effectively improve the accuracy of image feature point location and the accuracy and efficiency of image feature matching, and reduce the time consumption. The algorithm uses sub-pixel interpolation to optimize the traditional ORB algorithm, which improves the accuracy and characteristics of clustering calculation."
9675746,Application of AI And Computer Vision To Face Mask And Social Distance Detection in CCTV Video Streams,"According to the figures obtained by the Department of Health (DOH), COVID-19, the worldwide pandemic, has had an enormous global impact, infecting a growing population and causing over three million fatalities.Because of the global epidemic, governments all over the world were obliged to institute lockdowns in order to prevent virus transmission. The use of face masks and safe social distance, according to sources, are two of the best safety precautions to be observed by the public to avoid the transmission of the virus. Modern deep neural network models are combined with geometrical approaches to develop a strong model that incorporates three components of the system’s detection, monitoring, and testing. As a consequence, the technique presented saves time while reducing corona virus transmission.It might be used efficiently in the current circumstances, where lockdown is being loosened in order to inspect people at public meetings, retail malls, and other locations. Automated inspection saves time and money by reducing the number of people needed to examine the public, and it can be used everywhere to ensure safety."
8901626,Pothole Visual Detection using Machine Learning Method integrated with Internet of Thing Video Streaming Platform,"A good condition of road was very needed by the people, because the movement of goods and services, and also a lot of people activities is indirectly depending on the road condition. So for keeps the road on good quality, early detection of road damage, especially pothole must be held. This thing is very important because a small damage that is not immediately handled can be large, so that the danger will decreased. It also make the cost that needed for repair it become greater. In the other hand, detecting road damage cannot be done with a manual checking, because it will take a lot of time and money. So supporting technology is needed to detect this kind of roads hazard. One of the technology easily used to detect road damage is computer vision. In this research, we build a system that can detect a pothole on the road which is captured by the camera. The camera used here is a wireless portable camera. Also for the location tagging, the GPS sensor is used here. The vision object detection system using imageZMQ library for stream the frames and process it in the processor PC. The capturing and streaming activity performed very well from the mini portable computer camera which is attached in the vehicle. the next step of this research is making the detection performed well in the more robust computational device."
9004358,ROBO - Autonomous Football Robot,"The autonomous football robot (ROBO) was designed to enhance the knowledge in the field of robotics and computer vision through the robot development. The goal was to learn the use of robotic kits, the Raspberry Pi micro-controller to control the robot motion, the Raspbian operating system (Linux based OS), the python programming language and the computer vision library (OpenCV). The robot design consists of hardware and software designs. For the robot hardware design and implementation, the robotic kit were built, tested, and attached to a 3D custom printing gripper and kicker along with the 180 degrees rotational Raspberry Pi camera by servo motor. Lastly, a small breadboard was attached to set up the pulse, voltage, and the ground for the servo motors. For the robot software design, there are two algorithms or modes in the program, ball mode and goal mode. The ball mode start with ball detection and end with the ball grab. The goal mode start with the goal search and end with kick the ball into the goal. A 150 experiments done for three different configuration of the robot. Each configuration consists of 50 experiments. The results were 2% task error for the first configuration, 10% for the second, and 22% for the third. The task errors were because of the mechanical failure of the kicker. The kicker was unable to reach the ball into the goal from far distance. A future work can be a better kicker with more power."
9710468,Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling,"Normalizing flows have recently demonstrated promising results for low-level vision tasks. For image super-resolution (SR), it learns to predict diverse photo-realistic high-resolution (HR) images from the low-resolution (LR) image rather than learning a deterministic mapping. For image rescaling, it achieves high accuracy by jointly modelling the downscaling and upscaling processes. While existing approaches employ specialized techniques for these two tasks, we set out to unify them in a single formulation. In this paper, we propose the hierarchical conditional flow (HCFlow) as a unified framework for image SR and image rescaling. More specifically, HCFlow learns a bijective mapping between HR and LR image pairs by modelling the distribution of the LR image and the rest high-frequency component simultaneously. In particular, the high-frequency component is conditional on the LR image in a hierarchical manner. To further enhance the performance, other losses such as perceptual loss and GAN loss are combined with the commonly used negative log-likelihood loss in training. Extensive experiments on general image SR, face image SR and image rescaling have demonstrated that the proposed HCFlow achieves state-of-the-art performance in terms of both quantitative metrics and visual quality."
9096264,Work Safety Assessment through Contextual Analysis with Computer Vision,"Work accidents are a reality for which companies must establish constant efforts to reduce both their likelihood of occuring and their impact should they occur and, where possible, eliminate. Accidents are caused by hazards which depend on the type of occupation and the activity carried out in the company. In Honduras, the third largest occupational group is the manufacturing industry. With the aim of reducing the impact of accidents, this research seeks to implement computer vision as a method of monitoring compliance of established safety measures. Firstly, the safety measures used in the industry are identified for the purpose of defining the measure to monitor with computer vision. Data is collected qualitatively in the form of images to perform the training and testing of the object recognition algorithm, YOLOv3-Tiny. Consequently, by applying a quantitative methodology, the performance of the prototype program is measured with the accuracy of the system compared to the ground truth established through the manual annotation of the images in the test data set. Finally, an evaluation of compliance with safety measures is incorporated through the qualitative labeling of the risk zone and the results delivered by the object recognition algorithm."
9456180,Computer Vision and IoT Enabled Bot for Surveillance and Monitoring of Forest and Large Farms,"This paper aims to combine both Computer Vision and IoT for surveillance and monitoring of forest and large farms. As it is applied on wheeled robot it can also be used for multi-purpose application as its not immobile like traditional cameras.In this project I have used Esp32 cam board for video capture and transmit to server where video feed is processed for human, animals and object detection which helps for surveillance by alerting the user. For monitoring purpose I have included sensors like microphone, smoke sensor, temperature and humidity sensor. Main aim is to detect fire out bursts and intruders. Sensor data will be stored and monitored in cloud IoT platform. It is capable of initiating actuators on its own when necessary. It has RF trans-receiver for long and emergency communication when it's in out of WiFi range.Through this paper I am trying to achieve low cost and efficient surveillance and monitoring robot with better application and usage of Computer Vision and IoT technology practically."
8742778,Detection and Recognition of Pavement Cracks Based on Computer Vision Technology,The article deals with the image crack analysis issues in pavement management systems based on the computer vision technology. The development of pavement management systems is necessary to predict the long-term performance of roads and prioritize maintenance policies. An algorithm for detecting cracks in pavement images is proposed. The algorithm includes the following steps: enhancing the image contrast; the operation of the morphological opening; image conversion to halftone; Bottom-hat transform; threshold conversion; image cleaning from noise by opening operation; the operation of connecting potentially fractured cracks; search for the edges of cracks; crack edges classification; visualization of the found cracks edges. The article also describes the implementation of the proposed algorithm in the form of experimental automated system software. This software is developed in the C++ using the computer vision library OpenCV and operates on the Jetson TX2 modular supercomputers hardware platform. The results of studies of the proposed approach are given.
9397092,Waste Collection & Segregation using Computer Vision and Convolutional Neural Network for Vessels,"The global crisis of pollution has influenced our lives adversely. Preparing the most salutary in reducing the catastrophe has its dominant necessity. Due to the expanding population, plastics in our neighborhood water bodies have increased for the past ten years, progressing the obligation to clean it up. This work intends towards the analysis of an automatic garbage collection system for a vessel/boat with improved performance. The focus of this paper is the development of a sorting and classification mechanism/model for the collected garbage by the vessel/boat incorporating the application of convolutional neural network (CNN) and computer vision. Using CNN & computer vision, the garbage features can be extracted and can be classified further into biodegradable and non-biodegradable with a prediction accuracy of more than 90%, which can be further increased by increasing the data set quantity for the model."
8917062,FaultNet: Faulty Rail-Valves Detection using Deep Learning and Computer Vision,"Regular inspection of rail valves and engines is an important task to ensure safety and efficiency of railway networks around the globe. Over the past decade, computer vision and pattern recognition based techniques have gained traction for such inspection and defect detection tasks. An automated end-to-end trained system can potentially provide a low-cost, high throughput, and cheap alternative to manual visual inspection of these components. However, such systems require huge amount of defective images for networks to understand complex defects. In this paper, a multi-phase deep learning based technique is proposed to perform accurate fault detection of rail-valves. Our approach uses a two-step method to perform high precision image segmentation of rail-valves resulting in pixel-wise accurate segmentation. Thereafter, a computer vision technique is used to identify faulty valves. We demonstrate that the proposed approach results in improved detection performance when compared to current state-of-the-art techniques used in fault detection."
9204329,Computer Vision in Control and Optimization of Road Traffic,"This paper presents the approaches and methods used in computer vision for the detection of moving objects and their tracking. Ways of using computer vision methods to optimize and control traffic are investigated. Every day, the number of vehicles on the roads is increasing and congestion problems are getting worse. The main goal in the development of modern traffic management systems is to create effective traffic management mechanisms in accordance with dynamic traffic conditions. Nowadays, the systems that regulate traffic have many drawbacks. The main ones, that such systems are working according to a predefined program and not being aware of the proper real-time data. This paper focuses on a novel approach to road traffic management by incorporating an intelligent traffic light controlling system using an algorithm that consumes real data from closed-circuit television (CCTV) cameras. As part of the solution, have been developed a program using a popular programming platform that would calculate sets of drive orders for traffic signal lights. The main goal of the proposed system is to provide better results in terms of reduced waiting delay for pedestrians and vehicles, shorter travel time, and increased average velocity of vehicles."
8752801,Adaptive Quality Optimization of Computer Vision Tasks in Resource-Constrained Devices using Edge Computing,"This paper presents an approach to optimize the quality of computer vision tasks in resource-constrained devices by using different execution versions of the same task. The execution versions are generated by dropping irrelevant contents of the input images or other contents that have marginal effect on the quality of the result. Our execution model is designed to support the edge computing paradigm, where the tasks can be executed remotely on edge nodes either to improve the quality or to reduce the workload of the local device. We also propose an algorithm that selects the suitable execution versions, which includes selecting the configuration and the location of the execution, in order to maximize the total quality of the tasks based on the available resources. The proposed approach provides reliable and adaptive task execution by using several execution versions with various performance and quality trade-offs. Therefore, it is very beneficial for systems with resource and timing constraints such as portable medical devices, surveillance video cameras, wearable systems, etc. The proposed algorithm is evaluated using different computer vision benchmarks."
9277543,Computer Vision Technology for Quality Monitoring in Smart Drying System,"Drying is one of the most viable and effective preservation technologies to improve the shelf-life of foods. Carrots are among the most consumed vegetables, owing to their nutritional profile as well as their wide use in dried foods, ready-to-eat and ready-to-use convenience products like snacks, meals, and soups. As for the dried products, the quality of produce depends on the timely recognition of the dehydration state. Traditional off-line analyses in combination with drying rates to identify the end-time of the process can fail in identifying process discrepancies and avoiding product degradation. The use of computer vision (CV) as a Process Analytical Technology (PAT) tool in the drying system can be of interest to monitor the drying process and product quality. The objective of this study was to study the drying behavior of carrot slices during drying at 35 °C for 36 h using a smart dryer augmented with computer vision system and load cell. The system developed was effective in measuring the weight, size, and color of the untreated (control) and pre-treated (blanched) carrot slices along the drying time. The image analysis and the weight loss of the slices enabled the prediction of relative moisture content (MC) using linear and thin-layer (Newton-Lewis) models in comparison. The applicability of the models was further evaluated by use of different pretreatments (i.e. blanched at 90 °C for 2 min or not blanched). The results showed promising prediction capability for the linear models, which was independent of time with a Root Mean Square Error (RMSE) similar to the thin-layer models, an adj. R 2 > 0.99 as well as both Mean BIAS Error (MBE) and reduced χ2 tending towards zero. The blanching treatment affected the model parameters but negligibly affected the model performances."
9033578,"Smart Fleet Management System Using IoT, Computer Vision, Cloud Computing and Machine Learning Technologies","The objective of this paper was to present the effective use of niche technologies in solving most critical problems of Fleet Industry. The proposed fleet management system described in the paper used a rich set of technologies like Internet of Things(IoT), Cloud Computing, Computer Vision, Machine Learning, Deep Learning and Embedded. IBM Watson IoT and Heroku platforms hosted the application to receive data from vehicle dashboard device. Driver's face authentication and driving pattern monitoring, Fuel Consumption prediction modules used OpenCV (Computer Vision), SVN (Machine Learning) and CNN (Deep Learning) techniques. Vehicle's Telematics, deviation from route, unauthorized entry in the container, continuous monitoring of trucks internal environment was handled by a high end embedded device with a set of sensor box, cameras, OBD-Π device and a gateway. Real data was collected to train and test the face detection and authentication models used in the system. Simulation results demonstrated that the proposed approach can achieve realistic demand of handing and manipulating humongous data coming every few seconds from several vehicles through IoT, NoSQL CloudantDB database and Cloud Computing. The paper also presented the architecture of the system and experimentation results done for various modules of the system."
9103018,Measuring Gait Variables Using Computer Vision to Assess Mobility and Fall Risk in Older Adults With Dementia,"Fall risk is high for older adults with dementia. Gait impairment contributes to increased fall risk, and gait changes are common in people with dementia, although the reliable assessment of gait is challenging in this population. This study aimed to develop an automated approach to performing gait assessments based on gait data that is collected frequently and unobtrusively, and analysed using computer vision methods. Recent developments in computer vision have led to the availability of open source human pose estimation algorithms, which automatically estimate the joint locations of a person in an image. In this study, a pre-existing pose estimation model was applied to 1066 walking videos collected of 31 older adults with dementia as they walked naturally in a corridor on a specialized dementia unit over a two week period. Using the tracked pose information, gait features were extracted from video recordings of gait bouts and their association with clinical mobility assessment scores and future falls data was examined. A significant association was found between extracted gait features and a clinical mobility assessment and the number of future falls, providing concurrent and predictive validation of this approach."
9389305,Efficient visual monitoring of offshore windmill installations with online image annotation and deep learning computer vision,"The number of offshore windmills keeps growing around the world since they are an essential part of any strategy to produce energy without using nuclear power plants or burning fossil resources. Due to the growing number of installations new monitoring and inspection strategies and routines need to be developed that are not only effective but efficient. In this work we will present first results from a joint project of industry partners and academia that aims at the development of new approaches making use of modern imaging technologies and new methods from artificial intelligence and computer vision research. We investigate three different strategies for collecting digital photos or video and posterior computational analysis using Convolutional Neural Networks (CNN). Our results indicate, that patterns of interest like rust or coating damage can be detected and classified with F 1 scores between 0.80 and 0.94 in photos collected by inspectors. In videos collected with unmanned aerial vehicles (UAV) rust patterns, oils spills and coating damage were detected with F 1 = 0.91. To support users in the time-consuming task of visual data exploration, we present a new visualization method referred to as “virtual twin”. Using hand - recorded image collections, a data-driven 3D model of a windmill is generated, that can be used to investigate damage patterns in the full context of the windmill and in full detail. Altogether our results indicate, that although each single approach has its limitations, a combination of different imaging methods, deep learning computer vision algorithms and sophisticated visual data exploration by experienced users appears to have potential to overcome the bottleneck in data analysis, interpretation and decision making in the context of the future inspections of offshore windmills, platforms or other constructions."
9418754,Computer Vision System for Reading Analog Gauges at Power Substation,"This paper proposes a computer vision system for reading analog gauges at a power substation. The gauges mentioned in this paper include oil level gauges, winding temperature gauges and SF6 gas density gauges. We present two different algorithms since an oil level gauge has different characteristics from a winding temperature gauge or an SF6 gas density gauge. For the oil level gauge, a color segmentation method is used to detect the pointer. Then we use Canny Edge Detector in combination with the Hough Circle Transform method to detect scale marks. Based on the position of the pointer and the marks, we detect the indicating value of the gauge. For winding temperature gauges and SF6 gas density gauges, we find the direction of the pointer based on the geometry properties of a double-edged pointer. With this information, we can calculate the value indicated on the gauge face using interpolation between the direction of the pointer and the value of the gauge. The system error is almost zero in case of an oil level gauge and remains low in the case of a winding temperature gauge and an SF6 gas density gauge. Mobile robots integrated with this computer vision system will work faster and more accurately than humans and will also help avoid dangerous accidents."
9165215,Hybrid Electric Vehicle Energy Management With Computer Vision and Deep Reinforcement Learning,"Modern automotive systems have been equipped with a highly increasing number of onboard computer vision hardware and software, which are considered to be beneficial for achieving eco-driving. This article combines computer vision and deep reinforcement learning (DRL) to improve the fuel economy of hybrid electric vehicles. The proposed method is capable of autonomously learning the optimal control policy from visual inputs. The state-of-the-art convolutional neural networks-based object detection method is utilized to extract available visual information from onboard cameras. The detected visual information is used as a state input for a continuous DRL model to output energy management strategies. To evaluate the proposed method, we construct 100 km real city and highway driving cycles, in which visual information is incorporated. The results show that the DRL-based system with visual information consumes 4.3-8.8% less fuel compared with the one without visual information, and the proposed method achieves 96.5% fuel economy of the global optimum-dynamic programming."
9157397,Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis,"The success of deep learning in visual recognition tasks has driven advancements in multiple fields of research. Particularly, increasing attention has been drawn towards its application in agriculture. Nevertheless, while visual pattern recognition on farmlands carries enormous economic values, little progress has been made to merge computer vision and crop sciences due to the lack of suitable agricultural image datasets. Meanwhile, problems in agriculture also pose new challenges in computer vision. For example, semantic segmentation of aerial farmland images requires inference over extremely large-size images with extreme annotation sparsity. These challenges are not present in most of the common object datasets, and we show that they are more challenging than many other aerial image datasets. To encourage research in computer vision for agriculture, we present Agriculture-Vision: a large-scale aerial farmland image dataset for semantic segmentation of agricultural patterns. We collected 94,986 high-quality aerial images from 3,432 farmlands across the US, where each image consists of RGB and Near-infrared (NIR) channels with resolution as high as 10 cm per pixel. We annotate nine types of field anomaly patterns that are most important to farmers. As a pilot study of aerial agricultural semantic segmentation, we perform comprehensive experiments using popular semantic segmentation models; we also propose an effective model designed for aerial agricultural pattern recognition. Our experiments demonstrate several challenges Agriculture-Vision poses to both the computer vision and agriculture communities. Future versions of this dataset will include even more aerial images, anomaly patterns and image channels."
9305816,Projectile Detection and Avoidance using Computer Vision,"There are numerous projects based on bullet detection and calculating its trajectory. For example, using an infrared camera, the heat property of a projectile can be extracted to determine the position of the projectile at a given point in time. For these calculations, more serious equipment and preparations are required to achieve their efficiency. This paper presents a novel solution, which can determine from first-person view perspective screenshots, the trajectory of the approaching object and the movement necessary to avoid it, knowing the body parts of the endangered entity using image processing and optimization algorithms."
9098298,Research on Image Recognition of Supermarket Commodity Based on Convolutional Neural Network,"For the actual shopping scenes in supermarkets, there will be more complex background environment, more diverse detection objects, and more changeable actual scenes, so the traditional image recognition network cannot meet the existing needs. In this paper, Convolutional Neural Network technology and YOLOv3 target detection algorithm are adopted to realize the recognition and detection of target objects in images, automated analysis and processing of these product image data with computer vision algorithms. The convolution neural network and COCO data set are used to train the neural network. Finally, the well-trained network is tested. Tested, the effect is good."
9298212,Smart gym trainer using Human pose estimation,"Human pose estimation becomes a popular project today in the field of computer vision. The human pose estimation can be developed using Artificial Intelligence or Machine learning in which the system is fed with sample data or trained models and hence can localize joints in the human body over a video or an image. Now once the joints of the human body are localized we can use it for wide applications such as getting the gait cycle of a person walking or tracking down the movements of a professional athlete in order to understand the physical techniques and strategies involved to achieve his/her success. Thus one of the applications of Human pose estimation could be developing a smart gym trainer software, that could help struggling bodybuilders to achieve their goals."
9337624,A low contrast defect detection method for PCB surface based on manual labeling,"In order to solve the detection problem of low contrast scratches on PCB board, a detection method based on manual marking was designed by using computer vision technology and OpenCV visual library. Mark the scratches on the PCB board with marker pen, take photos and sample the PCB board with ordinary camera, process the picture (grayscale processing, image filtering and edge detection, etc.), extract the edge with canny edge detection algorithm, and finally extract the ROI area on the defective PCB board. The experimental results show that this method can successfully detect low contrast scratches on PCB."
9570026,Application of Intelligent Control Systems in Electric Drives of Rail Vehicles,"This article shows the results of the development and modeling of a neural network control system for a two-mass electromechanical system of the main-line electric locomotive DS3. The simulation model of the electric drive of rail vehicles has the integration of the computer vision unit. The developed systems can automatically speed control rail vehicles, depending on the curvature of the track, to increase the level of automation and secure traffic."
9249142,Self-Driving Cars: A Platform for Learning and Research,"We present a self-driving car platform that can be used by both students and researchers. Students can use the platform to learn about the technology and challenges that exist in the self-driving car industry, while researchers can use it to test and iterate through potential solutions to self-driving car challenges. The platform is low-cost and small-scale, which makes it a desirable tool for both research and learning. The main technologies used on autonomous vehicles, such as computer vision and object detection, are implemented on this platform. We begin with a high-level overview of how self-driving cars work. Then we present the platform we have created, followed by a description of how it compares with industry level self-driving cars. Finally, we discuss the future work of this research."
9208144,Image Borders Detection Based on the Weight Model Analysis and the Morphological Gradient Operation,"The paper describes an approach to the image borders detection that is based on the use of the weight model and the operation of the morphological gradient calculating. The proposed method relates to multiscale image segmentation methods that use the Haar orthogonal wavelet transform. The border detection procedure includes smoothing the original image, building the weight model of the image (weight image), smoothing the weight image, setting the threshold weight value, performing threshold transformation of the weight image to form a binary image, and performing gradient morphological processing of the binary image. The described method allows segmenting the original image to extract information about the borders. This information is used to calculate attributes and their analysis in computer vision systems."
9649106,A technology of contactless three-dimensional reconstruction of animal models using depth cameras,This article is about an automated computer vision system capable to assess animals. The proposed technology is based on non-contact 3-D reconstruction of an animal model using Kinect depth cameras. The research solves the problem of creating a three-dimensional model of cattle using RGB-D data from three cameras in real-time. The proposed system produces high-precision 3-D measurements of animals.
9407429,Novel Census Transform Hardware IP,"Census Transform is a commonly used pixel transform technique in Computer Vision (CV) algorithms to provide robustness against illumination variance. Implementing this as a low gate-count and power-efficient HW IP is critical to support CV algorithms in SoCs targeting Automotive and Industrial markets. This paper gives an introduction to designing a HW IP for Census Transform as defined in the literature and proposes an efficient implementation by removing redundant computation by up-to 50% compared to the original algorithm. The paper also proposes a novel Census Transform algorithm by introducing a new pre-processing step which eliminates a key equality check during comparisons in the above implementation. The proposed novel algorithm results in a 27% lower gate-count and 13% lower power IP compared to the original implementation. The quality simulation of the proposed novel algorithm in the context of Optical Flow (an example CV algorithm) application shows only 1.48% flow vectors have differences compared to the original algorithm, resulting in a negligible impact to optical flow application. The proposed novel solution is implemented in Texas Instruments (TI) Jacinto7 series of product line."
9550996,Real-Time Detection and Recognition of Cards in the Game of Set,"This paper aims to present a procedure for real-time detection and recognition of the cards from the card game Set using computer vision techniques. Our approach to recognizing cards in images is done in three steps: segmenting cards from the image, extracting features from card images with horizontal and vertical lines, and card feature classification with support vector machine (SVM). Small number of features are extracted from relatively small subset of pixels to achieve real-time processing."
9180700,A Binary Line Buffer Circuit Featuring Lossy Data Compression at Fixed Maximum Data Rate,"Video processing requires an increasing amount of buffered data. The paper proposes a multi-line buffer circuit that stores compressed data thus saving logic and power. The lossy compression algorithm provides the output stream with a fixed, decided by the user, delay from the input stream. Further, the amount of memory of the compressed buffer can be designed to trade off the correctness of the output with the logic resources footprint. The circuit is tested in a computer vision processing chain as a binary line buffer for the stream of foreground pixels. The paper also proposes a multi-line buffer circuit that integrates a morphological operation. The latter, when compared against the state of the art and against the proposed multi-line lossy buffer, allows larger saving of logic resources (up to 75%) and power (up to 85%), with reduced penalty on video quality."
9620908,Intelligent real-time control system through socket communication using deep learning-based de-hazing and object detection in an embedded board environment,"This paper is a paper on the development of an intelligent real-time marine signal light that applies Socket communication after sea fog remove and object detection using artificial intelligence and computer vision through camera video in an embedded board environment. The embedded board used NVIDIA Jetson Nano board, and for sea fog remove, deep learning-based FFA-Net with excellent sea fog and fog removal performance among current technologies was used. For human detection, YOLOv4 based on light-weight deep learning with excellent detection performance among current technologies was applied for the purpose of research. For learning and testing, customized data was built through RESIDE dataset, MSCOCO dataset and data labeling, and socket communication was used for information delivery."
9039471,Development of a Complex for Supporting Small Citizens Using a Free License,"As the basis of the project, the urgent problem was chosen, the movement of people with limited mobility in an urban environment where the geography of the area is constantly changing. The urgency of the problem is expressed by the lack of mass (modern) devices for helping visually impaired (invisible) citizens, and is also clearly expressed by the active position of the state itself and the implementation of the ""Accessible Environment"" program. According to the results of the analysis of existing solutions, an independent solution is proposed, built on a device with open source software, which will maximize the cost and availability of the device, as well as change its functionality. As the basis of the solution, the RaspberryPi hardware platform is currently proposed on which software running using the OpenCV open-source computer vision library will work."
9593340,Motion-Based Multiple Object Detection and Tracking in Video,"Detecting and tracking objects is recently very important problem in computer vision field. It is widely used in areas like surveillance, security, automated vehicle systems and many more. This application is capable of detecting and tracking multiple objects in video sequences captured by a static camera."
9102678,Towards Effective Categorization of Weather Images using Deep Convolutional Architecture,"In the era of Industry 4.0, when various industries are coming together to achieve commercial enterprise goals, factors like weather plays an important role in enterprise growth. Weather conditions not only influence our daily lives, but it's also a big factor in various industries like logistics, retails, construction, etc. Apart from that, it also influences the functionality of many visual systems, like vehicle assistance driving systems, outdoor video surveillance, etc. Foggy weather condition is a vital threat to car accidents. The effect of harsh weather conditions can also be seen in India. There are loads of research that has been going on in the field of `Weather Analytics' but it nevertheless wants a lot of research to follow artificial intelligence in this domain. This paper is presenting a precise work to fulfill the gap. The study investigates the applications of a deep learning algorithm (convolutional neural networks) in weather detection problems. The traditional ways of detecting weather conditions rely on expensive sensors. To minimize the cost, this work presents a computer vision approach to detect live weather condition."
9301928,Predicting Surface Roughness using Keras DNN Model,"In the manufacturing industries, computer vision techniques play an important role in predicting the surface roughness of the workpiece in turning operations. The regression method has become more popular in the implementation of deep learning as it is the most basic application and. In this study, a regression with Keras deep neural network model is used to accurately evaluate the correlation between surface image characteristics and actual surface roughness using the machining parameters and the gray level of the surface image. The obtained results showed that the Keras Deep Neural Network model with a training data set of 57 values has an average accuracy of 80.52 % for predicting surface roughness in turning operations."
8802861,GUI-Guided Repair of Mobile Test Scripts,"Graphical User Interface (GUI) testing has been the focus of mobile app testing. Manual test cases, containing valuable human knowledge about the apps under test, are often coded as scripts to enable automated and repeated execution for test cost reduction. Unfortunately, many test scripts may become broken due to changes made during app updates. Broken test scripts are expected to be updated for reuse; however, the maintenance cost can be high if large numbers of test scripts require manual repair. We propose an approach named METER to repairing broken test scripts automatically when mobile apps are updated. METER novelly leverages computer vision techniques to infer GUI changes between two versions from screenshots and uses the GUI changes to guide the repair of test scripts. In experiments conducted on 18 Android apps, METER was able to repair 78.3% broken test scripts."
9016919,Automatized Food Quality Detection and Processing System Using Neural Networks,"In today's world, food processing industries play a vital role either directly or indirectly in human's sustenance. Majority of the food processing industries do still hinge on direct human intervention. The quality of food can be evaluated using computer vision which provides a solution through an automatized nondestructive and cost effective approach. This paper proposes a sustenance quality recognizing and reviewing framework dependent on OpenCV python library. The Convolution Neural Network (CNN) is implemented to perform the tasks of fruit type recognition and its quality detection through precise, dependable, reliable and quantitative data. The proposed system comprises of microcontroller, sensors, a camera and a conveyor belt setup to segregate the fruits. To preserve the quality of fruits for longer duration the storage unit environment condition is measured using sensors and the data is sent over the cloud based on the technology of Internet of Things."
9172861,A Survey of Depth Estimation Based on Computer Vision,"Currently, the method based on computer vision for depth information extraction and depth estimation is widely used. It can get depth information from 2D images, depth maps, or binocular vision images and has been a popular application in the field of artificial intelligence such as depth detection, pose estimation, as well as 3D reconstruction. This paper introduces the basic theory and some implementation methods of depth information acquisition based on computer vision. As well, it briefly summarizes the existing research results and makes an outlook on the future development trend of the field."
8338139,Discriminative Optimization: Theory and Applications to Computer Vision,"Many computer vision problems are formulated as the optimization of a cost function. This approach faces two main challenges: designing a cost function with a local optimum at an acceptable solution, and developing an efficient numerical method to search for this optimum. While designing such functions is feasible in the noiseless case, the stability and location of local optima are mostly unknown under noise, occlusion, or missing data. In practice, this can result in undesirable local optima or not having a local optimum in the expected place. On the other hand, numerical optimization algorithms in high-dimensional spaces are typically local and often rely on expensive first or second order information to guide the search. To overcome these limitations, we propose Discriminative Optimization (DO), a method that learns search directions from data without the need of a cost function. DO explicitly learns a sequence of updates in the search space that leads to stationary points that correspond to the desired solutions. We provide a formal analysis of DO and illustrate its benefits in the problem of 3D registration, camera pose estimation, and image denoising. We show that DO outperformed or matched state-of-the-art algorithms in terms of accuracy, robustness, and computational efficiency."
9344943,Human Action Recognition Based on Multi-level Feature Fusion,"Human action recognition has always been the focus of research in the field of computer vision. Different from the traditional human action recognition methods based on handcrafted features, the deep learning-based human action recognition methods can automatically learn features and hence have been widely concerned in recent years. However, with the increasing application of deep network in human action recognition, the problem of information loss cannot be ignored due to the deepening of convolutional layers, which will eventually affect the performance of recognition. To solve the abovementioned problem, we propose a deep learning-based action recognition method using a multi-level feature fusion mechanism which helps make full use of detailed features of middle layers of CNN network. Convolutional autoencoders are employed to reduce dimension of the middle-layer feature while retaining its representativeness. At the same time, a joint optimization module is designed to reduce the feature redundancy and achieve better recognition performance. Experimental results have shown the superiority of the proposed method, and the average accuracy of action recognition reaches 92.54%."
9239766,Correlation Filter Network Model Performance Analysis,"Object tracking is an significant field of computer vision, which is extensive used in the fields of automatic driving and intelligent monitoring. However, target tracking still has difficulties in tracking caused by factors such as occlusion, illumination, and scale changes. In order to accurately track the target and increase the tracking performance of the model, the paper reproduces the two goals of the full convolution twin neural network and the correlation filtering neural network. The tracking algorithm is tested using the OTB100 data set. The target tracking overlap rate differs by 1.3%, and the success rate differs by 1.5%. Experiments show that, in the face of complex environments, the relevant filtering neural network updates the model in time and shows excellent tracking results."
8869407,Smart Entrance System Using Computer Vision at Corporate Environment,"In this paper, a system has been proposed that can be used to a corporate office to log in their employees. Under this system, process involves, face detection using Viola-Jones Algorithm. The alignment of the face is corrected if there has seen any tortuous neck of an employee by applying Kazemi algorithm. The filtering process is being used to account for the fact that male employees might have different mustache and beard on their face on different days. So filtering is added to the picture to detect how he may look if they have mustache or beard. As a result, the risk of a faulty detection regardless of different facial appearance is minimized. These pictures are then added to database. The face is recognized by using Eignefaces which is easier and quicker in computing an employee under smart entrance system. Under conventional system, it is known that corporate employees get entrance by card punching which is sometimes seen to be fraudulent. But with the innovation and enactment of smart entrance system, which evolves face recognition, these illegitimate tasks can be restrained."
9130874,"A Vision-Based Pipeline for Vehicle Counting, Speed Estimation, and Classification","Cameras have been widely used in traffic operations. While many technologically smart camera solutions in the market can be integrated into Intelligent Transport Systems (ITS) for automated detection, monitoring and data generation, many Network Operations (a.k.a Traffic Control) Centres still use legacy camera systems as manual surveillance devices. In this paper, we demonstrate effective use of these older assets by applying computer vision techniques to extract traffic data from videos captured by legacy cameras. In our proposed vision-based pipeline, we adopt recent state-of-the-art object detectors and transfer-learning to detect vehicles, pedestrians, and cyclists from monocular videos. By weakly calibrating the camera, we demonstrate a novel application of the image-to-world homography which gives our monocular vision system the efficacy of counting vehicles by lane and estimating vehicle length and speed in real-world units. Our pipeline also includes a module which combines a convolutional neural network (CNN) classifier with projective geometry information to classify vehicles. We have tested it on videos captured at several sites with different traffic flow conditions and compared the results with the data collected by piezoelectric sensors. Our experimental results show that the proposed pipeline can process 60 frames per second for pre-recorded videos and yield high-quality metadata for further traffic analysis."
9438159,A Hardware Accelerated Computer Vision Library for 3D Reconstruction Onboard Small Satellites,"Here we present benchmarks and the expected performance of the SSRLCV (Small Satellite Research Laboratory Computer Vision) library that will be tested in Low Earth Orbit onboard the 6U MOCI (Multiview Onboard Computer Vision) cube satellite. The SSRLCV software library, used on the MOCI cubesat, is written in CUDA and C++ for Nvidia GPU/SoCs and performs structure from motion to generate 3D terrain information from a series of locally generated orbital images. Scale and Rotation invariant features are extracted from images, matched between those images, then an initial 3D point cloud is estimated with feature triangulation. Noise is removed from the point cloud and a gradient descent method, known as bundle adjustment, is used to refine the estimated camera parameters and location information of the satellite. The research simulates satellite imagery from LEO with 3D rendering software to test image data. Tests are run on the Nvidia TX2 and TX2i with timing, state, and power usage tracking. Reconstruction accuracy is measured by volumetric comparison and an Iterative Closest Point algorithm to allow for comparison to ground truth 3D models. The results show accurate 3D reconstruction of the surface of Earth feasible within 15 to 100 meters, depending on the camera system and altitude, while maintaining favorable power usage and computation time."
9133338,Biased Mixtures of Experts: Enabling Computer Vision Inference Under Data Transfer Limitations,"We propose a novel mixture-of-experts class to optimize computer vision models in accordance with data transfer limitations at test time. Our approach postulates that the minimum acceptable amount of data allowing for highly-accurate results can vary for different input space partitions. Therefore, we consider mixtures where experts require different amounts of data, and train a sparse gating function to divide the input space for each expert. By appropriate hyperparameter selection, our approach is able to bias mixtures of experts towards selecting specific experts over others. In this way, we show that the data transfer optimization between visual sensing and processing can be solved as a convex optimization problem. To demonstrate the relation between data availability and performance, we evaluate biased mixtures on a range of mainstream computer vision problems, namely: (i) single shot detection, (ii) image super resolution, and (iii) realtime video action classification. For all cases, and when experts constitute modified baselines to meet different limits on allowed data utility, biased mixtures significantly outperform previous work optimized to meet the same constraints on available data."
9609708,Impact of Computer Vision With Deep Learning Approach in Medical Imaging Diagnosis,"Medical experts are usually the ones who analyze the interpretations of medical data. A medical expert's ability to interpret images is limited due to subjectivity and the complexity of the images. This research purpose is to find out if the uses of computer vision in medical imaging will harm the patient with the impact and challenges, we will face while implementing computer vision in healthcare, especially medical imaging. This research will uncover how well deep learning algorithms compared to health-care professionals at classifying diseases based on medical imaging. In this research, the methods that we use is systematic literature review about Computer Vision. Deep Learning approach in Computer Vision for Medical Imaging is the secret to aiding physicians in maximizing the accuracy of diagnoses, it is harmless and are safe to use for assisting doctors in medical imaging diagnosis."
8370108,Wearable Vision Assistance System Based on Binocular Sensors for Visually Impaired Users,"Blind or visually impaired people face special difficulties in daily life. With the advances in vision sensors and computer vision, the design of wearable vision assistance system is promising. In order to improve the life quality of the visually impaired group, a wearable system is proposed in this paper. Typically the performance of visual sensors is affected by a variety of complex factors in practice, resulting in a large number of noise and distortion. In this paper, we will creatively leverage image quality evaluation to select the captured images through vision sensors, which can ensure the input quality of scenes for the final identification system. First, we use binocular vision sensors to capture images in a fixed frequency and choose the informative ones based on stereo image quality assessment. Then the captured images will be sent to cloud for further computing. Specially, the detection and automatic result will be done for all the received images. Convolutional neural network based on big data will be used in this step. According to image analysis, the cloud computing can return the requested information for users, which can help them make a more reasonable decision in further action. Simulations and experiments show that the proposed method can solve the problem effectively. In addition, statistical results also demonstrate that wearable vision system can make visually impaired group more satisfied in visual needed situations."
9403684,Research on Tennis Recognition Technology Based on Computer Vision,"Visual attention mechanism is one of the important means for human beings to perceive the external world. Using mathematical models to introduce visual attention mechanism into computer vision to simulate human visual perception system is a hot research topic in the field of computer vision. The research of visual attention model is not only helpful for human beings to better explore the working mechanism of human visual attention, but also has very important significance for solving large-scale data screening and improving image processing efficiency, which has important application value in moving object detection, machine vision, image information matching, image compression and other fields. The visual attention model is used to preprocess the video sequence, and the region of interest is found as the candidate region for target detection. Then the color and shape matching algorithm is used to match the candidate regions. Experiments on tennis video show that the algorithm can recognize the target well when the color and shape of the target are relatively single and the significance in the background is high."
9157633,GP-NAS: Gaussian Process Based Neural Architecture Search,"Neural architecture search (NAS) advances beyond the state-of-the-art in various computer vision tasks by automating the designs of deep neural networks. In this paper, we aim to address three important questions in NAS: (1) How to measure the correlation between architectures and their performances? (2) How to evaluate the correlation between different architectures? (3) How to learn these correlations with a small number of samples? To this end, we first model these correlations from a Bayesian perspective. Specifically, by introducing a novel Gaussian Process based NAS (GP-NAS) method, the correlations are modeled by the kernel function and mean function. The kernel function is also learnable to enable adaptive modeling for complex correlations in different search spaces. Furthermore, by incorporating a mutual information based sampling method, we can theoretically ensure the high-performance architecture with only a small set of samples. After addressing these problems, training GP-NAS once enables direct performance prediction of any architecture in different scenarios and may obtain efficient networks for different deployment platforms. Extensive experiments on both image classification and face recognition tasks verify the effectiveness of our algorithm."
8666488,Computer Vision for Attendance and Emotion Analysis in School Settings,"This paper presents facial detection and emotion analysis software developed by and for secondary students and teachers. The goal is to provide a tool that reduces the time teachers spend taking attendance while also collecting data that improves teaching practices. Disturbing current trends regarding school shootings motivated the inclusion of emotion recognition so that teachers are able to better monitor students' emotional states over time. This will be accomplished by providing teachers with early warning notifications when a student significantly deviates in a negative way from their characteristic emotional profile. This project was designed to save teachers time, help teachers better address student mental health needs, and motivate students and teachers to learn more computer science, computer vision, and machine learning as they use and modify the code in their own classrooms. Important takeaways from initial test results are that increasing training images increases the accuracy of the recognition software, and the farther away a face is from the camera, the higher the chances are that the face will be incorrectly recognized. The software tool is available for download at https://github.com/ferrabacus/Digital-Class."
9607766,"What Does TERRA-REF’s High Resolution, Multi Sensor Plant Sensing Public Domain Data Offer the Computer Vision Community?","A core objective of the TERRA-REF project was to generate an open-access reference dataset for the evaluation of sensing technologies to study plants under field conditions. The TERRA-REF program deployed a suite of high-resolution, cutting edge technology sensors on a gantry system with the aim of scanning 1 hectare (10 4 m) at around 1 mm 2 spatial resolution multiple times per week. The system contains co-located sensors including a stereo-pair RGB camera, a thermal imager, a laser scanner to capture 3D structure, and two hyperspectral cameras covering wavelengths of 300-2500nm. This sensor data is provided alongside over sixty types of traditional plant phenotype measurements that can be used to train new machine learning models. Associated weather and environmental measurements, information about agronomic management and experimental design, and the genomic sequences of hundreds of plant varieties have been collected and are available alongside the sensor and plant phenotype data.Over the course of four years and ten growing seasons, the TERRA-REF system generated over 1 PB of sensor data and almost 45 million files. The subset that has been released to the public domain accounts for two seasons and about half of the total data volume. This provides an unprecedented opportunity for investigations far beyond the core biological scope of the project.The focus of this paper is to provide the Computer Vision and Machine Learning communities an overview of the available data and some potential applications of this one of a kind data."
9607790,Time Lab’s approach to the Challenge on Computer Vision for Remote Physiological Measurement,"Computer vision for remote physiological measurement is novel and uniquely challenging task, which enables non-contact monitoring of the blood volume pulse (BVP) using a commonly accessible camera. This paper introduces Time Lab’s approach presented at the 2nd challenge on Remote Physiological Signal Sensing (RePSS) organized within ICCV2021. We propose an end-to-end rPPGNet for remote photoplethysmographyraphy (rPPG) signals estimation. A improved design of spatial-temporal map is also made, which is an efficient representation of the rPPG signal by removing most of the irrelevant background content. Furthermore, our approach achieved first place on the 2nd RePSS Challenge Track 1 and has outperformed the methods of other participants as we have achieved M_IBI = 117.25(4.51% improvement compared to the challenge top-2 result), R_HR = 0.62(8.77% improvement). The codes are publicly available at https://github.com/yuhang1070/2nd_RePSS_Track1_Top1_Solution."
9156974,Scalable Uncertainty for Computer Vision With Functional Variational Inference,"As Deep Learning continues to yield successful applications in Computer Vision, the ability to quantify all forms of uncertainty is a paramount requirement for its safe and reliable deployment in the real-world. In this work, we leverage the formulation of variational inference in function space, where we associate Gaussian Processes (GPs) to both Bayesian CNN priors and variational family. Since GPs are fully determined by their mean and covariance functions, we are able to obtain predictive uncertainty estimates at the cost of a single forward pass through any chosen CNN architecture and for any supervised learning task. By leveraging the structure of the induced covariance matrices, we propose numerically efficient algorithms which enable fast training in the context of high-dimensional tasks such as depth estimation and semantic segmentation. Additionally, we provide sufficient conditions for constructing regression loss functions whose probabilistic counterparts are compatible with aleatoric uncertainty quantification."
8769905,First Impressions: A Survey on Vision-Based Apparent Personality Trait Analysis,"Personality analysis has been widely studied in psychology, neuropsychology, and signal processing fields, among others. From the past few years, it also became an attractive research area in visual computing. From the computational point of view, by far speech and text have been the most considered cues of information for analyzing personality. However, recently there has been an increasing interest from the computer vision community in analyzing personality from visual data. Recent computer vision approaches are able to accurately analyze human faces, body postures and behaviors, and use these information to infer apparent personality traits. Because of the overwhelming research interest in this topic, and of the potential impact that this sort of methods could have in society, we present in this paper an up-to-date review of existing vision-based approaches for apparent personality trait recognition. We describe seminal and cutting edge works on the subject, discussing and comparing their distinctive features and limitations. Future venues of research in the field are identified and discussed. Furthermore, aspects on the subjectivity in data labeling/evaluation, as well as current datasets and challenges organized to push the research on the field are reviewed."
9132967,Rice Quality Prediction using Computer Vision,"The largest cultivated crop in India is rice. It means 70% population of India used to eat rice in their daily basis i.e. two times in a day. The easiest method one person can get contaminated by any kind of disease is food. We always get concerned about our health when we got to know about any kind of diseases and we started taking precautions. For example to avoid pollution in our day to day life we are using mask in our face and to avoid getting contaminated by water we are using bacteria free hand-wash and sanitizers. But when it comes to food we are buying it directly from the market and wash it with water and directly used it for cooking which means we are totally dependable on the food grains that we are buying from the market. Sometimes it happens that we found different small particles i.e. small stones, half sized rice, husk etc. after buying the rice packet, sometimes we found the unwanted products while eating also. We are not concerned about the quality of it. In this paper we have shown our concern towards the quality of rice. We belief a rice size should have more accurate, i.e. it must grow in its full size to provide its full proteins to everyone. We have used the image of rice to predict the quality. We have implemented feature extraction techniques and train our model by different machine learning techniques."
9011031,Scene Text Visual Question Answering,"Current visual question answering datasets do not consider the rich semantic information conveyed by text within an image. In this work, we present a new dataset, ST-VQA, that aims to highlight the importance of exploiting high-level semantic information present in images as textual cues in the Visual Question Answering process. We use this dataset to define a series of tasks of increasing difficulty for which reading the scene text in the context provided by the visual information is necessary to reason and generate an appropriate answer. We propose a new evaluation metric for these tasks to account both for reasoning errors as well as shortcomings of the text recognition module. In addition we put forward a series of baseline methods, which provide further insight to the newly released dataset, and set the scene for further research."
9707523,Detecting Arbitrary Intermediate Keypoints for Human Pose Estimation with Vision Transformers,"Most human pose estimation datasets have a fixed set of keypoints. Hence, trained models are only capable of detecting these defined points. Adding new points to the dataset requires a full retraining of the model. We present a model based on the Vision Transformer architecture that can detect these fixed points and arbitrary intermediate points without any computational overhead during inference time. Furthermore, independently detected intermediate keypoints can improve analyses derived from the keypoints such as the calculation of body angles. Our approach is based on TokenPose [9] and replaces the fixed keypoint tokens with an embedding of human readable keypoint vectors to keypoint tokens. For ski jumpers, who benefit from intermediate detections especially of their skis, this model achieves the same performance as TokenPose on the fixed keypoints and can detect any intermediate keypoint directly."
9087086,Adaptive Fuzzy Network based Transfer Learning for Image Classification,"With the introduction of Convolutional Neural Networks (CNN) the computer vision domain has witnessed a tremendous increase in novel architectures achieving results on vision tasks that exceed human performance. Neuro-fuzzy hybrid systems are a great avenue for enhancing the interpretability of neural networks. A lot of research in recent times has explored the technique of transfer learning applied to CNNs for computer vision applications. However, a pre-trained deep convolutional network with a subsequent adaptive fuzzy based network is yet to be explored. Hence in this paper, a novel adaptive fuzzy network based convolutional network is proposed. The paper focuses on using non-hybrid learning based adaptive fuzzy networks in conjunction with pre-trained convolutional networks for the task of image classification. The results illustrate the proposed approach eclipses over existing architectures used for image classification."
9325304,Research on Quantification Method of Maize Leaf Phenotype Parameters Based on Machine Vision,"This paper aims to obtain a model (method) for monitering the growth of corn automatically and accurately. The latest computer vision technology is applied to calculate the method of extracting the surface characteristic parameters. Binocular vision technology is used to calculate the 3D point cloud of maize image. Meanwhile, the 3D point noise reduction established by bilateral filtering is adopted to establish the 3D reconstruction model of maize. In the 3D model, the height and width of maize are calculated proportionally. The error rate of the proposed method is 0.52%. This work provides a reference for the growth monitoring and virtual growth of corn."
9026903,Eigendecomposition-Free Training of Deep Networks for Linear Least-Square Problems,"Many classical Computer Vision problems, such as essential matrix computation and pose estimation from 3D to 2D correspondences, can be tackled by solving a linear least-square problem, which can be done by finding the eigenvector corresponding to the smallest, or zero, eigenvalue of a matrix representing a linear system. Incorporating this in deep learning frameworks would allow us to explicitly encode known notions of geometry, instead of having the network implicitly learn them from data. However, performing eigendecomposition within a network requires the ability to differentiate this operation. While theoretically doable, this introduces numerical instability in the optimization process in practice. In this paper, we introduce an eigendecomposition-free approach to training a deep network whose loss depends on the eigenvector corresponding to a zero eigenvalue of a matrix predicted by the network. We demonstrate that our approach is much more robust than explicit differentiation of the eigendecomposition using two general tasks, outlier rejection and denoising, with several practical examples including wide-baseline stereo, the perspective-n-point problem, and ellipse fitting. Empirically, our method has better convergence properties and yields state-of-the-art results."
9250950,Patient Monitoring System using Computer Vision for Emotional Recognition and Vital Signs Detection,"Patient monitoring is a pivotal part of the healthcare system nowadays, either at hospitals or at home. Critical patients require to be monitored consistently and less human involvement, 24 hours a day to enable them to get medical assistance in the moment of need. However, these types of services are only available in private hospitals. Typically, there is a small number of patients in private hospitals, especially from the higher socio-economic backgrounds. Conversely, in public hospital, a huge number of patients require medical attention due to the imbalance between staff to patient ratio. The patient monitoring system is restricted when they are asleep or unconscious due to incapability to call for assistance during an emergency. This may delay the treatment as the medical staff are unaware of the patients' condition, hence resulting fatality. This work proposes a smart integrated patient monitoring system that automatically detects patient's emotional state and heartbeat levels through face recognition algorithms, heartbeat and temperature sensors. A Raspberry Pi and NodeMCU are used as client nodes to collect the patient data. These data are then transmitted to an IoT cloud for realtime visualization. Through this monitoring system, critical patients can get immediate attention without the requirement of the staff being present there 24 hours a day. This system offers a faster response from medical staff to provide treatment in critical times."
9145131,Modern Architectures for Core Computer Vision on Videos,"Over the last half decade, the computer vision field has made huge strides on core image understanding tasks such as object recognition, detection, segmentation and keypoint estimation - today these are technologies that are ubiquitously available to everyday consumers. However, video understanding technology is relatively less pervasive as computer vision models must deal with more difficult appearance, less labeled data, and computational challenges. In this talk, I will cover some of what I consider to be the core challenges of computer vision for video understanding, discussing these in the context of several recent research projects by the Perception team at Google."
9096002,Maize Growth Monitoring Based on Embedded Vision System,"Monitoring the growth of maize rapidly and accurately is meaningful in research and management. However, traditional plant growth monitoring methods based on machine vision required ideal environment for image measurement, lack of exploration for the application of complex environment. This paper proposed embedded devices in Maize growth environment, extracting features of plant growth in the embedded vision system by the artificial, then monitoring by the user-defined algorithm. Plant growth monitoring system based on embedded device integrates a standard process flow contained five part such as filter, color space transformation, image segmentation, morphological operations and feature quantization. Each step in the process includes a variety of algorithms, users can combine algorithms freestyle to get analysis values of plant on different environment. Continuously monitor the maize plants at jointing stage, the results showed that the determination coefficient of system analysis value and manual measurement value reaches 0.907, which indicated that the system can be used for monitoring maize growth under complex environment."
9532834,Identification of Citrus Fruit Defect using Computer Vision System,"Agronomy production accounts for a significant portion of the Indian economy. Since there is a high demand for fruits in the market, fruit disease detection is a critical component, the incorporation of computer vision systems in agriculture also deal with the identification and classification of fruit diseases. There is an increasing need for detecting flaws in various fruits, which is laborious and ineffective. The procedure identifies citrus fruit illnesses using several classifiers and assesses the accuracy and effectiveness of each algorithm. This is an involuntary recognition system and the procedure is both labor-intensive and cost-effective. This research work has performed a comparative analysis of several classifiers such as KNN, SVM, and DT by providing the image of a sick fruit as an input. By using the proposed classicification model, upto 97% accuracy can be obtained. The proposed system has major advantages such as user friendliness, good reliability etc. The primary goal of the proposed strategy is to increase the consumer knowledge about fruit illness and whether or not they should be consumed."
9350415,Sports-Net18: Various Sports Classification using Transfer Learning,"With the advancement of Computer Vision (CV) classification of sports images is very popular nowadays. Deep learning techniques have become feasible to use and see what happens inside the contents of an image. As videos are the collection of continuous images, or `frames', this technique can be also used in videos. Computer Vision tends to reduce the complexities in the human visualization system and understanding by applying convolutional neural network models to accurately recognize and classify objects from the potential and asymmetrical physical world. Classification of images is now very popular in recent years as the emergence of computer vision technology but it is quite challenging. In this paper, we proposed a VGG16 transfer learning model to classify eighteen categories of various sports and we have created our sports dataset which contains 9000 images. Our proposed model has shown a promising result which is 93%."
9022306,VisDrone-MOT2019: The Vision Meets Drone Multiple Object Tracking Challenge Results,"The Vision Meets Drone Multiple Object Tracking (MOT) Challenge 2019 is the second annual activity focusing on evaluating multi-object tracking algorithms on drones, held in conjunction with the 17-th International Conference on Computer Vision (ICCV 2019). Results of 12 submitted MOT algorithms on the collected drone-based dataset are presented. Meanwhile, we also report the results of 6 state-of-the-art MOT algorithms, and provide a comprehensive analysis and discussion of the results. The results of all submissions are publicly available at the website: http://www.aiskyeye.com/. The challenge results show that MOT on drones is far from being solved. We believe the challenge can largely boost the research and development in MOT on drone platforms."
9230587,Pothole and Speed Breaker Detection Using Smartphone Cameras and Convolutional Neural Networks,"Poor road conditions are one of the major causes for road accidents. Developing countries in particular are witnessing increased accident rates due to these poor road conditions. Potholes, deep ridges, missing pitches, improper speed breakers, poorly constructed manhole covers and slabs all combine to greatly increase the probability of serious accidents thus transforming roads into obstacle courses. In this study we have developed a model to detect unwanted potholes, deep ridges and speed breakers using computer vision and machine learning tools. We have developed a customized dataset (called Bumpy) that we use to train our machine learning algorithms. In this paper we propose a method where we use the Tensorflow pre-trained model to detect the potholes, deep ridges and speed breakers. Our experimental results demonstrate high accuracy although there are many obstacles on the road."
9548440,Detection and Breed Classification of Cattle Using YOLO v4 Algorithm,"Cattle detection is related to the determination of the position of cattle in the given image. Breed classification is classification of this detected animal by using the computer vision technique. This study is carried out to evaluate the performance of cattle detection and breed classification according to specific regions on the body of cattle that differ according to their breed. You Only Look Once (YOLO) the algorithm is used for cattle detection and breed classification on the data set. To demonstrate the effectiveness of the proposed technique in cattle detection and classification, a custom data set is created with images collected over Google Images. Experimental results show the success of using the YOLO algorithm in detection and breed classification on cattle images with 92.85% accuracy."
9130016,Human-Computer Interaction using hand gestures,"The human-computer interaction became an instrument of maximum importance in bringing the idea that the link between a user and a computer should look more and more like one between two human beings. Later becoming available for the common consumers, the idea attracted many additional fields. Human-computer interaction is now defined as a sum of other related fields named: computer science, cognitive science, and human factors engineering, which ease the communication between users and computers. Research in the human interaction field is done by innovation in the field of software tools, studies of computer-supported work, or intelligent agents. Cognitive models and gesture recognition are also part of this multidisciplinary initiative. The researchers design, construct and inquire new ways and tools to support human activity concerning computers. In the past few years, a big focus of human-computer interaction was the vocal part such as Microsoft's Cortana, Apple's Siri, or Amazon's Alexa. Also, the current trend includes more and more advancements in the fields of augmented reality and virtual reality. The paper is focusing on the interaction between the users' hands and computer programs. The starting point is the given device, a “LEAP MOTION” controller, which can pick up the gestures made by the user and transform them into signals that can be interpreted by the computer, through custom-made software."
9022405,Leveraging Vision Reconstruction Pipelines for Satellite Imagery,"Reconstructing 3D geometry from satellite imagery is an important and growing topic of research. However, disparities exist between how this 3D reconstruction problem is handled in the remote sensing context and how multi-view reconstruction pipelines have been developed in the computer vision community. In this paper, we explore whether state-of-the-art reconstruction pipelines from the vision community can be applied to the satellite imagery. Along the way, we address several challenges adapting vision-based structure from motion and multi-view stereo methods. We show that vision pipelines can offer competitive speed and accuracy in the satellite context."
8908665,Computer Vision-Assisted 3D Object Localization via COTS RFID Devices and a Monocular Camera,"In most RFID localization systems, acquiring a reader antenna's position at each sampling time is challenging, especially for those antenna-carrying robot or drone systems with unpredictable trajectories. In this article, we present RF-MVO that fuses RFID and computer vision for stationary RFID localization in 3D space by attaching a light-weight 2D monocular camera to two reader antennas in parallel. First, the existing monocular visual odometry only recovers a camera/antenna trajectory in the camera view from 2D images. By combining it with RF phase, we design a model to estimate a scale factor for real-world trajectory transformation, along with spatial directions of an RFID tag relative to a virtual antenna array due to the mobility of each antenna. Then we propose a novel RFID localization algorithm that does not require exhaustively searching all possible positions within the pre-specified region. Second, to speed up the searching process and improve localization accuracy, we propose a coarse-to-fine optimization algorithm. Third, we introduce the concept of horizontal dilution of precision (HDOP) to measure the confidence level of localization results. Our experiments demonstrate the effectiveness of proposed algorithms and show RF-MVO can achieve 6.23 cm localization error."
9670944,Computer Vision enabled Adaptive Speed Limit Control for Vehicle Safety,"Over speeding, especially by heavy vehicles, is the primary cause of accidents in India. The reason can be attributed to the lack of a framework to maintain strict road safety rules. This leads to heavy vehicles occupying a high-speed lane, which leads to frustration and rapid lane switching among passenger vehicles. With the recent advancements in Computer Vision and IoT, it is possible to enforce such safety rules without on-ground personnel. In this paper, we have proposed an IoT-based solution for vehicle speed control that uses computer vision to detect the lane and dynamically limit the vehicle's speed, thus discouraging higher speeds of certain vehicles on specific lanes. We have manually labelled ~1.2 lakh images of the TuSimple lane dataset for training the models. We have provided CNN models as a baseline and used a pixel counting based SVM method for detecting lanes which achieved CNN levels of accuracy while being computationally efficient. Our proposed solution aims to automate the regulation of speed on a per vehicle basis, which can be very effective in reducing the number of accidents in India."
9150816,Diagnosing Rarity in Human-object Interaction Detection,"Human-object interaction (HOI) detection is a core task in computer vision. The goal is to localize all human-object pairs and recognize their interactions. An interaction de-fined by a <; verb, noun> tuple leads to a long-tailed visual recognition challenge since many combinations are rarely represented. The performance of the proposed models is limited especially for the tail categories, but little has been done to understand the reason. To that end, in this paper, we propose to diagnose rarity in HOI detection. We propose a three-step strategy, namely Detection, Identification and Recognition where we carefully analyse the limiting factors by studying state-of-the-art models. Our findings indicate that detection and identification steps are altered by the interaction signals like occlusion and relative location, as a result limiting the recognition accuracy."
9334560,A Review on Computer Vision-Based Techniques for Autism Symptoms Detection and Recognition,"Autism is a mental disorder appearing in children as a delay in their social and communicational skills. ASD causes are still mysterious but scientists believe that they are caused by genetic defects. Diagnosing autism has been an exhaustive process that attracts several researchers' attentions. In this work, an overall description of autism its classes, signs and diagnosing protocols are covered. As computational technologies helped in assisting almost every field it added advantages in detecting and recognizing autism In this work, deep investigation of computer vision-based protocols in cooperation with machine learning technologies are discussed to propose autism diagnosing solution."
8990020,Self-Supervised Linear Motion Deblurring,"Motion blurry images challenge many computer vision algorithms, e.g., feature detection, motion estimation, or object recognition. Deep convolutional neural networks are stateof-the-art for image deblurring. However, obtaining training data with corresponding sharp and blurry image pairs can be difficult. In this letter, we present a differentiable reblur model for selfsupervised motion deblurring, which enables the network to learn from real-world blurry image sequences without relying on sharp images for supervision. Our key insight is that motion cues obtained from consecutive images yield sufficient information to inform the deblurring task. We therefore formulate deblurring as an inverse rendering problem, taking into account the physical image formation process: we first predict two deblurred images from which we estimate the corresponding optical flow. Using these predictions, we re-render the blurred images and minimize the difference with respect to the original blurry inputs. We use both synthetic and real dataset for experimental evaluations. Our experiments demonstrate that self-supervised single image deblurring is really feasible and leads to visually compelling results. Both the code and datasets are available at https://github.com/ethliup/SelfDeblur https://github.com/ethliup/SelfDeblur."
9354946,Task Mapping and Scheduling for OpenVX Applications on Heterogeneous Multi/Many-Core Architectures,"Computer vision applications have stringent performance constraints that must be satisfied when they are run at the edge on programmable low-power embedded devices. OpenVX has emerged as the de-facto reference standard to develop such applications. OpenVX uses a primitive-based programming model that results in a directed-acyclic graph (DAG) representation of the application, which can then be used for automatic system-level optimizations and synthesis to heterogeneous multi- and many-core platforms. Although OpenVX has been standardized, its state-of-the-art algorithm for task mapping and scheduling does not deliver the performance necessary for such applications to be deployed on heterogeneous multi-/many-core platforms. This article focuses on addressing this challenge with three main contributions: First, we implemented a static task scheduling and mapping approach for OpenVX using the heterogeneous earliest finish time (HEFT) heuristic. We show that HEFT allows us to improve the system performance up to 70 percent on one of the most widespread smart systems for applying computer vision and intelligent video analytics in general at the edge (i.e., NVIDIA VisionWorks on NVIDIA Jetson TX2). Second, we show that HEFT, in the context of a vision application for edge computing where some primitives may have multiple implementations (e.g., for CPU and GPU), can lead to load imbalance amongst heterogeneous computing elements (CEs), thus suffering from degraded performance. Third, we present an algorithm called exclusive earliest finish time (XEFT) that introduces the notion of exclusive overlap between single implementation primitives to improve the load balancing. We show that XEFT can further improve the system performance up to 33 percent over HEFT, and 82 percent over the native OpenVX scheduler. We present the results on a large set of benchmarks, including a real-world localization and mapping application (ORB-SLAM) combined with an NVIDIA inference application b...
(Show More)"
8411472,Computer-Vision Techniques for Water-Fat Separation in Ultra High-Field MRI Local Specific Absorption Rate Estimation,"Objective: The purpose of this paper is to prove that computer-vision techniques allow synthesizing water-fat separation maps for local specific absorption rate (SAR) estimation, when patient-specific water-fat images are not available. Methods: We obtained ground truth head models by using patient-specific water-fat images. We obtained two different label-fusion water-fat models generating a water-fat multiatlas and applying the STAPLE and local-MAP-STAPLE label-fusion methods. We also obtained patch-based water-fat models applying a local group-wise weighted combination of the multiatlas. Electromagnetic (EM) simulations were performed, and B1+ magnitude and 10 g averaged SAR maps were generated. Results: We found local approaches provide a high DICE overlap (72.6 ± 10.2% fat and 91.6 ± 1.5% water in local-MAP-STAPLE, and 68.8 ± 8.2% fat and 91.1 ± 1.0% water in patch-based), low Hausdorff distances (18.6 ± 7.7 mm fat and 7.4 ± 11.2 mm water in local-MAP-STAPLE, and 16.4 ± 8.5 mm fat and 7.2 ± 11.8 mm water in patch-based) and a low error in volume estimation (15.6 ± 34.4% fat and 5.6 ± 4.1% water in the local-MAP-STAPLE, and 14.0 ± 17.7% fat and 4.7 ± 2.8% water in patch-based). The positions of the peak 10 g-averaged local SAR hotspots were the same for every model. Conclusion: We have created patient-specific head models using three different computer-vision-based water-fat separation approaches and compared the predictions of B1+ field and SAR distributions generated by simulating these models. Our results prove that a computer-vision approach can be used for patient-specific water-fat separation, and utilized for local SAR estimation in high-field MRI. Significance: Computer-vision approaches can be used for patient-specific water-fat separation and for patient specific local SAR estimation, when water-fat images of the patient are not available."
9313909,Design of the Vision Control System for the Laboratory test rig,"The paper deals with the design of the vision control system for the laboratory test rig for spring stiffness measurement and simultaneously is presenting the project-based learning in the study program Mechatronics. The implementation of the machine vision is following two goals. Firstly, to decrease the number of the manual operations done by the operator by the spring testing and to extend the automatic controlled movement of the actuator during the testing, secondly to improve the safety of the operating staff during testing. The presented results were achieved in the frame of the projects which are leading to the master thesis in the study program Mechatronics."
9056818,Audomni: Super-Scale Sensory Supplementation to Increase the Mobility of Blind and Low-Vision Individuals—A Pilot Study,"Objective: Blindness and low vision have severe effects on individuals’ quality of life and socioeconomic cost; a main contributor of which is a prevalent and acutely decreased mobility level. To alleviate this, numerous technological solutions have been proposed in the last 70 years; however, none has become widespread. Method: In this paper, we introduce the vision-to-audio, super-scale sensory substitution/supplementation device Audomni; we address the field-encompassing issues of ill-motivated and overabundant test methodologies and metrics; and we utilize our proposed Desire of Use model to evaluate proposed pilot user tests, their results, and Audomni itself. Results: Audomni holds a spatial resolution of 80 x 60 pixels at ~1.2° angular resolution and close to real-time temporal resolution, outdoor-viable technology, and several novel differentiation methods. The tests indicated that Audomni has a low learning curve, and several key mobility subtasks were accomplished; however, the tests would benefit from higher real-life motivation and data collection affordability. Conclusion: Audomni shows promise to be a viable mobility device – with some addressable issues. Employing Desire of Use to design future tests should provide both high real-life motivation and relevance to them. Significance: As far as we know, Audomni features the greatest information conveyance rate in the field, yet seems to offer comprehensible and fairly intuitive sonification; this work is also the first to utilize Desire of Use as a tool to evaluate user tests, a device, and to lay out an overarching project aim."
9526923,A High-Precision Lens Distortion Correction Method for Vision Measurement,"The deviation caused by lens manufacturing and assembling will inevitably lead to imaging distortion of the vision system. For high-precision vision measurement tasks, the lens distortion must be accurately corrected. Aiming at the application of planar object geometry measurement, an efficient lens distortion correction method is proposed, it can obtain the distortion parameters of each pixel and improve the correction accuracy through multiple measurements. A chessboard plate is placed on the measurement plane and captured, the map from pixel coordinates to chessboard coordinates is calculated for each image. Using affine model, chessboard coordinates under several random movements are warped to the original coordinate system, and then amortization error method is applied on these frames of images to achieve higher correction accuracy."
8882216,SoCodeCNN: Program Source Code for Visual CNN Classification Using Computer Vision Methodology,"Automated feature extraction from program source-code such that proper computing resources could be allocated to the program is very difficult given the current state of technology. Therefore, conventional methods call for skilled human intervention in order to achieve the task of feature extraction from programs. This research is the first to propose a novel human-inspired approach to automatically convert program source-codes to visual images. The images could be then utilized for automated classification by visual convolutional neural network (CNN) based algorithm. Experimental results show high prediction accuracy in classifying the types of program in a completely automated manner using this approach."
9035305,Optimal U-Net Architecture for Object Recognition Problems in Multispectral Satellite Imagery,"Geospatial data follows Moore's law. On the back of improvements of optical Earth observation satellite hardware [1] (weight, propulsion systems, signal transmission and resolution) as well as reduced costs of rocket launch carrying these satellites, number of nanosatellites deployed to Lower Earth Orbit (LEO) in 2018 was larger than in the previous 10 years combined [2]. This allowed an exponential growth in satellite imagery data production that is available for military and commercial use. Machine learning tools enable us to process high resolution, multi-spectral satellite imagery data to recognize objects at scale [3] and generate insights with practical industry applications. It allowa us to calculate global oil reserves, track tanker ships or estimate retail revenue based on the car count, all exceptionally valuable financial information. In this paper, we investigate various computer vision techniques to develop an optimal machine learning technique for object recognition problems at this unique type of the dataset: multi-spectral satellite imagery."
9190858,Input Dropout for Spatially Aligned Modalities,"Computer vision datasets containing multiple modalities such as color, depth, and thermal properties are now commonly accessible and useful for solving a wide array of challenging tasks. However, deploying multi-sensor heads is not possible in many scenarios. As such many practical solutions tend to be based on simpler sensors, mostly for cost, simplicity and robustness considerations. In this work, we propose a training methodology to take advantage of these additional modalities available in datasets, even if they are not available at test time. By assuming that the modalities have a strong spatial correlation, we propose Input Dropout, a simple technique that consists in stochastic hiding of one or many input modalities at training time, while using only the canonical (e.g. RGB) modalities at test time. We demonstrate that Input Dropout trivially combines with existing deep convolutional architectures, and improves their performance on a wide range of computer vision tasks such as dehazing, 6-DOF object tracking, pedestrian detection and object classification."
9574492,Siamese Region Proposal Tracker with Feature Pyramid Network,"Visual object tracking is a fundamental task in computer vision, and numerous tracking algorithms are proposed to improve both accuracy and robustness. However, recent trackers often adopt the feature from the last layer to track the target, which ignores the detailed information from the shallow layers. This paper aims to aggregate both shallow and deep features to improve the tracker's performance. Inspired by Feature Pyramid Network (FPN) in object tracking, we add a bottom-up pathway in the Siamese-RPN to obtain a more representative performance. We test our method on OTB-100, and we obtain an overall 0.617 maximum success rate. The experimental results show the strength of the shallow feature to improve the tracking accuracy."
9402606,Pest Detection on Leaf using Image Processing,"A survey report showed that 70% Indian population depends on agriculture sector. Numerous heterogeneous diseases and various kind of pests affect the production of crops which leads to quality and quantitative loss. Automatic in-field pest detection using computer vision technique is an important topic in modern intelligent agriculture but suffers from serious challenges including complexity of wild environment, detection of tiny size pest and classification into multiple classes of pests. In the literature attention is mostly focused on machine learning based techniques and image processing has not received equal attention. This paper illustrates an automatic approach for pest detection using Wavelet transformation and Oriented FAST and rotated BRIEF (ORB). The core objective of the research is to enhance feature extraction phase to improve the detection efficiency. The proposed approach is implemented on images of fluffy caterpillar pests on mustard crop and fava bean collected from farms in Rajasthan. The experimental results affirm the efficiency of the proposed approach."
9114684,Optimized Distribution of an Accelerated Convolutional Neural Network across Multiple FPGAs,"Convolutional Neural Networks (CNN) have achieved a resounding success especially in computer vision and collaborative filtering. The general trend in CNN architectures has been to build deeper networks with a substantial number of convolution filters and several large feature maps. As a result, most of the current CNN inference routines are highly compute-intensive and have significant storage requirements. Field Programmable Gate Arrays (FPGAs) are among the most popular choices for accelerating CNN inference workloads as they can perform complex and massively parallel jobs. Recently, notable efforts have been made to distribute CNN inference workloads across multiple FPGAs [1]. These strategies, however, do not take into account variations in computational complexity across different layers of a CNN resulting in suboptimal performance gains. This work proposes an optimal distribution of CNN layers across different FPGA nodes while accounting for each layer’s performance to achieve maximum overall throughput."
9465880,A Computer Vision Supported Investigation on Anxiety-Success Relationship in Educational Processes,"It is a common knowledge that fear and anxiety affect the success during an examination. In this work we examine the relationship between anxiety and success by using computer vision techniques. In our experiments, firstly we classify different kind of emotions extracted from videos and then investigate the effects of them on success as exam scores. Additionally, we use the same techniques to implement an effective proctoring during examinations which is very important process for online educations that become widespread due to the pandemic."
9363976,Deep Learning Platforms for High-Performance and Distributed Computing Object Detection,"Deep learning developed in the last decade and has been established as recent, modern and very promising technique with large potential to be successfully applied in various domains. Despite deep learning outperforms alternative techniques in providing solution to many different problems there are several concerns and limitations of the its usage, one of them being the computational complexity of the training process. The paper discusses the possibilities of taking advantage of high-performance and distributed computing for efficiently and effectively training deep learning models for computer vision applications. A comparison of deep learning platforms and frameworks utilized on high-performance computer systems is presented and their application for computer vision problem of object detection is evaluated based on a suggested parallel computation architectural model and utilization of RetinaNet model."
9449154,A Vision-Based Fruit Packaging Robot,"The rapid development of computer vision makes human-computer interaction possible and has a wide application prospect. Since the first case of COVID-19 was found, up to now, the global number of infected people has reached 119220681 [1]. In China, although the epidemic is under control, affected by the external environment, the epidemic will also spread through cold chain transportation, and imported fruits may also carry the virus. In order to solve this problem, based on deep learning, we collected 300 fruit images and used YOLOV5 to recognize some common fruit images. The experimental results are shown in the proposed method, and the average accuracy can reach about 84%."
9515926,A Radar Image Compression Algorithm Based on Machine Vision,"Modern radars have high resolution and large data volume, and the echo contains a large amount of radar image data, this has caused great difficulties for the high-speed transmission and real-time storage of radar data. The existing radar compression algorithm has high complexity and cannot meet the real-time requirements. In response to this problem, through the analysis of the actual radar echo data, it is found that the data has the characteristics of strong correlation and short correlation length. On this basis, this article is based on machine vision and morphological image processing, by setting the neighborhood search range and priority, to improve the search method of the radar image target contour. Experimental tests show that this radar lossy compression algorithm has low complexity and real-time characteristics."
9428974,Analysis of Pupil Dilation on Different Emotional States by Using Computer Vision Algorithms,"Measurement of pupil diameter can be helpful to study human mental health. In medical science pupillary tracking helps the clinicians to detect the level of depression, stress and anxiety. Many studies have been undertaken to detect emotional states of a person from his pupil movement as well as size. Pupil diameter measurement follows the stages of detecting face, detecting eyes and detecting pupil. In this paper we have analyzed the effect of three emotions, i.e., happy, sad and surprise on the pupil diameter using the computer vision techniques."
9676383,Communication System For Non-Verbal Paralyzed Patients Using Computer Vision,"This paper presents a solution for assisting the paralyzed patients in their day to day lives, by integrating a system that would be controlled by their eyes. The presented system contains a set of different messages in a variety of languages that the user can select from without any speech or physical movement, other than eyes. This system uses computer vision to translate the movement of eye-ball to the cursor on the screen, and eye-blinking to register clicks on the computer to trigger the desired output. The technique used to implement object detection in the following system is called HOG (Histogram of Oriented Gradient). The system is tested primarily on two datasets, one of which is an open-source dataset containing pictures of people, obtained from Kaggle, and the other one is collected locally at DHA Suffa University."
9176333,Prediction of Patient Demographics using 3D Craniofacial Scans and Multi-view CNNs,"3D data is becoming increasingly popular and accessible for computer vision tasks. A popular format for 3D data is the mesh format, which can depict a 3D surface accurately and cost-effectively by connecting points in the (x, y, z) plane, known as vertices, into triangles that can be combined to approximate geometrical surfaces. However, mesh objects are not suitable for standard deep learning techniques due to their non-euclidean structure. We present an algorithm which predicts the sex, age, and body mass index of a subject based on a 3D scan of their face and neck. This algorithm relies on an automatic pre-processing technique, which renders and captures the 3D scan from eight different angles around the x-axis in the form of 2D images and depth maps. Subsequently, the generated data is used to train three convolutional neural networks, each with a ResNet18 architecture, to learn a mapping between the set of 16 images per subject (eight 2D images and eight depth maps from different angles) and their demographics. For age and body mass index, we achieved a mean absolute error of 7.77 years and 4.04 kg/m 2 on the respective test sets, while Pearson correlation coefficients of 0.76 and 0.80 were obtained, respectively. The prediction of sex yielded an accuracy of 93%. The developed framework serves as a proof of concept for prediction of more clinically relevant variables based on 3D craniofacial scans stored in mesh objects."
9303201,Using Computer Vision and Deep Learning for Cells Recognition,"The task of the objects identification, counting, and measurement is a huge part of scientific investigations and technological applications. Automated methods using traditional processing such as segmentation, edge detection, and so on represented by available software (e.g. CellProfiler) are not flexible, can be used only with images of high-quality, and in addition require setting a part of parameters by hand. This contribution presents the applying the deep learning method for recognition of HeLa cells expressing green fluorescent protein (EGFP) automatically. We used Cascade Mask R-CNN neural networks which has a ResNeXt backbone and deformable convolutional networks layers. Training dataset contained seven pictures with 5754 labeled cells. Three images with 2469 labeled cells were used as test-dataset. The trained neural network showed mAP=0.4."
9280268,Glasses Connected to Google Vision that Inform Blind People about what is in Front of Them,"More than a billion people around the world have vision problems for different reasons, and these numbers are increasing every year. This leads us to make different innovations in the field of computer vision, with the aim of providing a better quality of life for these people. In this document we present as a resource, the development of an intelligent lens, which incorporates a Raspberry Pi ZW connected to the Google Cloud Vision API through the Wifi of the user's mobile phone, where at the touch of a button, the Raspberry camera, captures the image, processes it in a few seconds and retrieves its main features, obtaining important information for mobilization such as: pedestrian crossings, bus stop sign, vehicles, green light, etc. 150 people from the National Union of the Blind of Peru (NUBP) were evaluated with different degrees of blindness, obtaining a 40.5% increase of independence for their mobilization."
9610823,UAV obstacle detection with bio-motivated computer vision,"Unmanned aerial vehicles (UAVs) are becoming more and more common. They show excellent potential for multiple types of autonomous work, although they must achieve these tasks safely. For flight safety, it must be assured that the UA V will not endanger its surroundings during autonomous operations; it will avoid collision with any objects in its flight path. Camera-based computer vision and artificial neural networks have shown to be effective in many applications. However, biological vision systems and the brain areas responsible for visual processing may hold solutions capable of acquiring information effectively. Previous work has shown the usability of biologically motivated algorithms using vision systems of insects or even behavioral patterns to solve computer vision problems. We are proposing a novel system, which performs visual cue extraction with algorithms based on the structure and functionality of the retina and the visual cortex of the mammalian visual system. We are also developing a modular artificial neural network with a training dataset, which will perform autonomous obstacle recognition tasks using the data from the image processing algorithm."
9085096,Assistive Mobile Robot with Shared Control of Brain-Machine Interface and Computer Vision,"The daily life of stroke patients may be severely limited. In this paper, we proposed a shared control based assistive mobile robot system to improve their ability. Brain-machine interface (BMI) was used for obtaining the user's intention to give a command to the robot. The computer vision technology with laser radar and camera was equipped to detect the environment for obstacle avoidance and target recognition. With the spatial coordinate provided by the vision part, the robot would grasp the target back to the user eventually. The test was carried out on both the BMI based console part and the robot executive. The results showed good performance with satisfied BMI accuracy, quite small route planning error, recognition error and motion planning error, which make us convinced that this assistive mobile robot can help the disabled people."
8819632,A Novel Approach to Age Classification from Hand Dorsal Images using Computer Vision,"Estimation of age of any sample (non-living) or living, comes into play in multiple domains. For instance, archaeological research involves age estimation of the excavations. In case of accidents and murder, if the body of the victim is dismembered or if the victim is unrecognizable, then in general investigations, rough age estimation of the individual (victim) is very crucial from the legible body parts like hair or hand. Also apart from general investigation, in criminal cases, Forensic Science is a very important domain where age evaluation of the victim is done via Forensic Research which involves thorough study of the sample (body part) followed by multiple tests. This study aims at classification of an individual into 3 age groups (<;22, 22-26, >26) from the image of his or her dorsal side of hand. A Deep Learning-Machine Learning Hybrid Model is developed yielding a Validation Accuracy of 100% and Recall of 1.0. S uch an Intelligent System can prove to be very swift and highly essential for Forensic Research."
8857420,Obstacle Recognition using Computer Vision and Convolutional Neural Networks for Powered Prosthetic Leg Applications,"In this work we combine computer vision and a machine learning algorithm, Convolutional Neural Networks (CNNs), to identify obstacles that powered prosthetic leg users might encounter during walking. Our motivation is that powered prosthetic legs could react in synchronicity with their users by recognizing and anticipating the terrain in front of them. We focus on identifying stairs and doors that are within the visual field of a person. To achieve this, we used a compact CNN architecture to optimize image processing for real-time applications. We built and tested a wearable system prototype that included a camera mounted on a pair of glasses and a single-board computer. The prototype was used by able-bodied users to collect and label obstacle and non-obstacle videos, which were used later to train the CNN. In validation, the system was able to recognize around 90% of obstacles across different indoor and outdoor scenarios. The accuracy achieved and the practicality of the prototype shows the potential of computer vision and machine learning in the field of powered prosthetic legs."
9123060,Review on Image Processing Based Adversarial Example Defenses in Computer Vision,"Recent research works showed that deep neural networks are vulnerable to adversarial examples, which are usually maliciously created by carefully adding deliberate and imperceptible perturbations to examples. Several states of the art defense methods are proposed based on the existing image processing methods like image compression and image denoising. However, such approaches are not the final optimal solution for defense adversarial perturbations in DNN models. In this paper, we reviewed two main approaches to deploying image processing methods as a defense. By analyzing and discus!sing the remaining issues, we present two open questions for future research direction including the definition of adversarial perturbations and noises, the novel defense-aware threat model. A further research direction is also given by re-thinking the impacts of adversarial perturbations on all frequency bands."
9545055,iPonder: A Novel Approach to Multimodal Mental Health Diagnosis and Therapy for Teens via Deep Transfer Learning and Computer Vision,"Suicide is the second leading cause of death for teenagers, tarnishing the future potential of the world. More teenagers die from suicide than from cancer, heart disease, stroke, and many more diseases, combined. Although the greatest mental health improvement occurs from a conversation, over 50% of suicidal teenagers stay quiet about their depression due to shame. Thus, iPonder, a cross-platform accessible mobile application for teenagers to privately discuss, confront, and overcome their depression, was created. Using a four-level diagnosis approach based on user data, responses to psychological questionnaires, choice specification, and free-response answers, iPonder constructs a hyper-personalized user profile, which can then be used to pair teenagers anonymously online with similar profiles to discuss and vent about their issues. Questionnaires include the Patient Health Questionnaire-9, General Anxiety Disorder- 7, UCLA Loneliness Scale, and Myers-Briggs Personality Type Indicator test. Using Reddit's Self-Reported Mental Health Diagnosis text-corpus with over 200,000 data points, 11 artificial neural networks were constructed to predict user risk of various mental illnesses based on their free-text-responses, using transfer learning from a token-based text embedding neural network with over 48 million parameters trained on the English Google News 7B corpus with over 4 billion data points. Additionally, two other artificial neural networks were created, trained on the Distress Analysis Interview Corpus with tremendous labeled video data regarding facial landmark tracking and audio analysis for depression-diagnosed people, to predict the risk of depression from a user-submitted video. All neural networks were evaluated on their accuracy, precision, recall, and f1-score. Besting state-of-the-art metrics on similar tasks in the relevant literature, iPonder achieves over 95% accuracy, recall, precision, and f1-score per neural network, pointing to a new age of acce...
(Show More)"
8803027,Single Image Depth Estimation Using Deep Adversarial Training,"Scene understanding is an active area of research in computer vision that encompasses several different problems. The LiDARs and stereo depth sensor have their own restrictions such as light sensitiveness, power consumption and short-range [1]. In this paper, we propose a two-stream deep adversarial network for single image depth estimation in RGB images. For stream I network, we propose a novel encoder-decoder architecture using residual concepts to extract course-level depth features. Stream II network purely processes the information through the residual architecture for fine-level depth estimation. Also, we designed a feature map sharing architecture to share the learned feature maps of the decoder module of stream I. Sharing feature maps strengthen the residual learning to estimate the scene depth and increase the robustness of the proposed network. A benchmark NYU RGB-D v2 [2] database is used to evaluate the proposed network for single image depth estimation. Both qualitative and quantitative analysis has been carried out to analyze the effectiveness of the proposed network for scene depth prediction. Performance analysis shows that the proposed method outperforms other existing methods for single image depth estimation."
9710145,Revealing the Reciprocal Relations between Self-Supervised Stereo and Monocular Depth Estimation,"Current self-supervised depth estimation algorithms mainly focus on either stereo or monocular only, neglecting the reciprocal relations between them. In this paper, we propose a simple yet effective framework to improve both stereo and monocular depth estimation by leveraging the underlying complementary knowledge of the two tasks. Our approach consists of three stages. In the first stage, the proposed stereo matching network termed StereoNet is trained on image pairs in a self-supervised manner. Second, we introduce an occlusion-aware distillation (OA Distillation) module, which leverages the predicted depths from StereoNet in non-occluded regions to train our monocular depth estimation network named SingleNet. At last, we design an occlusion-aware fusion module (OA Fusion), which generates more reliable depths by fusing estimated depths from StereoNet and SingleNet given the occlusion map. Furthermore, we also take the fused depths as pseudo labels to supervise StereoNet in turn, which brings StereoNet’s performance to a new height. Extensive experiments on KITTI dataset demonstrate the effectiveness of our proposed framework. We achieve new SOTA performance on both stereo and monocular depth estimation tasks."
9607624,Limited Sampling Reference Frame for MaskTrack R-CNN,"With the great achievement for the computer vision tasks, e.g., image classification, object detection and segmentation, people are diving into more complex vision tasks. Video instance segmentation is a new task which includes detection, segmentation and tracking of instances simultaneously in a video. Occluded Video Instance Segmentation (OVIS) is used for this task, and it includes many heavily occluded scenes. Besides, there is a long range for the length of videos in this dataset. In order to track instances in videos with different lengths, we make some improvements based on MaskTrack R-CNN. Based on these optimizations, a refinement model can be well used to detect and segment instances, which acquires a better track accuracy in long videos. Furthermore, we apply Stochastic Weights Aver-aging training strategy to get a better result. Finally, The proposed method can achieve the mAP score of 28.9 for the validation set and 32.2 for the test set on the OVIS dataset."
8743060,Ways for Improving Efficiency of Computer Vision for Autonomous Vehicles and Driver Assistance Systems,"This paper presents analysis of human vision features which affects the development of systems of computer vision. Human eyes have known limitations and defects. One of characteristics of vision, for example, the field of vision should be taken into account at the arrangement of cameras on an unmanned vehicle. This characteristic has a great influence on safety of driving and can considerably change depending on many factors. The authors analyzed the range and reasons of changes of the mentioned factor. Also, the paper presents parameters for assessment of the field of vision. Computer vision systems have to fulfill many requirements connected with operational conditions, for example, backlight, illumination overfalls, and great changes of brightness, motion in darkness. The variant of arrangement of two cameras for proving binocular vision is presented."
9281214,Methodological recommendations to face an online semester in a catastrophic situation: the case of a computer science program during the COVID-19 lockdown,"The pandemic context forced most of the Higher Education Institutions, and therefore their faculties, to face a semester of teaching in an online mode. The universities defined general guidelines and strategies to try to guarantee a minimum quality of the process. However, the literature reports that, depending on its particularities, preparing a semester-long online course requires a design of several months. Therefore, each lecturer worked according to her experience, guidelines, and support she received: ""What could be done, was done."" This article reports the teaching experience of the 1st semester 2020 of the Computer Science undergraduate program at the Universidad Católica de Temuco. Some lecturers of this undergraduate program had previous experience in online training; hence they designed strategies that differed in teaching, evaluation, and interaction methodologies while using several technologies available to them. A sample of 115 of our students answered a questionnaire to assess the preferences and problems during this semester. The results show a heterogeneous set of clusters. However, they point to a constructivist asynchronous vision in a group of our students that prefer short videos uploaded by their professors, and the resources that, by their means, they find on the Internet, point towards a constructivist asynchronous vision in a group of our students. The results also allowed us to rescue good practices that help in proposing an emergency online teaching model that will guide the design of the second-semester courses."
9729402,ViP: A Hierarchical Parallel Vision Processor for Hybrid Vision Chip,"Nowadays, the vision chip bridging sensing and processing has been extensively employed in high-speed image processing, owing to its excellent performance, low power consumption, and economical cost. However, there is a dilemma in designing processors to support conventional computer vision algorithms and neural networks since the two algorithms have a non-trivial trade-off in proposing a unified architecture. By analyzing computation properties, we propose a novel hierarchical parallel vision processor (ViP) for hybrid vision chips to accelerate both traditional computer vision (CV) and neural network (NN). The ViP architecture includes three parallelism levels: PE for pixel-centric, computing core (CC) for block, and vision core (VC) for global. PEs contain dedicated computing units and data paths for convolution operations without degrading its flexibility. Each CC is driven by customized SIMD instructions and can be dynamically connected for meeting block parallelism requirements. ViP is fabricated in 65nm CMOS technology and achieves a peak performance of 614.4 GOPS and an energy efficiency of 640 GOPS/W at 200 MHz clock frequency. Notably, several experiments on CV and NN are performed, illustrating an ultra-low latency in executing hybrid algorithms."
9214853,Computer Vision to Automatically Assess Infant Neuromotor Risk,"An infant's risk of developing neuromotor impairment is primarily assessed through visual examination by specialized clinicians. Therefore, many infants at risk for impairment go undetected, particularly in under-resourced environments. There is thus a need to develop automated, clinical assessments based on quantitative measures from widely-available sources, such as videos recorded on a mobile device. Here, we automatically extract body poses and movement kinematics from the videos of at-risk infants (N = 19). For each infant, we calculate how much they deviate from a group of healthy infants (N = 85 online videos) using a Naïve Gaussian Bayesian Surprise metric. After pre-registering our Bayesian Surprise calculations, we find that infants who are at high risk for impairments deviate considerably from the healthy group. Our simple method, provided as an open-source toolkit, thus shows promise as the basis for an automated and low-cost assessment of risk based on video recordings."
9607456,From VIS To OVIS: A Technical Report To Promote The Development Of The Field,"Occluded Video instance segmentation(OVIS) is a new vision task that has emerged in this years and is processed by video deep learning algorithms. It uses continuous video frames as input, generally ranging from a few frames to hundreds of frames. Before OVIS, there has a task called VIS. To tackle the task of OVIS and VIS, we design a new alghorithm called SimVTR, which based on DETR and VisTR. During the experiment, although we acquire the 27.66 mAP on OVIS test, 25.18m AP on OVIS val, and 31.9 mAP on VIS test, we have found a surprising phenomena that the evaluation mechanism is not sensitive to our mothod SimVTR. When we only use one frame to inference, the model can acquire the similar mAP as dozens frames. SimpleVTR trade off and optimizes the computing resources and effects of end-to-end video instance segmentation algorithm. We used one RTX1080Ti (11G) to experiment, and the batch size can change from 1 to 16 frames. We were surprised to find that only one frame can also get a very high score in inference. The VIS and OVIS cocoapi have some unreasonable place in ytvoseval.py. In this technical report, we prudently point out the phenomena that the evaluation mechanism could have some bug. If this is true, we need check our model to promote the process of the video instance segmentation."
9607508,The VAROS Synthetic Underwater Data Set: Towards realistic multi-sensor underwater data with ground truth,"Underwater visual perception requires being able to deal with bad and rapidly varying illumination and with reduced visibility due to water turbidity. The verification of such algorithms is crucial for safe and efficient underwater exploration and intervention operations. Ground truth data play an important role in evaluating vision algorithms. However, obtaining ground truth from real underwater environments is in general very hard, if possible at all.In a synthetic underwater 3D environment, however, (nearly) all parameters are known and controllable, and ground truth data can be absolutely accurate in terms of geometry. In this paper, we present the
VAROS
environment, our approach to generating highly realistic under-water video and auxiliary sensor data with precise ground truth, built around the Blender modeling and rendering environment.
VAROS
allows for physically realistic motion of the simulated underwater (UW) vehicle including moving illumination. Pose sequences are created by first defining waypoints for the simulated underwater vehicle which are expanded into a smooth vehicle course sampled at IMU data rate (200 Hz). This expansion uses a vehicle dynamics model and a discrete-time controller algorithm that simulates the sequential following of the waypoints.The scenes are rendered using the raytracing method, which generates realistic images, integrating direct light, and indirect volumetric scattering. The
VAROS
dataset version 1 provides images, inertial measurement unit (IMU) and depth gauge data, as well as ground truth poses, depth images and surface normal images."
9443449,Detecting Adversarial Images via Texture Analysis,"Neural networks have been shown to be vulnerable to carefully crafted adversarial examples. Recently, new adversarial attacks, including dispersion reduction (DR), have been proposed, and shown to be transferable across different computer vision tasks. This means that an ensemble of different defense/detection mechanisms can be evaded all at once. Unlike previous attack methods, the DR attack minimizes the dispersion of an internal feature map providing state-of-the-art results. In this paper, we propose an algorithm to detect the adversarial examples generated by different adversarial attacks, including the dispersion reduction, projected gradient descent, diverse inputs method and momentum iterative fast gradient sign method. Our approach employs 1D Gabor filter responses, and detects adversarial examples generated from different surrogate neural network models and datasets with high accuracy."
9152859,An Introduction to Zero-Shot Learning: An Essential Review,"With deep learning achieving more successful results than traditional machine learning methods, researches in the field of computer vision have evolved towards this area. However, in order to obtain successful models in deep learning methods, it needs a large number of training samples similar to traditional machine learning methods. In order to meet this requirement, auxiliary information of visual data has been used in recent years. Zero-shot learning methods focused on the compatibility functions of image embeddings and class embeddings, and researches aimed at better representation of class embeddings on visual data. In this paper, recent studies on zero-shot learning have been examined and evaluated."
9603737,Research on Video Classification Based on Deep Learning,"At present, the video classification is a challenging computer vision problem, and there are few researches in this field. For ordinary image video which contains many uncertain factors such as complex background and illumination changes. However, human skeleton image video has the advantages of simple content and less redundant information. But for such a simple video, classification is also difficult. With the development of deep learning, this paper combines the method of deep learning to detect and classify human skeleton behavior. The purpose of this study is to help researchers to label large-scale bone behavior data, reduce the pressure of follow-up research, and it can expand to the field of short video classification in the future. After video preprocessing operations such as frame and image down-sampling, the processed data is input into the model for convolution. The next is to extract frame sequence features through LSTM layer. Finally, the full connection is performed to output the probability of classification labels. This method has high accuracy in classifying human skeleton behavior."
9467188,Thermopile Array Sensor for Children Detection in Automobile Application,"Identifying children in thermal image using thermopile arrays sensor is vital for several computer vision programs, such as MATLAB, especially for detecting children left unattended in a parked automobile under extreme high temperature. Several fatal cases have been reported due to parents' negligence, and majority has shown regretting of the mishap. The purpose of this report is to propose a dynamic background removal algorithm for human movement detection. Besides, this project aims to develop the proposed algorithm into image processing system. This can be achieved by design and setup a thermal imaging camera using thermopile array sensor, powered by microcontroller which is Raspberry Pi and then processing the thermal image by applying Background Subtraction (BS) technique to extract the foreground or human object from the background image. The expected result from this project is the extraction of foreground image (human) from background image for human detection. By adding other method such as masking method, the final outcome of the exact human detection from the thermal image can be obtained."
9513083,Location of Irregular Holes Based on Machine Vision,"In the actual environment, the various deformations of the positioning holes of the mechanical parts and the different backgrounds will cause the circle to have irregular boundaries, which will cause the identification and positioning of the center of the circle to be inaccurate. Therefore, this paper proposes a centroid compensation algorithm based on the centroid of a circular hole. The algorithm first performs image preprocessing on the circular hole to enhance its contrast, and then performs binary and filtering processing to find its centroid. According to the binary image in the pixel The area occupied by the plane is used to identify, determine and compensate the center position of the circular hole. Experiments show that the algorithm can accurately locate the center of irregular holes with deformation, and has high accuracy and stability."
9169032,Automatic Color Recognition Technology of UAV Based on Machine Vision,"With the rapid development of artificial intelligence technology, machine vision, as a branch of it, has been widely used in various fields. Among them, color recognition is an important application in machine vision. Based on the background of the international UAV innovation competition, this paper studies the real-time color direction light image collected by the UAV camera, and proposes an automatic color recognition model of UAV based on machine vision. In this paper, the characteristics of the directional signal light image acquired by UAV under different color space models and its influence on the color recognition effect are expounded, and the principle and conversion method of the HSV color space model are emphatically discussed. Through image edge detection, erosion and dilation and closed operation processing, the geometric features of the image of the directional signal light are extracted, thereby identifying the color of the directional signal light. Finally, the experiment of automatic recognition of the color of the airport signal lights shows that the proposed color recognition model can effectively detect the objects in complex background."
9602880,Efficient Hand Gesture Recognition System based on Computer Vision: An Overview,"Gesture recognition has been applied in many fields such as human-machine interface (HCI), home automation, robot control, etc. Hand gestures can be considered as a form of nonverbal communication. Various techniques including software and hardware are used to improve hand gesture recognition. Unlike the use of sensors for gesture detection, the recognition algorithm can be as simple as conventional image processing methodology based on rule-based classification algorithms, or complicated as deep learning identification that uses large datasets and computational resources to achieve dynamic gesture recognition. This paper provides a brief review of the approaches for gesture recognition and applications in various fields on computer vision."
9463487,Deep Learning Object Detection Techniques for Thin Objects in Computer Vision: An Experimental Investigation,"Efficient detection of thin objects, from stationary or moving images, is significant in a variety of research areas. These research areas include but are not limited to electric power line detection systems, sperm tail detection for clinical sperm research, mooring lines detection, road-lane line detection for autonomous vehicles, and cracks detection for the integrity assessment of building structures. However, the detection of thin objects is a challenging computer vision task owing to the slimmer and less compact nature of these objects. Moreover, the complexity present in certain images, such as the background clutter, further adds to this problem of accurately detecting thin objects. In this work, we investigate a series of state-of-the-art deep learning detectors for thin objects' detection. The detectors examined in this work were: EfficientDet, YOLOv5 and U-Net. The experimental results of this study reveal that generic state-of-the-art deep detectors are not suitable for detecting thin objects due to their reliance on coarse bounding boxes and/or excessive pixel-level computations while the application-specific detectors possess poor generalization capabilities and do not work accurately outside their domains. These empirical findings indicate the necessity of the identification of critical factors affecting thin objects detection and the subsequent design of a generic thin objects' detector."
9397164,Automated Student Review System with Computer Vision and Convolutional Neural Network,"Detecting Emotion from a face is a prevalent topic, and computer vision researchers are continuously working on perfecting this challenging task. Facial emotion is widely used in applications like Snapchat, Face apps, Cameras, etc., to predict emotions from faces, to detect smiles, and many more. The task is very challenging, as a person's facial features vary from one to another. As convolutional neural networks can detect such complicated features so, the idea of deep learning has been used to tackle this problem. We have tried to predict facial emotion from images and generate reviews from that. Since there are already many works on emotion detection, we focused mainly on its application. Our main aim is to design a system that can generate automated reviews from human emotion. We have labeled different facial expressions with some scores and used that score to predict student review in the classroom."
8857807,Assessment of Laboratory Mouse Activity in Video Recordings Using Deep Learning Methods,"Analysis of laboratory animal behavior allows assessment of animal wellbeing. We present a method for the classification of different activities of laboratory mice by analyzing video clips using three deep learning methods. Animals placed in observation cages are filmed and short video clips are labelled as belonging to one of five defined behaviors. Subsequently, three different methods based on convolutional neural networks (CNNS) are applied to classify the clips. The best performing method - a two-stream network that analyzes individual frames as well as the video's optical flow - achieves an accuracy of 86.4%, including detection of important behavioral patterns such as self-grooming. These results show that the presented analysis protocol allows automated assessment of animal behavior by algorithmic analysis of videos of mice on observation boxes."
9528162,Obstacle Detection and Avoidance For Mobile Robots Using Monocular Vision,This paper proposes a robust approach for obstacle detection and avoidance algorithm using a single camera. Monocular Vision using single camera architecture cannot identify depth with a single image and thus depends on pixel gradient or keypoint extractors to identify traversable path and obstacles. Pixel gradient does not work well where there are shadows and sharp illumination changes and keypoint extractor does not work well in the absence of dense texture. In this paper we propose an algorithm that is able to use edges as keypoints along with pixel gradient. The entire algorithm was successfully tested on Sphero RVR Rover platform that uses Raspberry Pi and a color camera with IR. The proposed method performs well in obstacle detection and obstacle avoidance and is potentially an alternative to a binocular solution.
8935962,Development of Tank-Based Military Robot and Object Tracker,"Indonesia is a country whose economy is developing very fast. According to World Bank data, Indonesia is the country with the largest economic development in Southeast Asia. Unfortunately, the threat of terrorism in Indonesia is very large, therefore it requires a smart system such as military robots. Today's vision-based robot technology has also been used in the development of autonomous military robots. A sophisticated military robot is a robot that is needed by the military/police because it can be deployed to the battlefield or the eradication of terrorism in a remote or autonomous manner. This system is needed to reduce the remaining casualties from the army, and this combat robot system can also be operated at any time with more numbers than regular soldiers and with minimal operator needs. In this paper, we propose a prototype of a tank-based military robot with object detection and tracking and turrets for simulation of shooting the enemy target based on the computer vision. The object tracker will detect an upper body of the target using a library in OpenCV and the tank will track until it reaches the best position to shoot the target. The methods explained, and experimental results were presented."
9578848,Causal Attention for Vision-Language Tasks,"We present a novel attention mechanism: Causal Attention (CATT), to remove the ever-elusive confounding effect in existing attention-based vision-language models. This effect causes harmful bias that misleads the attention module to focus on the spurious correlations in training data, damaging the model generalization. As the confounder is unobserved in general, we use the front-door adjustment to realize the causal intervention, which does not require any knowledge on the confounder. Specifically, CATT is implemented as a combination of 1) In-Sample Attention (IS-ATT) and 2) Cross-Sample Attention (CS-ATT), where the latter forcibly brings other samples into every IS-ATT, mimicking the causal intervention. CATT abides by the QK-V convention and hence can replace any attention module such as top-down attention and self-attention in Transformers. CATT improves various popular attention-based vision-language models by considerable margins. In particular, we show that CATT has great potential in large-scale pre-training, e.g., it can promote the lighter LXMERT [57], which uses fewer data and less computational power, comparable to the heavier UNITER [14]. Code is published in https://github.com/yangxuntu/lxmertcatt."
9270566,Considering Spherical Refraction in Visual Ocean Gas Release Quantification,"Compared to traditional gas flow quantification methods, the stereo vision system has some advantages. However, underwater vision systems usually suffer from light refraction which can degrade the measurement accuracy from images. Cameras centered in spherical glass housings, dome ports, can theoretically avoid refraction, but misalignments in the dome create even more complex refraction effects than cameras behind flat glass windows. This paper introduces the spherical refraction model into a stereo vision gas flow quantification system. Also, this paper adds some contributions to an existing bubble quantification workflow for bubble size histogram and bubble volume estimation. First, the spherical glass dome port and the light propagation are modeled, and then the camera system is calibrated via underwater/in-air image pairs. Afterwards, the Epipolar Geometry Constraint is used to optimize the bubble matching. For volume estimation, an ellipsoid triangulation method is employed to improve ellipsoidal volume estimation. According to the calibration experiments and control experiments, the results show that the stereo vision gas flow quantification system can produce the volume of gas release accurately, which satisfies the requirements of long-term gas release monitoring in marine science."
9039352,Review of Modern UAV Detection Algorithms Using Methods of Computer Vision,"Lately unmanned aerial vehicles (UAV) became an unseparated part of modern life. These machines gained their particular popularity in the field of amateur or professional video production and small cargo deliveries. But they are not only limited by civilian lines of activity. Nowadays they are also used in military conflicts either as scout or as a deadly precise weapon. Due to UAV's easy market access, these drones can be used by the hands of terrorists groups. It's not a rocket science to attach an explosive device to UAV, guide it to strategic area and detonate it from far away. This is one of the main danger sources for such places like nuclear power plants. A downward trend also led to new type of thread - swarm attacks, when group of small UAVs strikes their victim. A timely detection of such attacks is not a trivial task. This question is being studied by many researches from different fields. The most promising methods comes from radiolocation and computer vision areas. Next in the paper, modern algorithms of detection, classification and tracking of UAVs by using optical flow will be reviewed. Usually, the common approach for this task comes from classic computer vision methods or, highly popular today, deep learning methods."
9710889,Online Refinement of Low-level Feature Based Activation Map for Weakly Supervised Object Localization,"We present a two-stage learning framework for weakly supervised object localization (WSOL). While most previous efforts rely on high-level feature based CAMs (Class Activation Maps), this paper proposes to localize objects using the low-level feature based activation maps. In the first stage, an activation map generator produces activation maps based on the low-level feature maps in the classifier, such that rich contextual object information is included in an online manner. In the second stage, we employ an evaluator to evaluate the activation maps predicted by the activation map generator. Based on this, we further propose a weighted entropy loss, an attentive erasing, and an area loss to drive the activation map generator to substantially reduce the uncertainty of activations between object and background, and explore less discriminative regions. Based on the low-level object information preserved in the first stage, the second stage model gradually generates a well-separated, complete, and compact activation map of object in the image, which can be easily thresholded for accurate localization. Extensive experiments on CUB-200-2011 and ImageNet-1K datasets show that our framework surpasses previous methods by a large margin, which sets a new state-of-the-art for WSOL. Code will be available soon."
9260310,A Face Recognition Approach Based on Computer Vision,"Most face recognition algorithms are easily affected by external factors, or require the recognizer to stand in a fixed position, so the recognition is slow and the accuracy is not very high. This paper implements an application platform of face recognition based on Open CV. Firstly, the face image is collected and preprocessed by optimizing the algorithms of Eigenfaces, Fisherfaces and LBP platform, which are used for face recognition by cooperation with each other. In addition to the restrictions on the recognition of the target to be detected by the face recognition system, eye location is added to the traditional AdaBoost based face detection algorithm to realize the function of face module training and face recognition. After testing, the system is proved to have a friendly interface, stable operation, good robustness to face location and environmental light changes, and it can detect and recognize face in real time quickly and accurately."
9743965,Trans4Trans: Efficient Transformer for Transparent Object and Semantic Scene Segmentation in Real-World Navigation Assistance,"Transparent objects, such as glass walls and doors, constitute architectural obstacles hindering the mobility of people with low vision or blindness. For instance, the open space behind glass doors is inaccessible, unless it is correctly perceived and interacted with. However, traditional assistive technologies rarely cover the segmentation of these safety-critical transparent objects. In this paper, we build a wearable system with a novel dual-head Transformer for Transparency (Trans4Trans) perception model, which can segment general- and transparent objects. The two dense segmentation results are further combined with depth information in the system to help users navigate safely and assist them to negotiate transparent obstacles. We propose a lightweight Transformer Parsing Module (TPM) to perform multi-scale feature interpretation in the transformer-based decoder. Benefiting from TPM, the double decoders can perform joint learning from corresponding datasets to pursue robustness, meanwhile maintain efficiency on a portable GPU, with negligible calculation increase. The entire Trans4Trans model is constructed in a symmetrical encoder-decoder architecture, which outperforms state-of-the-art methods on the test sets of Stanford2D3D and Trans10K-v2 datasets, obtaining mIoU of 45.13% and 75.14%, respectively. Through a user study and various pre-tests conducted in indoor and outdoor scenes, the usability and reliability of our assistive system have been extensively verified. Meanwhile, the Tran4Trans model has outstanding performances on driving scene datasets. On Cityscapes, ACDC, and DADA-seg datasets corresponding to common environments, adverse weather, and traffic accident scenarios, mIoU scores of 81.5%, 76.3%, and 39.2% are obtained, demonstrating its high efficiency and robustness for real-world transportation applications."
9621830,Design of a Fuzzy Inference Based Robot Vision for CNN Training Image Acquisition,"With the promotion of Industry 4.0, artificial intelligence has recently been attracting attention in the manufacturing industry. In particular, the image recognition such as Convolutional Neural Network (CNN) are using active inspection and testing processes. However, the learning of image recognition such as CNN is using a large amount of images as training data and it takes a lot of time and effort to image acquisition. In this paper, we propose a fuzzy inference based robot vision system for the automatic image acquisition of CNN training image from a camera mounted to the robot arm. In addition, the proposed system is considering fuzzy inference to suppress the vibration of the servo motor in the robot arm to speed up the image acquisition process."
8806053,An Affect Computing based Attention Estimation,"Advancement in ubiquitous system has the potential to change the way we interact with the environment around us. In current smart systems, body language and visual cues form an important part of input to analyse and interpret the human perception. Understanding the emotional state of a human will help in generating a personalized response accordingly. Even-though there are multitude ways of capturing the affect of a person, common tools are found in computer vision techniques. In this paper, we present an attention measurement technique in a classroom setting using computer vision. Understanding the cognitive state of a student attending the class will help in improving the quality of education. Results indicate that the proposed system can be used to detect affective state automatically."
9157232,Gold Seeker: Information Gain From Policy Distributions for Goal-Oriented Vision-and-Langauge Reasoning,"As Computer Vision moves from passive analysis of pixels to active analysis of semantics, the breadth of information algorithms need to reason over has expanded significantly. One of the key challenges in this vein is the ability to identify the information required to make a decision, and select an action that will recover it. We propose a reinforcement-learning approach that maintains a distribution over its internal information, thus explicitly representing the ambiguity in what it knows, and needs to know, towards achieving its goal. Potential actions are then generated according to this distribution. For each potential action a distribution of the expected outcomes is calculated, and the value of the potential information gain assessed. The action taken is that which maximizes the potential information gain. We demonstrate this approach applied to two vision-and-language problems that have attracted significant recent interest, visual dialog and visual query generation. In both cases the method actively selects actions that will best reduce its internal uncertainty, and outperforms its competitors in achieving the goal of the challenge."
9578525,Dynamic Class Queue for Large Scale Face Recognition In the Wild,"Learning discriminative representation using large-scale face datasets in the wild is crucial for real-world applications, yet it remains challenging. The difficulties lie in many aspects and this work focus on computing resource constraint and long-tailed class distribution. Recently, classification-based representation learning with deep neural networks and well-designed losses have demonstrated good recognition performance. However, the computing and memory cost linearly scales up to the number of identities (classes) in the training set, and the learning process suffers from unbalanced classes. In this work, we propose a dynamic class queue (DCQ) to tackle these two problems. Specifically, for each iteration during training, a subset of classes for recognition are dynamically selected and their class weights are dynamically generated on-the-fly which are stored in a queue. Since only a subset of classes is selected for each iteration, the computing requirement is reduced. By using a single server without model parallel, we empirically verify in large-scale datasets that 10% of classes are sufficient to achieve similar performance as using all classes. Moreover, the class weights are dynamically generated in a few-shot manner and therefore suitable for tail classes with only a few instances. We show clear improvement over a strong baseline in the largest public dataset Megaface Challenge2 (MF2) which has 672K identities and over 88% of them have less than 10 instances. Code is available at https://github.com/bilylee/DCQ"
9710208,AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer,"Fast arbitrary neural style transfer has attracted widespread attention from academic, industrial and art communities due to its flexibility in enabling various applications. Existing solutions either attentively fuse deep style feature into deep content feature without considering feature distributions, or adaptively normalize deep content feature according to the style such that their global statistics are matched. Although effective, leaving shallow feature unexplored and without locally considering feature statistics, they are prone to unnatural output with unpleasing local distortions. To alleviate this problem, in this paper, we propose a novel attention and normalization module, named Adaptive Attention Normalization (AdaAttN), to adaptively perform attentive normalization on per-point basis. Specifically, spatial attention score is learnt from both shallow and deep features of content and style images. Then perpoint weighted statistics are calculated by regarding a style feature point as a distribution of attention-weighted output of all style feature points. Finally, the content feature is normalized so that they demonstrate the same local feature statistics as the calculated per-point weighted style feature statistics. Besides, a novel local feature loss is derived based on AdaAttN to enhance local visual quality. We also extend AdaAttN to be ready for video style transfer with slight modifications. Experiments demonstrate that our method achieves state-of-the-art arbitrary image/video style transfer. Codes and models are available on https://github.com/wzmsltw/AdaAttN."
9706788,One-shot Compositional Data Generation for Low Resource Handwritten Text Recognition,"Low resource Handwritten Text Recognition (HTR) is a hard problem due to the scarce annotated data and the very limited linguistic information (dictionaries and language models). For example, in the case of historical ciphered manuscripts, which are usually written with invented alphabets to hide the message contents. Thus, in this paper we address this problem through a data generation technique based on Bayesian Program Learning (BPL). Contrary to traditional generation approaches, which require a huge amount of annotated images, our method is able to generate human-like handwriting using only one sample of each symbol in the alphabet. After generating symbols, we create synthetic lines to train state-of-the-art HTR architectures in a segmentation free fashion. Quantitative and qualitative analyses were carried out and confirm the effectiveness of the proposed method."
9022609,Temporal Coherence for Active Learning in Videos,"Autonomous driving systems require huge amounts of data to train. Manual annotation of this data is time-consuming and prohibitively expensive since it involves human resources. Therefore, active learning emerged as an alternative to ease this effort and to make data annotation more manageable. In this paper, we introduce a novel active learning approach for object detection in videos by exploiting temporal coherence. Our active learning criterion is based on the estimated number of errors in terms of false positives and false negatives. The detections obtained by the object detector are used to define the nodes of a graph and tracked forward and backward to temporally link the nodes. Minimizing an energy function defined on this graphical model provides estimates of both false positives and false negatives. Additionally, we introduce a synthetic video dataset, called SYNTHIA-AL, specially designed to evaluate active learning for video object detection in road scenes. Finally, we show that our approach outperforms active learning baselines tested on two datasets."
9476746,Vision-Based Sense and Avoid with Monocular Vision and Real-Time Object Detection for UAVs,"The use of unmanned aerial vehicles (UAVs) or drones have become ubiquitous in the recent years. Collision avoidance is a critical component of path planning, allowing multi-agent networks of cooperative UAVs to work together towards common objectives while avoiding each other. We implemented, integrated and evaluated the effectiveness of using a low cost, wide angle monocular camera with real-time computer vision algorithms to detect and track other UAVs in local airspace and perform collision avoidance in the event of a communications degradation or the presence of non-cooperative adversaries, through experimental flight tests where the UAVs were set on collision courses."
9598447,Open Hardware Analog Computer for Education – Design and Application,"Nowadays, most technical study courses in universities and similar institutions focus on digital computation and signal processing. Accordingly, graduates have some kind of tunnel vision because they have been trained to think of digital computation as a panacea for all problems encountered in real life.As digital computing is nearing basic physical boundaries with respect to integration densities and energy consumption it is important to extend the curriculum in computer science to include unconventional computing approaches such as analog computing. 1 Therefore it is necessary to give students and hobbyists as well the opportunity to get some hands-on-experience with analog computers. Since classic analog computers have become museum pieces, there is a need for a modern cheap and small analog computer, which can be used not only for teaching analog computer programming but can also be used in mathematics education to give students an intuitive grasp of dynamic systems and differential equations. Other fields of application are the engineering sciences, life sciences and basically all fields of technology where differential equations play a crucial role.This paper describes such an analog computer, called THE ANALOG THING, which has been developed to satisfy the above requirements. This analog computer is cheap (about 300 EUR) and versatile. Several of these computers can be coupled together to form a larger machine, and can also be used as part of a hybrid computer setup. The key design decisions are outlined and several problems suitable for class room use are described."
9220026,Visualization and Data Analysis for Intracellular Transport using Computer Vision Techniques,"Internalization of nanoparticles into intracellular area includes key information in biomedical field, such as cell signal pathway and drug delivery. Although the tracking of the individual nanoparticles in the cytoplasmic area has revealed the movement of the target in terms of single-particle level, the whole cell-level study is fundamental in order to efficiently acquire a large dataset of intracellular transport. In the present study, visualization and data analysis methods for understanding the entire cell-level intracellular transport in a living cell is suggested, by applying computer vision techniques to the cell images collected on the camera image sensor. Using the changes in the optical flow of the quantum dot-labeled vesicles for the entire intracellular area, our method showed the possibility of the time series analysis of vesicle movement related to the transport by two different types of molecular motors, dynein and kinesin."
9355377,Research on Vehicle Detection and Vehicle Type Recognition under Cloud Computer Vision,"Cloud computer vision technology has a broad application prospect in transportation system, which provides more intuitive and convenient analysis means and advanced technology for the realization of vehicle assisted driving. In this paper, the video stream of traffic image captured by fixed camera is taken as the research object, and the real-time extraction of traffic parameters is taken as the research purpose, and the detection algorithm of traffic flow parameters is studied and improved. In this paper, the application of automatic encoder in vehicle recognition is studied, and the performance of automatic encoder features in vehicle recognition is analyzed. On the basis of research, the convolution neural network and sparse automatic encoder network are fused to extract convolution features of vehicles, and the deep convolution automatic encoder network is trained by greedy training method layer by layer to realize vehicle classification. The results show that the vehicle detection network can achieve high detection accuracy in different environments, and the vehicle classification network can achieve good classification results for vehicles in traffic videos."
9474859,A computer vision approach for trajectory classification,"Nowadays, the increasing number of moving objects tracking sensors, results in the continuous flow of high-frequency and high-volume data streams. This phenomenon can especially be observed in the maritime domain since most of the vessels worldwide are now transmitting their positions periodically. Therefore, there is a strong necessity to extract meaningful information and identify mobility patterns from such tracking data in an automated fashion, eliminating the need for experts' input. To this end, a novel approach is presented in this paper, which fuses the research fields of computer vision and trajectory classification, in order to deliver a high-precision classification of mobility patterns. The experimental results demonstrate that the classification performance of the proposed approach can reach an f1-score of over 95%."
9532514,Automation of Material Takeoff using Computer Vision,"Automated material takeoff (MTO) can significantly impact construction productivity of the projects control team. The takeoff work is often a repetitive and mundane routine since it involves a manual counting of a variety of items sprawled in all kinds of locations over a drawing layout. For larger projects, such takeoffs can be time-consuming and the results can be prone to counting errors. In order to automate the task, we propose the Smart Layout Analyzer (SLA) that uses computer vision capabilities to automatically detect and recognize the items in an electrical engineering drawing layout with the aim of producing an overall item count. The software trains a Faster R-CNN with a ResNet50 convolution neural network (CNN) on the different items and their respective labels in the layout legend to subsequently localize and count the items in the drawing layout. The proposed model is different from other commercial programs that automate the takeoff making during the design process, as it can efficiently learn to count the different elements by being directly trained on the drawing layout legend."
9499291,Recyclable Waste Classification Using Computer Vision And Deep Learning,"Recycling solid waste is an important step to reduce harmful impact such as sanitary and health problems resulting from the over use of landfills. Yet, recycling requires the sorting of solid waste, which is complex and expensive. In an attempt to ease this process, our work proposes a Deep Learning approach using computer vision to automatically identify the type of waste and classify it into five main categories: plastic, metal, paper, cardboard and glass. Our conceptual system consists of an automated recycling bin which automatically opens the lid corresponding to the type of waste identified. This work focuses mainly on the Machine Learning algorithms which can be trained for efficient identification. Pre-existing images have been used to train a minimum of 12 variants of the Convolutional Neural Network (CNN) algorithm over three classifiers: Support Vector Machine (SVM), Sigmoid and SoftMax. Our results show that VGG19 with SoftMax classifier has an accuracy of around 88%."
8917374,Analysis of Vehicle Trajectories for Determining Cross-Sectional Load Density Based on Computer Vision,"The goal of this work was to analyze the behavior of vehicles on third-grade roads with and without horizontal lane markings with small curvature (R ≤ 200m). The roads are not frequented by many vehicles, and therefore, a general short-term study would not be able to provide enough data. We used recording devices for long-term (weeks) recording of the traffic and designed a system for analyzing the trajectories of the vehicles employing computer vision. We collected a dataset at 6 distinct locations, containing 1 010 hours of day-time video. In this dataset, we tracked over 12 000 cars and analyzed their trajectories. The results show that the selected approach is functional and provides information that would be hard to mine otherwise. After application of the horizontal markings, the drivers slowed down and shifted slightly towards the outer side of the curvature."
9278072,Automated Crop Field Surveillance Using Computer Vision,"Artificial Intelligence is everywhere today. But unfortunately, Agriculture has not been able to get that much attention from Artificial Intelligence (AI). A lack of automation persists in the agriculture industry. For over many years, farmers and crop field owners have been facing a problem of trespassing of wild animals for which no feasible solution has been provided. Installing a fence or barrier like structure is neither feasible nor efficient due to the large areas covered by the fields. Also, if the landowner can afford to build a wall or barrier, government policies for building walls are often very irksome. The paper intends to give a simple intelligible solution to the problem with Automated Crop Field Surveillance using Computer Vision. The solution will significantly reduce the cost of crops destroyed annually and completely automate the security of the field."
9257941,Real-Time Collision Warning System Based on Computer Vision Using Mono Camera,"This paper aims to help self-driving cars and autonomous vehicles systems to merge with the road environment safely and ensure the reliability of these systems in real life. Crash avoidance is a complex system that depends on many parameters. The forward-collision warning system is simplified into four main objectives: detecting cars, depth estimation, assigning cars into lanes (lane assign) and tracking technique. The presented work targets the software approach by using YOLO (You Only Look Once), which is a deep learning object detector network to detect cars with an accuracy of up to 93%. Therefore, apply a depth estimation algorithm that uses the output boundary box's dimensions (width and height) from YOLO. These dimensions used to estimate the distance with an accuracy of 80.4%. In addition, a real-time computer vision algorithm is applied to assign cars into lanes. However, a tracking proposed algorithm is applied to evaluate the speed limit to keep the vehicle safe. Finally, the real-time system achieved for all algorithms with streaming speed 23 FPS (frame per second)."
9377082,Water Quality Estimation using Computer Vision in UAV,"The color change of water in a water body is often a tell - tale sign of its health. To counter the sources of water pollution an Unmanned Aerial Vehicle is deployed over a water body which reports back any discrepancies by channeling the feed through Computer Vision based model. This allows for rapid steps to be taken by the Concerned Authorities to mitigate the current situation. Algae formation, floating impurities and color change of the water body are the scope of the project and each of these are detected with an independent Machine Learning Models. The UAV communicates and sends results based on the accuracy of these models."
9576438,Apple Trees Diseases Detection Through Computer Vision in Embedded Systems,"In this paper, we address the problem of detecting diseases of apple trees. We report on a computer vision method for apple trees leaves segmentation. For this reason we collect the leaves images in the field using a thermal image camera. Data analysis is carried out using Neural Networks (NN) optimized for running on the embedded systems. We perform a comparative study on the embedded systems, embedded systems enriched with the GPU capability, and the PC. We achieved IoU=0.814. Our results demonstrate that the NNs running on the embedded systems is a promising solution for detecting the trees diseases using embedded systems and open up wide vista for its application in precision agriculture."
9481794,A joint Computer Vision and Reconfigurable Intelligent Meta-surface Approach for Interference Reduction in Beyond 5G Networks,"Reconfigurable Intelligent Meta-surfaces (RIMs) are particular devices able to control and manipulate radio frequency wireless signals. This promising technology allows to improve the reliability of wireless networks, thanks to the capacity of reflecting the desired signals through appropriate phase shifts. The joint use of RIMs and Computer Vision (CV) technology is the main objective of this paper. This synergistic approach is used to correctly identify the specific configuration of a radiation pattern, to be used as input for computing optimal coding sequences of the RIM. Indeed, by the means of a CV algorithm it is possible to infer a connectivity graph related to a real scenario, where people is moving. The information about network nodes such as their distance, the relative position, etc. is used for feeding an intelligent logic, able to compute the optimal configuration for re-directing the signals towards a given receiver target node.Numerical results show the huge potentiality of this combined approach in terms of interference reduction. It has been observed that for high traffic load, it is possible to reduce the average interference in the network of 40%. Furthermore, an analysis including the positioning estimation error of the CV algorithm has been addressed, in order to consider how it affects the interference reduction. Results show that, even though there is an increasing effect of interference, when the error is accounted, the interference reduction impact is still important."
9537374,Realization of Computer Vision System for Biometric Identification of Personality,"This paper discusses issues dedicated to the person’s identity defining. Biometric identification of a person, contrary to established stereotypes, is required not only in criminology, where it has been used initially. Methods applied for persons’ identification differ in the complexity of implementation, the time needed to receive a result (system response) after the identification procedure, and, most importantly, the reliability of obtained results. Biometric identification methods provide the maximal authenticity, and are based on the recognition of biometric data of a particular person, which include physiological data (static) and behavioral data (recognized in dynamics). Objectives of the research are: choice of the identification method based on the conducted analysis that is the most applicable for operational person’s identification within given restrictions, and the description of the biometric identification system realization based on computer vision. The development of the simple and operative system is described, and results are demonstrated."
9605823,Deep Learning & Computer Vision for IoT based Intelligent Driver Assistant System,"With the exponential increment of vehicles, roadside accidents also have been increasing rapidly where approximately 80% of these accidents were caused by human error. Therefore, the automobile industry and the government authorities are more focused on accident prevention by introducing improved road safety systems for the general public. Driver assistance system (DAS) is an intelligent road safety development where it senses the surrounding of a moving vehicle which will assist the driver to avoid the dangers and also warn the drivers of immediate dangers. With the current technological advancements, the automobile industry is equipped with the internet of things (IoT) based data transfer mechanisms, wherewith the concept of ‘connected car’ the passengers and the other interconnected vehicles with the internet can share data with back end applications. The data is consisting of the current location, the distance travelled by the vehicle, whether the vehicle requires urgent service. This study is mainly focused on the development of an intelligent driver assistance system based on computer vision and deep learning, which can prevent accidents with early detection of drowsiness, harmful objects and also by alerting the driver with the signboards and the road lines. The system is capable of passing the emergency messages to the driver as well as the other interconnected vehicles through the website by communicating the real-time road map generated within the system. The proposed system has been implemented and tested with multiple detection scenarios where machine learning has been employed to improve the accuracy of the results."
9446381,Computer Vision for Determining the Fraction of Proppant for Hydraulic Fracturing,"In this paper is described experience of computer vision technologies to detect the size of the proppant fraction from a smartphone photo. Neuro net model was trained with PyTorch framework and TorchVision library and used pre-trained MobileNet V2. The paper solves the problem of classification. Using augmentation methods, the public dataset of 790 images was increased to 50,000. The results of research showed that the classification accuracy on the test sample according to the F1-score metric was equal to 0.95. The developed model can be recommended for use in the oil industry to detect the proppant fraction by a smartphone photo."
9617435,Tool automation and computer vision methodologies for faster IC diagnostics,"Automated data acquisition and analysis using scripting and Computer Vision (CV) allow one to perform repetitive tasks faster with higher accuracy. In this paper, we discuss techniques to automate analytical tools and several examples of advanced image processing and data analysis to provide accurate IC diagnostics."
9253456,Research on Computer Vision-Based Waste Sorting System,"Nowadays, waste sorting has become a hot topic of society in China. Many cities, such as Beijing and Shanghai, have begun to strictly implement regulations of waste sorting. However, in this process, it still exists that people are not able to distinguish them for residual waste or household food waste. This paper utilizes computer vision approach to recognize two categories of wastes. We find out typical examples from them for image recognition, and collect relevant image dataset through the Internet, with a total of 2800 images. The methods used are support vector machine based on feature extraction and transfer learning based on convolutional neural network. Our experiment shows that the latter has better performance."
9442633,Comparative study between computer vision methods for the estimation and detection of the roadway,"Autonomous driving is a field of study that is progressing rapidly to ensure road safety. Recently, researchers have been particularly interested in the detection of the roadway. Several new approaches have been proposed in the last decade. In this paper, we present a comparative study of roadway detection methods using three algorithms. The one is based on HOUGH transform, the second on the RANSAC algorithm and the third on the RADON transform. The objective of this paper is to make a comparison of these method in order to robust estimation methods in computer vision and to apply the method of RADON for the detection of the roadway."
9474725,Emotions in Computer Vision Service Q&A,"Software developers are increasingly using cloud-based services that provide machine learning capabilities to implement `intelligent' features. Studies show that incorporating machine learning into an application increases technical debt, creates data dependencies, and introduces uncertainty due to their non-deterministic behaviour. We know very little about the emotional state of software developers who have to deal with such issues; and the impacts on productivity. This paper presents a preliminary effort to better understand the emotions of developers when experiencing issues with these services with the wider goal of discovering potential service improvements. We conducted a landscape analysis of emotions found in 1,425 Stack Overflow questions about a specific and mature subset of these cloud-based services, namely those that provide computer vision techniques. To speed up the emotion identification process, we trialled an automatic approach using a pre-trained emotion classifier that was specifically trained on Stack Overflow content, EmoTxt, and manually verified its classification results. We found that the identified emotions vary for different types of questions, and a discrepancy exists between automatic and manual emotion analysis due to subjectivity."
9545005,Computer Vision Based Vulnerable Road Users Hand Signal Recognition,"With development of computer vision (CV) and Artificial Intelligent (AI) the domains like object detection, object tracking their development from 2D to 3D tracking and human pose estimation have gained a lot of popularity and demand in the field of Advanced Driver Assistance Systems (ADAS) and Autonomous Driving (AD). Basically, these systems are designed with an objective to increase the safety and avoid road accidents while driving. These systems develop the ability of car driver to react to various types of hazards and dangers on the road. If it is possible to know the intentions of Vulnerable Road Users (VRUs) like pedestrian, cyclist or motorbike riders then the probability of road accidents can be reduced to great extents. The ADAS systems are able to warn the car driver through automatic systems and thus provide high safety. This paper proposes Convolutional neural Networks (CNN) based VRUs hand signal recognition system based on Region Multi-Person Pose Estimation (RMPE) [1] framework for human pose estimation to know the hand signals given by the bicyclist, scooter, e-scooter and motorbike riders to warn the car driver. The VRUs are well aware of the traffic rules and regulations and also follow them. Based on the signal given by bicyclist the ego-vehicle can stop or reduce its speed. The focus of this novel algorithm proposed in this paper is that it is capable of recognizing all the six hand signals of bicyclists left, right, stop, give way, slow down and road hazard which are not addressed before in any previous works. Also, this algorithm can recognize arm signals of scooter, e-scooter riders and motorbike riders efficiently. The dataset used for testing is the self-captured dataset for VRUs Arm Signal Recognition and to generalize it Cyclist Arm Signal Recognition (CASR) [2] dataset is also used. The proposed work provides state-of-the-art (SOTA) results for VRUs hand signal recognition and frame rate of 10 FPS and 92% acuuracy.
(Show More)"
9014884,"Human Drowsiness Detection In Real Time, Using Computer Vision","This paper presents a human drowsiness detection algorithm in real time using computer vision. Drowsiness is a state whose consequences can be very dangerous for vehicle drivers, air traffic controllers, nuclear plant controllers, etc. In 2018 in Ecuador, 353 traffic accidents were reported for driving while drowsy. The algorithm that we present obtains frontal images of the driver using an infrared camera, then performs automatic face detection using the Viola-Jones algorithm. After this, the eye portion is extracted and the classification between open and closed eye is done with two methods: a) method based on the extraction of maximums and minimums of horizontal and vertical edges of the eye and b) using a multilayer perceptron (MLP) neural network. Finally, it makes the detection of drowsiness during the time the eyes were closed within a time interval. For the open and close eye classification using the first method we obtain 84% of accuracy and for the second method using the MLP we obtain 97% of accuracy, including test images under dark conditions."
9067316,Learning to Detect Vegetation Using Computer Vision and Low-Cost Cameras,"A problem of current agriculture is the large amount of agrochemicals used to boost production due to their cost and the environmental pollution they cause. A partial solution to this problem consists in developing selective spraying techniques through the measurement of a green index that allows the selection of the precise amount of pesticide to be applied according to the specific conditions of each part of the field. Some of the problems of the existing systems are the inability to discriminate between types of vegetation and to pinpoint its location, since they only detect general patches of vegetation. In this work, we introduce a system prototype capable of measuring the presence of vegetation in an area using low-cost devices combined with current computer vision techniques. The system allows to generate a mask with the presence of vegetation in a certain area and it is also capable of distinguishing between different materials unlike current methods, which only allow to distinguish between green and non-green areas. The presented method opens the door to future research which can allow distinguishing between crops and weeds to make an even more selective application. The output of the system can be used also to design another type of weeding method that is not based on the application of agrochemicals."
9632155,Continuous Monitoring of Work Area Safety at Energy Enterprises by Online Cloud Monitoring and Computer Vision,"The paper deals with the issues of reduction of electrical injuries at industrial enterprises, the analysis of industrial accidents at the enterprises’ power facilities according to the type of work carried out. A comprehensive system for monitoring hazards and harmful production factors is proposed. It is based on an intelligent online cloud monitoring system. A computer vision system for detecting the presence of personal protective equipment for personnel has been implemented. Due to the modern approach and new technologies, it is possible to minimize emergency situations and accidents at the enterprises’ power facilities, to make timely remedial interventions into the labor protection management system."
9188522,Zipper classification and defect detection based on computer vision,"At present, the research progress of zipper classification and defect detection based on computer vision is slow in China. Generally, manual method is used to detect the appearance quality of zipper products, which is not only inefficient, but also low reliability. This paper proposes a reliable and accurate method. In the actual industrial production, there are various types, shapes and colors of zippers, and in the actual situation, lighting and other factors have a great impact on the detection effect. In view of the above situation, this paper proposes an adaptive region growing algorithm and template matching algorithm. By region growing algorithm, the background is removed to obtain the complete inner region of the zipper. Then, the deflection angle of the zipper is corrected by the smallest external rectangle of the zipper. Finally, the zipper type is determined by template matching algorithm. For defect detection, firstly, the complete zipper is extracted by morphological method, then the defect location is determined by searching the area of connected domain, and finally the defect is determined by gray value comparison. The experimental results show that the above method has high accuracy and execution efficiency in zipper classification and defect detection."
9239012,Implementation of a Computer Vision Framework for Tracking and Visualizing Face Mask Usage in Urban Environments,"The COVID-19 pandemic is an evolving situation in the United States and is spreading at alarming rates. The adoption of public health-informed hygienic practices can have a large impact on community transmission of COVID-19 including the wearing of face masks in public settings. Convolutional Neural Networks (CNN) can be trained to classify people wearing face masks with impressive accuracy. However, current face mask datasets contain clear, high-resolution close-up images of individuals with face masks which is unrepresentative of the lower fidelity images of distant faces more prominent in urban camera images. This paper proposes a practical deep learning computer vision framework for detection and tracking of people in public spaces and the use of face masks. A custom 6,000 image face mask dataset curated from over 50 hours of urban surveillance camera footage is created in this work. CNN-based detectors trained using the dataset are used to perform person detection and face mask classification. Then, a multi-target tracking module extracts individual trajectories from frame-by-frame detection. By associating detected face masks with tracked individuals, overall face mask usage can be estimated. The framework is implemented on several surveillance cameras along the Detroit RiverWalk, a 5-kilometer pedestrian park connecting various greenways, plazas, pavilions, and open green spaces along the Detroit River in Detroit, Michigan. The detection of park user types is shown to have an average precision of 89% and higher for most person classes with the mask detector having an accuracy of 96%. An interactive web application visualizes the data and is used by park managers to inform management decisions and assess strategies used to increase face mask usage rates."
9400669,Parking Occupancy Prediction using Computer Vision with Location Awareness,"Parking occupancy in large outdoor parking as well as at street parking slots is a big problem, and solutions for it were proposed in the past with limited success. Recent advances in Computer Vision (CV) algorithms made this technology more powerful by making models smaller and more precise. On the other hand development of hardware components made Edge devices more powerful with AI accelerators as built-in options. Based on this new development, we propose an architecture where the latest Smart Edge devices are used to calculate parking occupancy on the edge. The proposed solution uses newly available data streams to create a 3D model of the parking environment that will identify available on-street parking spaces with high precision."
9177006,Bibliometric Analysis of the Application of Convolutional Neural Network in Computer Vision,"This article analyzes the research progress in field of Convolutional Neural Networks (CNNs) using the bibliometric method. Literature samples of CNNs are analyzed by a basic statistic and co-citation network. Experimental results show that CNNs are being utilized in many computer vision applications, such as fault and image recognition diagnosis, seismic detection, positioning, and automatic detection of cracks and signals, image classification and image segmentation. In addition, there is systematic research on unbalanced problems in CNNs. Quantitative experimental research, extensive application fields, and market research informatization will be the three vital research tendencies in the future. The ideas and conclusions of this article provide insights to the academic research of CNNs and their practical application in the corporate world."
9511344,On the Influence of Viewpoint Change for Metric Learning,"Physical objects imaged through a camera change their visual representation based on various factors, c.g., illumination, occlusion, or viewpoint changes. Thus, it is the inevitable goal in computer vision systems to use mathematical representations of these objects robust to various changes and yet sufficient to determine even minor differences to distinguish objects. However, finding these powerful representations is challenging if the amount of data is limited, such as in few-shot learning problems. In this work, we investigate the influence of viewpoint changes in modern recognition systems in the context of metric learning problems, in which fine-grained differences differentiate objects based on their learned numeric representation. Our results demonstrate that restricting the degrees of freedom, especially by fixing the virtual viewpoint using synthetic frontal views, elevates the overall performance. We await that our observation of an increased performance using rectified patches is persistent and reproducible in other scenarios."
9591692,Image augmentation using GAN models in Computer Vision,"The efficiency of deep learning algorithms will increase when it is trained on a large size of data. Over fitting problems will also be solved working on a large dataset. To collect a huge quantity of data for training a model is more challenging job. Collecting data will take more time as well as resources. The data augmentation technique will increase the diversity of data that to be trained on deep learning algorithms. This will also help in not collecting new data. This led to the need for generative models. When the set of training data is given to this generative model, it will learn to produce the same statistical data as the set of training data. GANs are used in many fields including computer vision, medical, agriculture etc. In this paper along with the traditional GANs two architecture variant and two loss-variant GANs are reviewed and experiments are conducted to generate images. Detailed reviews on the performance metrics of GAN models are also discussed and observed that FID plot doesn’t give much insight to compare the generated images. Despite of the significant success achieved till date, applying GAN model to real time dataset has some challenging problems. Challenges faced while training GANs including mode collapse, image quality and vanishing gradients are discussed."
9513057,Recognition and Counting of Citrus Trees Based on UAV Images,"Accurate and rapid knowledge of the number of citrus trees is of great significance for predicting citrus yield, managing citrus growth and improving citrus yield. In this paper, the UAV is used to shoot the citrus planting area in Xinhui District of Jiangmen City. The visible light image obtained by the UAV is processed and analyzed by computer vision technology. The number of citrus trees in the shooting area can be accurately obtained, and the number of mature trees and saplings can be further counted. The accuracy rate is more than 95%.The results show that it can meet the requirements of fast and accurate acquisition of citrus trees in planting area at low cost."
9257843,Real Time Bangla Number Plate Recognition using Computer Vision and Convolutional Neural Network,"Automatic number plate identification in today's world plays a vital role in vehicle tracking and organization. Our proposed model of automation in the detection and recognizing vehicles through the use of number plate computerization is expected to create a new scope of evolution for large cities. The system can be used for the parking system of motor vehicles, as well as to collect tolls. The detection of the Bangla number plates from different cities and multi-class vehicles is the first step of the proposed system. The number plate detection has been performed with the computer vision approach, and the You Only Look Once v3 (YOLOv3) algorithm. Next, the Tesseract optical character recognition system, in conjunction with the Bangla character recognition model, has been used for vehicle indexing and convolutional neural network for the character recognition from the detected number plate. Numerical results demonstrate that the accuracies of license plate detection for the computer vision and YOLOv3 are 91% and 95%, respectively. For the character recognition, the accuracy for Tesseract and convolutional neural network are 90% if the license plate is detected and cropped successfully and 91.38%, respectively. Finally, our system has been tested using the convolutional neural network method in an environment of real-world where our system's Pi Camera captured video as input, which has a total of 18 different cars. From 18 cars, it has successfully detected 17 cars, which makes our overall system accuracy 88.89%."
9664996,Network Video Frame-Skip Modeling and Simulation,"A high video frame rate is ideal for many computer vision applications; however, this is not is usually the case for video feeds streamed from network cameras, which also exhibit frame skipping. Frame skipping can be considered as a type of degradation of the live-streamed video, and is detrimental to object tracking and other computer vision tasks. In this paper, we empirically model frame skipping and present a method for simulating frame skipping using videos with high frame rates. This simulation method was then validated by comparing the frame rate histograms of real and simulated frame-skipped videos. Finally, we demonstrate how frame skipping degrades the performance of an object tracker."
9188481,Computer Vision-Based Online Heterogeneity Assessment of the Sintering Transversal Thermal State,"The heterogeneity of the sintering transversal thermal state is one of the decisive factors of sinter quality and productivity. However, it is difficult to quantify directly the heterogeneity. In the paper, a computer vision-based online assessment method is proposed for the heterogeneity of the sintering transversal thermal state. The method makes use of the flame front distribution in the real-time collecting image of the sintering machine end section. First, the flame front was extracted using image segmentation methods. Second, the heterogeneity of the sintering transversal thermal state was converted into the spatial heterogeneity of the flame front distribution, by introducing the spatial point patterns. Finally, the sintering transversal heterogeneity index was established to evaluate quantifiably the transversal heterogeneity, by integrating the conventional spatial heterogeneity assessment methods and the features of the flame front. The proposed method performs more accurately, robustly, and comprehensively, in comparison with other existing heterogeneity assessment methods, and the sintering expertise. It was integrated into a sintering condition assessment and quality prediction system in a steelmaking plant. The industrial application of it proves its efficiency and significance for guiding the sintering operation, and improving the sinter quality and productivity."
9600491,Driver-in-the-Loop for computer-vision based ADAS testing,"Driver safety is a main concern nowadays. Hence, a real need for systems like Advanced Driver Assistance Systems (ADAS) has arisen. With this need to such systems, many challenges come up. These challenges are including the time consumption of testing of these systems as It needs to be tested in different weather conditions. Besides, the difficulty of covering all the possible scenarios while testing these systems and the carbon dioxide emission from cars in the process of testing the system. Hence, the purpose of the proposed paper is to implement a simulation-based driver in the loop simulation-based system for indoor testing of computer vision based ADAS. A simplified tricycle kinematics model is implemented for this purpose by using Matlab/Simulink. A vehicle virtual environment is implemented in unreal engine environment by MATLAB platform. The interaction of the driver with the virtual car in this virtual environment is made by a simplified hardware for the car. A feedback is taken from the driver through sensors connected to an Arduino microcontroller. Arduino microcontroller interpret the driver feedbacks and feed it as an inputs to the vehicle model to move the car in the virtual environment. The proposed system is tested for computer vision lane detection, departure, and change algorithms. These algorithms relay mainly on Hough transform. The proposed system is tested in real time and the results obtained is very similar to the results obtained from testing a computer vision based ADAS on a real street."
9272142,New Heuristic Method Merging Artificial Vision and Neural Networks used in a Sensorless Robotic Arm Position Control,"Inspired by the control system of voluntary movements developed in the human body based on vision and neural system, this paper presents a new heuristic method merging artificial vision and neural networks used in a sensorless robotic arm position control. This proposal is based on a structure of six artificial neural networks (ANN) of perceptrons, which correct the position of the arm in one of the six predefined directions, four in a the projection plane (forward, backward, right and left) and two in the vertical plane (up and down). The robotic arm displacement is based on the choose performed by the ANN processing the images capture by a camera, thus the chosen of the corresponding direction is derived from knowledge obtained during the supervised learning using similar situations. Finally, experimental results of the ANN learning process and robotic arm positioning tests are presented."
9576367,Multiple Object Trackers in OpenCV: A Benchmark,"Object tracking is one of the most important and fundamental disciplines of Computer Vision. Many Computer Vision applications require specific object tracking capabilities, including autonomous and smart vehicles, video surveillance, medical treatments, and many others. The OpenCV as one of the most popular libraries for Computer Vision includes several hundred Computer Vision algorithms. Object tracking tasks in the library can be roughly clustered in single and multiple object trackers. The library is widely used for real-time applications, but there are a lot of unanswered questions such as when to use a specific tracker, how to evaluate its performance, and for what kind of objects will the tracker yield the best results? In this paper, we evaluate 7 trackers implemented in OpenCV against the MOT20 dataset. The results are shown based on Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP) metrics."
9043107,A Scalable Computer Vision Framework For Mobile Device Auto-Typing,"In this paper, we present a computer vision framework that controls robots to auto type on a mobile device such as an Android phone or an iPad. The framework consists of three parts: (i) an image undistortion and segmentation algorithm that supports images captured by a top mounted camera or a side mounted camera, (ii) a deep neural network (DNN) algorithm that detects the keyboard region and recognizes isolated characters from an input image, and (iii) a grid click algorithm to construct data points to compute the image to robot coordinate translation matrix that is scalable to any touch device of different sizes. We have demonstrated in this paper the accuracy and scalability of the proposed system."
9737789,Inspection of machining defects on mechanical parts using a computer vision system,"Mechanical parts are currently mass produced by machines that perform high-speed machining. The mechanical parts will be assembled in a machine with other components, and therefore there will be holes for screws, grooves etc… These parts must have a precise shape. If the machining and drilling are not done in the right place or with a small dimensional error in this case the assembly process cannot be completed. The manuel inspection is cumbersome and the efficiency is low. To overcome this problem, the non-contact defect detection process is the subject of this research. We proposed a real-time measurement technique using edge detection, dilation and erosion algorithms to remove noise and enhance the collected part image to make the information clearer, then compare it with the perfect model. This technique uses the OpenCV library and a Raspberry Pi 4 board. The experimental results show that the defect on the part can be effectively inspected, located and recognized."
9327480,A Novel Illumination-Robust Hand Gesture Recognition System With Event-Based Neuromorphic Vision Sensor,"The hand gesture recognition system is a noncontact and intuitive communication approach, which, in turn, allows for natural and efficient interaction. This work focuses on developing a novel and robust gesture recognition system, which is insensitive to environmental illumination and background variation. In the field of gesture recognition, standard vision sensors, such as CMOS cameras, are widely used as the sensing devices in state-of-the-art hand gesture recognition systems. However, such cameras depend on environmental constraints, such as lighting variability and the cluttered background, which significantly deteriorates their performances. In this work, we propose an event-based gesture recognition system to overcome the detriment constraints and enhance the robustness of the recognition performance. Our system relies on a biologically inspired neuromorphic vision sensor that has microsecond temporal resolution, high dynamic range, and low latency. The sensor output is a sequence of asynchronous events instead of discrete frames. To interpret the visual data, we utilize a wearable glove as an interaction device with five high-frequency (>100 Hz) active LED markers (ALMs), representing fingers and palm, which are tracked precisely in the temporal domain using a restricted spatiotemporal particle filter algorithm. The latency of the sensing pipeline is negligible compared with the dynamics of the environment as the sensor's temporal resolution allows us to distinguish high frequencies precisely. We design an encoding process to extract features and adopt a lightweight network to classify the hand gestures. The recognition accuracy of our system is comparable to the state-of-the-art methods. To study the robustness of the system, experiments considering illumination and background variations are performed, and the results show that our system is more robust than the state-of-the-art deep learning-based gesture recognition systems. Note to Practitioners-This art...
(Show More)"
9663812,Computer Vision Based Pre-processing of Microscopic Images,"Preparing the microscopic images for analyses is a crucial step for any kind of image processing task. In this paper, the methods for analyzing and processing of biomedical images are presented. The main task is to select the appropriate object images of interest for further measurement of their morphological parameters. In the article, several methods are presented such as contrast adjustment, intensity adjustment, histogram equalization, morphological operation and background subtraction that can be used in pre-processing of microscopic images."
9607743,Virtual Touch: Computer Vision Augmented Touch-Free Scene Exploration for the Blind or Visually Impaired,"The Blind or Visually Impaired (BVI) individuals use haptics much more frequently than the healthy-sighted in their everyday lives to locate objects and acquire object details. This consequently puts them at higher risk of contracting the virus through close contact during a pandemic crisis (e.g. COVID-19). Traditional canes only give the BVIs limited perceptive range. Our project develops a wearable solution named Virtual Touch to augment the BVI’s perceptive power so they can perceive objects near and far in their surrounding environment in a touch-free manner and consequently carry out activities of daily living during pandemics more intuitively, safely, and independently. The Virtual Touch feature contains a camera with a novel point-based neural network TouchNet tailored for real-time blind-centered object detection, and a headphone telling the BVI the semantic labels. Through finger pointing, the BVI end user indicates where he or she is paying attention to relative to their egocentric coordinate system, based on which we build attention-driven spatial intelligence."
9150861,Leveraging combinatorial testing for safety-critical computer vision datasets,"Deep learning-based approaches have gained popularity for environment perception tasks such as semantic segmentation and object detection from images. However, the different nature of a data-driven deep neural nets (DNN) to conventional software is a challenge for practical software verification. In this work, we show how existing methods from software engineering provide benefits for the development of a DNN and in particular for dataset design and analysis. We show how combinatorial testing based on a domain model can be leveraged for generating test sets providing coverage guarantees with respect to important environmental features and their interaction. Additionally, we show how our approach can be used for growing a dataset, i.e. to identify where data is missing and should be collected next. We evaluate our approach on an internal use case and two public datasets."
8658842,Observing Pianist Accuracy and Form with Computer Vision,"We present a first step towards developing an interactive piano tutoring system that can observe a student playing the piano and give feedback about hand movements and musical accuracy. In particular, we have two primary aims: 1) to determine which notes on a piano are being played at any moment in time, 2) to identify which finger is pressing each note. We introduce a novel two-stream convolutional neural network that takes video and audio inputs together for detecting pressed notes and finger presses. We formulate our two problems in terms of multi-task learning and extend a state-of-the-art object detection model to incorporate both audio and visual features. In addition, we introduce a novel finger identification solution based on pressed piano note information. We experimentally confirm that our approach is able to detect pressed piano keys and the piano player's fingers with a high accuracy."
8974786,Automatic Monomer Filling System Using Machine Vision,"This paper presents the design and development of an automatic monomer filling system using machine vision in optics manufacturing. Currently, the production process is carried out by a trained worker. The monomer filling process, needs to be operated by hand and human vision is employed to control each step. The important procedure is to start-stop a valve to prevent liquid monomer overflow from glass molds. The proposed system consists of a Cartesian robot and a vision system. The vision system uses two cameras to capture four regions of interest (ROI) of four glass molds at the same time. The frame differencing technique is used to detect the difference between the last and the current video frames. The results show that each camera can be operated at real-time image processing at 52 frames per second, and the system can stop the valve for the filling monomer of each glass mold at the full level. As the result, productivity increased by 60% compared with the existing manual process making it possible to replace skilled workers."
9550916,Frame Rate Latency Reduction for Real-time Vehicle Tracking using Network Cameras,"Traffic monitoring and vehicle counting systems that use surveillance cameras employ several computer vision techniques, one of which is object tracking, which approximates the trajectory of the vehicle throughout the scene. However, a major challenge in processing videos from network camera feeds is the irregular and low frame rates, affecting the performance of object tracking. In this paper, we present a concurrent implementation framework intended to increase the input network video frame rate."
8884146,Computer-Vision Based Diagnosis of Parkinson’s Disease via Gait: A Survey,"Parkinson's Disease (PD) being the second most hazardous neurological disorder has developed its roots in damaging people's quality of life (QOL). The ineffectiveness of clinical rating scales makes the PD diagnosis a very complicated task. Thus, more efficient systems are required to perform an automated evaluation of PD for its earlier detection and to enhance life expectancy rate. Gait based clinical diagnosis can provide useful indications regarding the presence of PD. From recent years, computer vision-based (VB) analysis is in great demand and seems to be highly effective in PD inspection. The objective of this article is to systematically analyze the applications of computer vision in PD evaluation through gait. This paper surveys the VB PD gait acquisition modalities as well as provides a concise overview of preprocessing techniques. The study presents a description of PD related gait features, extraction and selection methods used for PD analysis. A number of machine learning techniques for classification of PD and healthy gait are also discussed. This article extensively surveys PD gait datasets considering data from 1997 to 2018. Also, several research gaps in existing studies have identified that need to be addressed in the future. At last, an outline of the proposed idea is given that can cope up with the related issues and can lead to quality VB PD gait investigation."
8778751,"Edge Detection Algorithm for
Musca−Domestica
Inspired Vision System","Compound vision sensors, mimicking the compound eye of the common housefly, possess motion hyperacuity, resulting in better motion detection and tracking capabilities as compared to traditional camera systems. Therefore, these sensors have applications requiring near instantaneous obstacle avoidance in its field-of-view. However, the obstacle detection algorithms used by current compound vision systems are insufficient to enable these capabilities due to their ability to only detect horizontal or vertical edges. In this paper, we propose an improved edge detection and location algorithm for fly inspired vision sensors which can correctly classify horizontal, vertical, forward diagonal, and backward diagonal edges."
9032689,Internet of Things (IoT) based Object Recognition Technologies,"Over the past years, the Object recognition technologies have matured to a great extent, where it has enabled the development of exciting solutions for visually impaired. A variety of solutions has been proposed using computer vision and object recognition to help the visually impaired with their day-to-day activities. This paper aims at the development of a solution that can be adopted by the visually impaired for identifying and locating household objects in their daily life. The solution includes a wearable device that listens for the user's voice, understands the user's command and locates the object in the surrounding environment. Once the target object is located, it gives user the information about the object and the maximum possible distance of the object from the user. The performance of the device has been increased and the battery consumption has been decreased by adopting an efficient algorithm that makes the device usable in day-to-day life.."
9677361,Leveraging Deep Learning for Computer Vision: A Review,"The usability of computer vision is everywhere, whereas deep learning revolutionized the concept of artificial intelligence including computer vision. This paper discusses the leverages of deep learning for computer vision. At first, the background details of computer vision and deep learning have been discussed. Important tasks of computer vision like image classification, object localization, object detection, segmentation are briefly explained. Various types of deep learning algorithms have been described. The architecture of the convolutional neural network has been explained briefly. Applications of deep learning in different fields of computer vision like image classifications, human activity recognition, scene text detection and recognition, object tracking, visual question answering, etc., have been discussed with the references of the recent state-of-the-art works."
9061448,Robotized Platform for Picking of Strawberry Berries,"Agricultural units technological capabilities utilization level optimization cannot be achieved without production processes wide automation and use of computer-controlled systems on the units. Study purpose is to develop an algorithm for recognizing the coordinates of the location and ripeness of strawberry in different lighting conditions and describe the technological process of their harvesting in the field using a robotic manipulator mounted on a self-propelled platform. As a research result, we developed a self-propelled platform with an automatic manipulator for harvesting strawberries, including a manipulator with six degrees of freedom, coaxial grip, servo drives, a controller, a computer vision camera, a single-board computer, laser sensors, a voltage regulator, a lithium-polymer battery. In programming language Python 3.7.2, an automatic manipulator control algorithm was developed. We concluded that in order to establish optimal operating modes and successful implementation of this technology, additional research is needed to fully optimize technology for a wide range of conditions in agricultural production of horticultural crops."
9298163,"A Black Box Approach to Inferring, Characterizing, and Breaking Native Device Tracking Autonomy","Autonomous capabilities have emerged as a ubiquitous technology in cars, robots, drones and many other useful devices that are destined to transform our society. Many of the more advanced autonomous devices use computer vision to track and recognize objects, while others may use LIDAR or SONAR or other forms of radar. Since many of these devices are positioned to be disruptive technologies in our society, the fidelity of their abilities is paramount. In this paper, we introduce our black box approach, which can accurately infer, characterize, and break native device autonomy. We demonstrate this approach by using the tightly guarded autonomy code in the immensely popular computer vision driven autonomous DJI drones (i.e., ActiveTrack Mode) as the target native autonomy. We illustrate that this method allows us to quickly infer detailed information about the computer vision tracking algorithm, characterize the autonomous capability, and identify targets that defy these algorithms. We posit that this approach can be extended to other computer vision driven autonomous systems and is a necessary step in developing nondestructive privacy preserving mechanisms that foil the presumed tracking process."
9381804,Vision System Prototype for Inspection and Monitoring with a Smart Camera,"This paper presents the design of an artificial vision system prototype for automatic inspection and monitoring of objects over a conveyor belt and using a Smart camera 2D BOA-INS. The prototype consists of a conveyor belt and an embedded system based on an Arduino Mega card for system control, and it has as main peripherals the smart camera, a direct current motor, a photoelectric sensor, LED illumination and LEDs indicating the status (good or defect) of each evaluated object. The application of the prototype is for educational purposes, so that undergraduate, master and diploma students can simulate a continuous production line, controlled by an embedded system, and perform quality control by monitoring through a visual system and a personal computer. This allows implementing the topics of embedded systems, artificial vision, artificial intelligence, pattern recognition, automatic control, as well as automation of real processes."
9240503,Evaluation of Color Vision Compensation Algorithms for People with Varying Degrees of Color Vision Deficiency,"People with color vision deficiency (CVD) may have difficulty in discriminating colors. To improve their color perception, several compensation methods have been proposed which considered naturalness maintenance and contrast emphasis. All these methods are based on the simulation model of severe CVD and hence it is not clear whether are also effective for people with light CVD. In this paper, we conduct subjective study to evaluate the effectiveness of these methods for people with varying degrees of CVD."
9650049,Calculation of Mahjong Score using AI,"In recent years, declining birth rates and aging populations had become more serious in developed countries such as Europe, the United States, and Asian countries, including Japan. Therefore, in this research, we focused on mahjong, which was effective in preventing dementia. However, it was difficult for beginners and even intermediate players to proceed with the game instantly because mahjong has complicated yaku (hand), and the calculation of scores using them is even more complicated. Therefore, we were developing a mahjong progression support system using computer vision aiming at helping players with dementia who have difficulty in calculating mahjong scores. Since simple learning did not produce results that could be used in the proposed system, we achieved significant results by changing the size of the mahjong tiles in the images to be learned, and the system is now able to calculate mahjong scores from the images of mahjong tiles."
9650248,Design and Implementation of Gesture Recognition Rehabilitation System Based on Deep Learning,"Gesture recognition is an important way of human-computer interaction. In view of the problems such as the single function of rehabilitation equipment, repetitive and boring movements in the training process, and slow recovery process, a gesture recognition rehabilitation system based on deep learning was proposed. Firstly, the key technologies of computer vision such as image acquisition, segmentation, smooth processing, classification and recognition are introduced. Secondly, the implementation details of gesture rehabilitation system are emphasized. Gesture sample data of people of different ages and genders were collected by camera to establish a rehabilitation gesture database. The convolutional neural network training model and PyQt graphical interface were used to build a rehabilitation system. The experimental results show that the system is reliable and accurate, which can not only improve the interest and enthusiasm of patients to rehabilitation training, but also is cheaper than other rehabilitation equipment and has a broader application prospect."
8920359,Perfect Storm: DSAs Embrace Deep Learning for GPU-Based Computer Vision,"This paper explores Domain-Specific Deep Learning Architectures for GPU Computer Vision through a ""brainstorming"" approach on selected hands-on topics in the area. We intend to discuss Deep Neural Networks (DNNs) to image classification problems through tools, frameworks and data pipelines commonly used to train and deploy DNNs in GPUs and Domain-Specific Architectures (DSAs)."
9432195,Machine Vision Based Potato Species Recognition,"Potato is one of the tasteful vegetable in the list of our daily delicious food. At present, there are 42000 kind of potatoes available in the world. In Bangladesh, we cultivate 82 species every year. Potatoes are used for other purposes besides eating. So it is produced by thinking of other purposes besides eating. Different varieties of potatoes are used for different purposes. But People usually do not know which variety of potato is suitable for which work. We took this research step to solve this problem. There are currently some conventional common detection methods that are not very convenient. Therefore, we have introduced Machine Vision Recognition (MVR) procedure to discover a suitable technique. As if, through this method people can easily identify the potato species. In this research paper, we want to show how to identify different varieties of potatoes in Bangladesh using machine vision approach. We have been collected total 1200 potato imagers from four fact for our experience. To reach our aim, several machine learning algorithms have been applied to the datasets, like Random Forest Classifier (RF), Linear Discriminant Analysis (LDA), Logistic Regression, Support Vector Machine (SVM), CART, NB, and KNN. Once we have been applied all the algorithms, different results have been shown by each algorithm. Logistic Regression shows the best result, which has an accuracy rate of 98%. In contrast, the lowest rate has been shown by the SVM. The accuracy rate of SVM is 33% which is not only a good fit for future research but also promising."
8822186,AI Based Automatic Robbery/Theft Detection using Smart Surveillance in Banks,"Deep learning is the segment of artificial intelligence which is involved with imitating the learning approach that human beings utilize to get some different types of knowledge. Analyzing videos, a part of deep learning is one of the most basic problems of computer vision and multi-media content analysis for at least 20 years. The job is very challenging as the video contains a lot of information with large differences and difficulties. Human supervision is still required in all surveillance systems. New advancement in computer vision which are observed as an important trend in video surveillance leads to dramatic efficiency gains. We propose a CCTV based theft detection along with tracking of thieves. We use image processing to detect theft and motion of thieves in CCTV footage, without the use of sensors. This system concentrates on object detection. The security personnel can be notified about the suspicious individual committing burglary using Real-time analysis of the movement of any human from CCTV footage and thus gives a chance to avert the same."
8772048,Computer Vision in Esophageal Cancer: A Literature Review,"Esophageal cancer is a disease with a high prevalence that can be evaluated by a variety of imaging modalities, including endoscopy, computed tomography, and positron emission tomography. Computer-aided techniques could provide a valuable help in the analysis of these images, decreasing the medical workflow time and human errors. The goal of this paper is to review the existing literature on the application of computer vision techniques in the domain of esophageal cancer. After an initial phase where a set of keywords was chosen, the selected terms were used to retrieve papers from four well-known databases: Web of Science, Scopus, PubMed, and Springer. The results were scanned by merging identical entries, and eliminating the out of scope works, resulting in 47 selected papers. These were organized according to the image modality. Major results were then summarized and compared, and main shortcomings were identified. It could be concluded that, even though the scientific community has already paid attention to the esophageal cancer problem, there are still several open issues. Two major findings of this review are the nonexistence of works on MRI data and the under-exploration of recent techniques using deep learning strategies, showing the need for further investigation."
9590697,Research on Machine Vision Technology of High Speed Robot Sorting System Based on Deep Learning,"With the improvement of industrial automation level and the progress of science and technology, the number of robots is increasing, the application scenarios are becoming more and more complex, and the requirements for automation, intelligence, precision, stability and flexibility of robots are also increasing. Machine vision refers to the use of machines instead of human eyes for measurement and judgment. The ultimate goal of machine vision is to enable machines to observe and understand the input image data accurately like human eyes, and finally make decisions to achieve the purpose of adapting to the environment autonomously. In the traditional industrial production line, the task of sorting workpieces is carried out manually, which is not only inefficient but also costly. It is the trend of industrial automation to apply machine vision technology to sorting tasks of industrial robots. According to the actual needs of China's production industry, based on advanced technologies such as deep learning algorithm and machine vision, this paper constructs a high-speed robot sorting system for product production to improve the overall operation effect of the robot sorting system."
9292281,A Chip-Level Verification Method for Programmable Vision Chip Based on Deep Learning Algorithms,"The past five years has witnessed tremendous success of deep learning (DL) algorithm in the computer vision field, attributing to its high degree of accuracy on numerous visual tasks. Unfortunately, for the development of programmable vision chips, algorithm verification remains a major challenge due to the high computational complexity of the DL neural network. In this paper, we propose a novel chip-level verification method to address the common issues including low efficiency and poor reusability in verifying vision chips. In contrast to the block-level verification technique, this method focuses on the rapid implementation of the complete DL algorithm in chip-level verification, fulfilling the advanced demands of vision chip prior to the tape-out. The experiments on MobileNet-v1 indicates the significant reduction of the simulation time and debugging overheads via the proposed verification method."
9418955,The Realization Method of Network Class Condition Monitoring System Based On Machine Vision,"Aimed at the problem of teachers' inability to monitor students' class status during the Online Class, the Online Class atmosphere is generally not as good as the offline. This paper proposed a method based on CNN (Convolutional Neural Network) to detect and analyze students' class status. Based on the proposed method, we designed a software system for supervising students in online classes. The main function of the software system includes intelligent judgement of students' class state through the camera and real-time feedback status information. After unit tests and system tests, it is found that the method can well identify and determine the student's class status. The system proved that it can effectively improve the atmosphere and order for the Online Class."
9112025,Methods of Automatic Correction of Technological Trajectory of Laser Welding Complex by Means of Computer Vision,"The paper deals with the problem of developing a system for automatically correcting the technological trajectory in the working space of a laser robotic complex in the pre-process. To solve this problem, a method was proposed for correcting the points of the technological trajectory based on detecting and recognizing the edges of the joint of welded workpieces in the image obtained from a CCD camera mounted on an industrial complex tool. To do this, they performed a preliminary segmentation of the separating line, then made a recognition of the edges of the workpieces and determined the correction point relative to the position of the tool. The results of the work were successfully tested at the industrial complex of laser robotic welding."
9077523,Challenges of Designing Computer Vision-Based Pedestrian Detector for Supporting Autonomous Driving,"In recent years, aiming to improve deriving safety and supporting autonomous driving, pedestrian detection has attracted considerable attention from both industry and academic. Moreover, by taking advantage of the powerful computational capacity of GPU and high-level feature learning ability of the deep convolutional neural network, tremendous image/video-based pedestrian detection methods have been proposed. However, most of the existing approaches are designed relying on the computer vision-based target detection techniques. Accordingly, the evaluation criteria they consider in the design are often from the computer vision research field. Therefore, these existing methods tend to focus on the improvement of accuracy and ignore some of the special requirements that need to be considered in the field of autonomous driving. In this paper, we will analyze and summarize the features of the state-of-the-art pedestrian detection methods in detail. Then, by considering the practical application scenarios of autonomous driving techniques, we further discuss the open challenges of designing a practical pedestrian detection method for supporting autonomous deriving."
9671222,Computer-Vision Enabled Waste Management System for Green Environment,"Waste management has become a critical requirement to maintain a green environment in Sri Lanka as well as other countries. Town councils have to regularly collect different types of wastes to clean cities/towns. Hence managing the waste of the cities is a challenging task. However, most of the urban councils currently use a manual approach to managing waste. However, it results in many difficulties for the people and cleaning staff who involve in the process by following strict guidelines. Issues due to waste contamination, no proper information management of waste collection, and no punctuality in removing waste from the garbage bins are some of the significant issues arising from the manual process. Due to the drawbacks of the manual approach, social issues, environmental issues, health issues can occur easily. This paper proposes a better solution to replace this manual system with an automated system to overcome these issues. Hence, the main objective of this research is to introduce an ICT-based innovative design that can be used to develop an effective waste management system in town councils. In the proposed model, we will introduce a Computer Vision-based smart waste bin system with real-time monitoring that incorporates various technologies such as computer vision, sensor-based IoT devices, and geographical information system (GIS) related technologies. Our proposed solution consists of a waste bin system, which is capable of automated waste segregation. Our design facilitates the admin users to expand the waste bin kit by adding more waste categories in a user-friendly manner, making our product adaptive in any environment. At the same time, waste bins can notify the real-time waste status. Our system generates the optimum collection routing path and displays it in a mobile app using those real-time status details. We also demonstrate a low-cost prototype."
9150822,NTIRE 2020 Challenge on Video Quality Mapping: Methods and Results,"This paper reviews the NTIRE 2020 challenge on video quality mapping (VQM), which addresses the issues of quality mapping from source video domain to target video domain. The challenge includes both a supervised track (track 1) and a weakly-supervised track (track 2) for two benchmark datasets. In particular, track 1 offers a new Internet video benchmark, requiring algorithms to learn the map from more compressed videos to less compressed videos in a supervised training manner. In track 2, algorithms are required to learn the quality mapping from one device to another when their quality varies substantially and weakly- aligned video pairs are available. For track 1, in total 7 teams competed in the final test phase, demonstrating novel and effective solutions to the problem. For track 2, some existing methods are evaluated, showing promising solutions to the weakly-supervised video quality mapping problem."
9734219,Improved Computer Vision-based Framework for Electronic Toll Collection,"The world is moving towards artificial intelligence and automation because time is the most crucial asset in today’s scenario. This paper proposes an automatic vehicle fingerprinting system that avoids long waiting times in toll plazas with the help of computer vision. The number plate recognition and vehicle re-identification focus on this research. Day/night IR cameras are used to get the images of the vehicle and its number plate. The VeRi776 datum, which contains real-world vehicle images, is used to facilitate the research of vehicle re-identification. The proposed framework employs Siamese model architecture to identify the attributes such as color, model, and type of vehicle. The Car License Plate Detection datum is used to evaluate the efficiency of the proposed license plate recognition system. An ensemble of image localization techniques using CNNs and application of the OCR model on the localized snapshot is used to recognize the vehicle’s license plate. A combination of license plate recognition and vehicle re-identification techniques is used in the proposed framework to improve the efficiency of identifying vehicles in toll plazas"
9390646,Design of Solder Quality Inspection System Based on Machine Vision,"The electronic water pump for vehicles adjusts the working state of the water pump with the help of the electronic control unit, and then pressurizes the coolant, to circulate the coolant in the cooling system to ensure that the engine temperature does not exceed the limit threshold and can work normally. Therefore, it is a very important link to inspect internal circuit boards in the production of water pumps. Traditional manual inspection has limited working time, while automatic machine vision inspection can perform 24-hour uninterrupted inspection. Compared with manual inspection, it is more reliable and stable, which has advantages in speed, and has a wide range of industrial applications. According to this, this paper designs a solder joint inspection system based on machine vision, which can detect the status of solder joints and feed back the current soldering results to the workers. The method solves the problem of automatically detecting the welding quality of the circuit board in the manual welding process, greatly improves the production efficiency of the workshop production line, and shortens the product production cycle."
9546367,Power Equipment State Recognition Method Based on Binocular Vision Video,"A state detection method of power equipment based on Binocular vision video is proposed. The power equipment video stream is obtained by binocular camera, and the inspection image is obtained by extracting the video key frames. The disparity map of the region of interest in the inspection image is calculated, and the distance information distribution is obtained from the disparity map. According to the distance information distribution, the status of electrical equipment is determined. Through the principle of binocular vision ranging, equipment status identification is realized, which provides a new method for the identification of substation equipment status, and realizes the rapid and accurate identification of power equipment status."
8778050,A 132 by 104 10μm-Pixel 250μW 1kefps Dynamic Vision Sensor with Pixel-Parallel Noise and Spatial Redundancy Suppression,"This paper reports a 132 by 104 dynamic vision sensor (DVS) with 10μm pixel in a 65nm logic process and a synchronous address-event representation (SAER) readout capable of 180Meps throughput. The SAER architecture allows adjustable event frame rate control and supports pre-readout pixel-parallel noise and spatial redundancy suppression. The chip consumes 250μW with 100keps running at 1k event frames per second (efps), 3-5 times more power efficient than the prior art using normalized power metrics. The chip is aimed for low power IoT and real-time high-speed smart vision applications."
8834015,Use of Computer Vision for White Line Detection for Robotic Applications,"Image processing and its use for object detection that allows to alter decisions based on what is being observed remains a challenge today. As participants in the Intelligent Ground Vehicle Competition (IGVC) 2018 and 2019, we were tasked to build an autonomous vehicle that is capable of maneuvering and traversing through a grassy course delineated with white lines and laid out with scattered obstacles. In order to remain within the white line boundaries, computer vision was used with the implementation of OpenCV and Python for image processing and white line detection. To accomplish the white line detection task, various image processing techniques such as change of color space, image filtering, blurring, histogram equalizing, and edge detection are utilized. Testing of the image processing techniques was performed using prerecorded videos from previous competitions as well as videos taken using the autonomous vehicle's ZED camera. In both test cases there were challenges due to differences in lighting conditions, image clarity between cameras used for both video types, and the angle of the camera."
8944953,Development of Training Package on Machine Vision applying STEM Approach for Industrial Education,"This paper aims to develop of training package on machine vision applying STEM education for industrial course. First, we surveyed the needs for training content before analyzing various training package from related documents and revised related literature. Next, we developed the STEM education which consists of four processes including Science, Technology, Engineering, and Mathematics. After that, we constructed a training package with LabView Program. The training package contains a trainer's manual, PowerPoint presentation, an experimental set and an achievement test. Finally, the quality of the training package was evaluated by five experts and was implemented by a sampling group of 15 trainees. The developed training package is efficient in accordance to the Meguigans' theory (equaled to 1.81). The trainee satisfaction towards developed training package obtains the average score of 4.51 (satisfied)."
8794906,Demonstration of Applications in Computer Vision and NLP on Ultra Power-Efficient CNN Domain Specific Accelerator with 9.3TOPS/Watt,"Computer Vision and Natural Language Processing (NLP) applications are becoming available at edge devices and mobile platforms with the mass production of low-power and high-performance AI chips. SPR2801s is the CNN Domain Specific Accelerator (CNN-DSA) with inference speed of more than 140fps for input image size of 224×224×3 and only 300mW power consumption. The convolution computations are all completed on SPR2801s chip which works as a co-processor. In this demo, we will show demos running on SPR2801s for computer vision and NLP applications, including image classification, text classification, sentiment analysis, and Compact Descriptor for Video Analysis (CDVA). The applications are demonstrated on a single chip and also demonstrated on a multi-chip board. The single chip is shown as a dongle with USB connecting to a host processor. And the eight-chip board shows the power of parallel processing with PCIe or M.2 interface."
9576770,A Simplified Skeleton Joints Based Approach For Human Action Recognition,"The growing technological development in the field of computer vision in general, and human action recognition (HAR), in particular, have attracted increasing number of researchers from various disciplines. Amid the variety of challenges in the field of human action recognition, one of the major issues is complex modelling which requires multiple parameters leading to troublesome training which further requires heavy configuration machines for real-time recognition. Therefore, there is a need to develop a simplified method that could result in reduced complexity, without compromising the performance accuracy. In order to address the mentioned issue, this paper proposes an approach that extracts the mean, variance and median from the skeleton joint locations and directly uses them in the classification process. The system used MCAD dataset for extracting 2D skeleton features with the help of OpenPose technique, which is suitable for the extraction of skeleton features from the 2D image instead of 3D image or using a depth sensor. Henceforth, we avoid using either the RGB images or the skeleton images in the recognition process. The method shows a promising performance with an accuracy of 73.8% when tested with MCAD dataset."
9262943,Vision-Aided Radio: User Identity Match in Radio and Video Domains Using Machine Learning,"5G is designed to be an essential enabler and a leading infrastructure provider in the communication technology industry by supporting the demand for the growing data traffic and a variety of services with distinct requirements. The use of deep learning and computer vision tools has the means to increase the environmental awareness of the network with information from visual data. Information extracted via computer vision tools such as user position, movement direction, and speed can be promptly available for the network. However, the network must have a mechanism to match the identity of a user in both visual and radio systems. This mechanism is absent in the present literature. Therefore, we propose a framework to match the information from both visual and radio domains. This is an essential step to practical applications of computer vision tools in communications. We detail the proposed framework training and deployment phases for a presented setup. We carried out practical experiments using data collected in different types of environments. The work compares the use of Deep Neural Network and Random Forest classifiers and shows that the former performed better across all experiments, achieving classification accuracy greater than 99%."
9239822,Research on Object Depth Information Measurement System in Image,"With the rapid development of computer science and technology and the electronic industry, 3d reconstruction technology in computer vision has received extensive attention, among which the accuracy of 3D reconstruction mainly depends on the acquisition of scene depth information. However, ordinary camera can obtain two-dimensional monocular visual image when shooting the objective world scene. This situation leads to the machine cannot get the object distance, size and motion speed information from the image itself, which affects the decision system to make the correct decision. In order to solve the traditional depth information measurement method complex operation, high cost, measurement equipment occupies a large space and load. In this paper, based on Harris-Sift algorithm, a monocular vision technique is proposed to measure the absolute depth information of the object in the image using monocular vision. The comparison results show that the error rate between the actual distance and the measured distance is less than 3.5%. This shows that this method can accurately measure the absolute depth of objects in static and short distance, and is superior to other methods."
9639161,Reconstruction of a Three-Dimensional Scene from its Projections in Computer Vision Systems,"The problem of reconstructing a three-dimensional scene from its projections is one of the most urgent and studied among the problems solved by computer vision methods. Most often, such systems are implemented by creating geographically distributed complexes that require the transfer of a large amount of data between their components, which creates a large load on the transmission lines. Scene reconstruction using projections obtained from several observation points is a special case and is not always possible, especially when used in unmanned aerial vehicles or autonomous vehicles. To resolve these limitations, the possibility of using a computer stereoscopic vision system and a three-dimensional convolutional neural network capable of reconstructing a complete scene from a single image containing subjective characteristics of scene objects, such as color and depth, is considered in this work, creating its three-dimensional representation. The article describes the architecture and principle of operation of a neural network, and also builds a model of a computer stereo vision stand for image registration. An image formation algorithm is proposed for this model."
8987908,Real-Time Two Way Communication System for Speech and Hearing Impaired Using Computer Vision and Deep Learning,"Sign Language is the most expressive form of communication for speech and hearing impaired people to communicate with normal person but a normal person cannot understand sign language. So in order to break this barrier of communication there needs to be a system that can enable conversion of sign language to voice or text and voice or text to sign language and do it in real time. The systems that currently exist are not real time, do not facilitate two-way communication, require static surrounding conditions or have low recognition accuracy. There exist systems that have good accuracy but require external hardware like gloves [3] which increases the cost. Our contribution to solving this problem consists of a Sign Language Communication System. It is a real-time communications system built using the advancements in Image Processing, Deep Learning and Computer Vision that provides real-time sign language to text and text to sign language conversion. The project is software-based which can be installed on any computer with good specifications. It is also a two-way communication system allowing not just speech and hearing impaired to communicate with normal people but also other way around. The primary goal of our system is to enable hearing and speech impaired people to communicate with people that are not disabled in real time by interpreting alphabets, numbers and words in the Indian sign language. The system is able to predict 17600 test images in 14 seconds with an average prediction time of 0.000805 seconds with an accuracy of 99%."
9005709,Digital Legacies on Paper: Reading Punchcards with Computer Vision,We describe the development of a computer visionbased workflow for normalizing images of the legacy punchcard data format (IBM 029 - 80 column punchcard standard) and then reading the encoded data. We show the role of a newly developed Punchcard Extractor Tool within the Brown Dog service API. We also point to our showcase of these same computer vision techniques in a Jupyter notebook system.
9412295,RescueNet: Joint Building Segmentation and Damage Assessment from Satellite Imagery,"Accurate and fine-grained information about the extent of damage to buildings is essential for directing Humanitarian Aid and Disaster Response (HADR) operations in the immediate aftermath of any natural calamity. In recent years, satellite and UAV (drone) imagery has been used for this purpose, sometimes aided by computer vision algorithms. Existing Computer Vision approaches for building damage assessment typically rely on a two stage approach, consisting of building detection using an object detection model, followed by damage assessment through classification of the detected building tiles. These multi-stage methods are not end-to-end trainable, and suffer from poor overall results. We propose RescueNet, a unified model that can simultaneously segment buildings and assess the damage levels to individual buildings and can be trained end-to-end. In order to to model the composite nature of this problem, we propose a novel localization aware loss function, which consists of a Binary Cross Entropy loss for building segmentation, and a foreground only selective Categorical Cross-Entropy loss for damage classification, and show significant improvement over the widely used Cross-Entropy loss. RescueNet is tested on the large scale and diverse xBD dataset and achieves significantly better building segmentation and damage classification performance than previous methods and achieves generalization across varied geographical regions and disaster types."
8803535,Estimating Physical Activity Intensity And Energy Expenditure Using Computer Vision On Videos,"Estimating physical activity (PA) intensity and energy expenditure (EE) is a problem that typically requires the use of wearable sensors such as a heart rate monitor, or accelerometer. We investigate the accuracy of a computer vision system using videos recorded from a pair of wearable video glasses to estimate PA strength and EE automatically using age, gender, speed, and activity cues. Age and gender are obtained using the Deep EXpectation network, while activity is estimated from joint angles and movement speed. We also present results on a study of 50 participants performing four different activities while measuring corresponding features of interest such as height, weight, age, sex, and ground truth EE and PA strength data collected via accelerometer. We present both the results of each computer vision subsystem and overall accuracy of the PA strength estimation (89.5%) and the average EE difference (1.96 kCal/min)."
8981084,Automatic Pointer Meter Reading Based on Machine Vision,"Pointer meters in power systems are wildly used. Although smart meters gradually replace pointer meters to conveniently acquire measurement, digitalization of some special meters have not been available. The paper presents a new method based on machine vision to detect pointer meter reading automatically. Two phases of training and testing are proposed in this method. In the training phase, a template of meter feature points on a training image is extracted. In the testing phase, the template matching performs on a testing image to find feature points. Then, an elliptic arc fitting based on the feature points to generate a 1-D signal. Finally, the 1-D signal is used to locate the pointer and the reading result is obtained. Differing with existing methods, the proposed method performs effectively and efficiently automatic meter reading without using Hough transformation."
9134692,"Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art","act:
Recent years have witnessed enormous progress in AI-related fields such as computer vision, machine learning, and autonomous vehicles. As with any rapidly growing field, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several survey papers on particular sub-problems have appeared, no comprehensive survey on problems, datasets, and methods in computer vision for autonomous vehicles has been published. This monograph attempts to narrow this gap by providing a survey on the state-of-the-art datasets and techniques. Our survey includes both the historically most relevant literature as well as the current state of the art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding, and end-to-end learning for autonomous driving. Towards this goal, we analyze the performance of the state of the art on several challenging benchmarking datasets, including KITTI, MOT, and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we also provide a website that allows navigating topics as well as methods and provides additional information."
9108684,Geometry-Guided Adaptation for Road Segmentation,"We propose a novel adaptation method for generalizing road segmentation to novel weather, lighting or viewing geometries. The method assumes a source domain consisting of an ensemble of labeled training datasets and an unlabeled target test dataset that deviates substantially from the training ensemble. The training dataset is used to compile a geometry-anchored prior over the road pixel locations and to train a fully-convolutional network road segmentation system. At inference, a probabilistic Houghing method is used to detect line intersections in the test image and thereby estimate the vanishing point of the road, thus anchoring the learned geometric prior. This prior is then used to extract high confidence road and background regions which serve as surrogate ground truth to adapt the network to the target domain. Leave-one-out evaluation across five diverse road segmentation datasets demonstrates substantial improvement in generalization across changes in viewing geometry and weather conditions, yielding results that are on average comparable and in some cases superior to a more complex GAN-based domain adaptation approach. These results demonstrate the potential for classical computer vision methods to guide adaptation of supervised machine learning algorithms, leading to improved generalization across domains."
9025480,Live Demonstration: A Real-Time Event-Based Fast Corner Detection Demo Based on FPGA,"Corner detection is widely used as a pre-processing step for many computer vision (CV) problems. It is well studied in the conventional CV community and many popular methods are still used nowadays such as Harris, FAST and SIFT. For event cameras like Dynamic Vision Sensors (DVS), similar approaches also have been proposed in recent years. Two of them are event-based harris(eHARRIS) and event-based FAST (eFAST). This demo presents our recent work in which we implement eFAST on MiniZed FPGA. The power consumption of the whole system is less than 4W and the hardware eFAST consumes about 0.9W. This demo processes at least 5M events per second, and achieves a power-speed improvement factor product of more than 30X compared with CPU implementation of eFAST. This embedded component could be suitable for integration to applications such as drones and autonomous cars that produce high event rates."
9464922,When Wireless Communications Meet Computer Vision in Beyond 5G,"This article articulates the emerging paradigm, sitting at the confluence of computer vision and wireless communication, enabling beyond-5G/6G mission-critical applications (autonomous/ remote-controlled vehicles, visuo-haptic virtual reality, and other cyber-physical applications). First, drawing on recent advances in machine learning and the availability of non-radio-frequency (RF) data, vision-aided wireless networks have been shown to significantly enhance wireless communication reliability without sacrificing spectral efficiency. In particular, we demonstrate how computer vision enables look-ahead prediction in a millimeter-wave channel blockage scenario before the blockage actually occurs. From a computer vision perspective, we highlight how RF-based sensing and imaging are instrumental in robustifying computer vision applications against occlusion and failure. This is corroborated via an RF-based image reconstruction use case, showcasing a receiver-side image failure correction resulting in reduced retransmission and latency. Taken together, this article sheds light on the much needed convergence of RF and non-RF modalities to enable ultra-reliable communication and truly intelligent 6G networks."
8776763,A Review of Application of Computer-vision for Quality Grading of Food Products,Quality grading is necessary for making any food product marketable. Quality parameters of food products include both Visual and Olfactory factors. The paper attempts to discuss studies conducted in the field of Computer Vision that in-turn uses Digital Image processing and Soft-computing tools in the field of Food Processing Industry. The outcome of this paper will help in understanding the application of Computer Vision in automating the inspection of food products based on various well define Quality parameters.
9468955,Refractive Light-Field Features for Curved Transparent Objects in Structure From Motion,"Curved refractive objects are common in the human environment, and have a complex visual appearance that can cause robotic vision algorithms to fail. Light-field cameras allow us to address this challenge by capturing the view-dependent appearance of such objects in a single exposure. We propose a novel image feature for light fields that detects and describes the patterns of light refracted through curved transparent objects. We derive characteristic points based on these features allowing them to be used in place of conventional 2D features. Using our features, we demonstrate improved structure-from-motion performance in challenging scenes containing refractive objects, including quantitative evaluations that show improved camera pose estimates and 3D reconstructions. Additionally, our methods converge 15-35% more frequently than the state-of-the-art. Our method is a critical step towards allowing robots to operate around refractive objects, with applications in manufacturing, quality assurance, pick-and-place, and domestic robots working with acrylic, glass and other transparent materials."
8966152,Development Environment Proposal for Automotive Vision-Based Algorithms,"Next generation autonomous driving functions heavily depend on various computationally intensive algorithms based on machine vision. Design and development of such algorithms is challenging, requiring engagement of a wider community and the availability of appropriate, cost-effective tooling. Setting up a hardware and software environment that would both reflect actual automotive requirements and facilitate development and prototyping, is an uneasy task. In this paper, we analyze the available development platforms which could be utilized in automotive machine vision, with specific regard to new algorithm requirements, such as the utilization of AI, hardware acceleration and low latency. We give a proposal of an affordable and scalable development setup consisting of heterogeneous, vision-enabled SoCs and automotive interfaces."
9532631,Graph Neural Network (GNN) in Image and Video Understanding Using Deep Learning for Computer Vision Applications,"Graph neural networks (GNNs) is an information - processing system that uses message passing among graph nodes. In recent years, GNN variants including graph attention network (GAT), graph convolutional network (GCN), and graph recurrent network (GRN) have shown revolutionary performance in computer vision applications using deep learning and artificial intelligence. These neural network model extensions, collect information in the form of graphs. GNN may be divided into three groups based on the challenges it solves: link prediction, node classification, graph classification. Machines can differentiate and recognise objects in image and video using standard CNNs. Extensive amount of research work needs to be done before robots can have same visual intuition as humans. GNN architectures, on the other hand, may be used to solve various image categorization and video challenges. The number of GNN applications in computer vision not limited, continues to expand. Human-object interaction, actin understanding, image categorization from a few shots and many more. In this paper use of GNN in image and video understanding, design aspects, architecture, applications and implementation challenges towards computer vision is described. GNN is a strong tool for analysing graph data and is still a relatively active area that needs further researches attention to solve many computer vision applications."
9607695,Is First Person Vision Challenging for Object Tracking?,"Understanding human-object interactions is fundamental in First Person Vision (FPV). Tracking algorithms which follow the objects manipulated by the camera wearer can provide useful cues to effectively model such interactions. Visual tracking solutions available in the computer vision literature have significantly improved their performance in the last years for a large variety of target objects and tracking scenarios. However, despite a few previous attempts to exploit trackers in FPV applications, a methodical analysis of the performance of state-of-the-art trackers in this domain is still missing. In this paper, we fill the gap by presenting the first systematic study of object tracking in FPV. Our study extensively analyses the performance of recent visual trackers and baseline FPV trackers with respect to different aspects and considering a new performance measure. This is achieved through TREK-150, a novel benchmark dataset composed of 150 densely annotated video sequences. Our results show that object tracking in FPV is challenging, which suggests that more research efforts should be devoted to this problem so that tracking could benefit FPV tasks."
9350927,Power Metering Wiring Evaluation System Based on Machine Vision Technology,"Based on machine vision technology, the power metering wiring process evaluation system realizes the objective evaluation of the wiring process quality. Through the high-precision machine vision acquisition, the accuracy, horizontality level, verticality level and line spacing of the process wiring are objectively digitized, and the system gathers real-time machine vision data, automatic distortion correction through an adaptive correction algorithm to achieve image data cleaning and storage, and through the machine vision algorithm, the process wiring parameters such as horizontality and verticality, and score are calculated according to the scoring standard. The evaluation system also interprets deductions, finds problems, and realizes iterative improvement of the wiring process level driven by objective data."
8857089,The Method for Visualization and Analysis of Eye-blinking Patterns using Dynamic Vision**,"This study proposes a method for measurement and visualization of eye blinking patterns using dynamic vision image. The dynamic vision data is the outputs of only the pixels with temporal luminance changes. Eye blinking analysis using dynamic vision data for can significantly reduce the time cost of comparing consecutive frames to find eyelid changes. In addition, image processing for analysis is simplified because the data is gray-scale image and does not include unnecessary data for analysis such as background or accessories worn by examinee. The shape and position of an upper eyelid are estimated from the dynamic vision data from which unnecessary data and noise has been removed. And the changes of the eyelid during a blinking sequence are visualized as a single image. The result image makes it easier to analyze the eye blinking patterns at a glance."
9497841,Detection and Blur-Removal of Single Motion Blurred Image using Deep Convolutional Neural Network,"This paper proposes a simple and efficient motion blur detection and removal method based on Deep CNN. The domain of computer vision has gained significant importance in recent years due to insurgence in the fields of self-driving cars, UAVs, medical image processing, etc. Due to low light conditions and the camera's fast motion, a large portion of image data generated is wasted. Such motion-blurred images impose a great challenge to the algorithms used for decision-making in machine vision. Although there have been significant improvements in denoising such image data, these methods are challenged by time constraints, insufficient data to train, reconstructed image quality, etc. The proposed paper employs a learning method to detect and deblur the single input image even in the absence of a ground-truth sharp image. We have used a synthetic dataset for experimental evaluation. This synthetic dataset that we have created and used for training the DCNN model has been made available for open source on Kaggle at the following link: https://www.kaggle.com/dikshaadke/motionblurdataset"
9521454,Rapid Multi-Sensor Fusion and Integration Using AI-enabled Machine Vision for Real-Time Operator Physiological Status,Real-time in-situ tracking of operator physiological status requires the integration and analysis of inputs from multiple sensors. Data formats and sample rates differ making the integration of data streams challenging. The use of external AIML Machine Vision can increase data analysis multivariate sensor fusion in such applications
9277027,Research on 3d Reconstruction of Target based on Double Vision,"For the 3D imaging needs of the target in virtual reality, a black background is constructed with the help of binocular stereo vision technology. Through panoramic image acquisition of the target, ORB feature matching and ICP point cloud registration are used to generate 3D panoramic point cloud data of the target. In view of the point cloud data generated, after filtering, get all non-zero target information and zero background information, build a minimum circumscribed cube., the transformation of coordinates of the point cloud data and generate a normalized coordinate of the point cloud data. After the experiment, the 3D reconstruction effect of the target under pure black background is better than that including the background. After the coordinate normalization, the reconstructed target can be rotated and scaled, which can reflect the texture characteristics of the target and provide data support for the target to be transplanted into virtual."
9692550,COMPUTER VISION MODEL FOR LOW VOLTAGE VEGETATION MANAGEMENT,"Vegetation management is a key activity for E-REDES, the Portuguese main DSO, in order to ensure the reliability and safety of the electric grid and to preserve the environment on its surroundings. In particular, the low voltage power (LV) grid poses a significant challenge, due to its extension, characteristics and lack of adequate technologies for inspection. Nowadays, the search for vegetation anomalies at low voltage level is done via visual inspection that are part of the systematic rounds, done periodically by vehicle to the grid. To face the vegetation management difficulties and to leverage on those car rounds, E-Redes developed and tested, in partnership with everis, a proof of concept for the application of computer vision technology to identify vegetation issues in the LV grid through the analysis of video footage of the grid recorded by a moving vehicle. The paper will address the strategy and methodology for implementing this solution, the technical approach, the results obtained during the test phase and the insights gathered from those tests. With this work, it was possible to recognize the challenges of using this type of technology in the low voltage grid context and to design future improvements to this type of solutions."
9315117,Redistribution Layer Defect Classification Using Computer Vision Techniques And Machine Learning,"In the semiconductor industry, defects are yield killers and the detection/classification of which can be expensive as well as time consuming. To overcome this challenge, we propose a solution involving Computer Vision Techniques and Machine Learning to accomplish defect binning procedure in typical wafer-level packaging scenario, focusing on 2um L/S redistribution layer (RDL) features. With this approach, inspection cycle time is reduced, thereby driving faster product development."
9324505,Computer Vision Aided Optical Correlator for SAR Target Recognition,"Target recognition is important in SAR applications. Conventional SAR target recognition is based on digital processing.A computer vision aided optical correlator is proposed in this paper. The method is implemented by the correlation between the SAR images and the target library in the optical domain. Ray tracing is used to generate the efficient and accurate target library, and the target parameters are adjustable. Finally, experiment results validate the proposed method."
8809754,Spatio-Temporal Manifold Learning for Human Motions via Long-Horizon Modeling,"Data-driven modeling of human motions is ubiquitous in computer graphics and computer vision applications, such as synthesizing realistic motions or recognizing actions. Recent research has shown that such problems can be approached by learning a natural motion manifold using deep learning on a large amount data, to address the shortcomings of traditional data-driven approaches. However, previous deep learning methods can be sub-optimal for two reasons. First, the skeletal information has not been fully utilized for feature extraction. Unlike images, it is difficult to define spatial proximity in skeletal motions in the way that deep networks can be applied for feature extraction. Second, motion is time-series data with strong multi-modal temporal correlations between frames. On the one hand, a frame could be followed by several candidate frames leading to different motions; on the other hand, long-range dependencies exist where a number of frames in the beginning are correlated with a number of frames later. Ineffective temporal modeling would either under-estimate the multi-modality and variance, resulting in featureless mean motion or over-estimate them resulting in jittery motions, which is a major source of visual artifacts. In this paper, we propose a new deep network to tackle these challenges by creating a natural motion manifold that is versatile for many applications. The network has a new spatial component for feature extraction. It is also equipped with a new batch prediction model that predicts a large number of frames at once, such that long-term temporally-based objective functions can be employed to correctly learn the motion multi-modality and variances. With our system, long-duration motions can be predicted/synthesized using an open-loop setup where the motion retains the dynamics accurately. It can also be used for denoising corrupted motions and synthesizing new motions with given control signals. We demonstrate that our system can create superi...
(Show More)"
9172799,Evaluation of Algorithm-Based Fault Tolerance for Machine Learning and Computer Vision under Neutron Radiation,"In the past decade, there has been a push for deployment of commercial-off-the-shelf (COTS) avionics due in part to cheaper costs and the desire for more performance. Traditional radiation-hardened processors are expensive and only provide limited processing power. With smaller mission budgets and the need for more computational power, low-cost and high-performance COTS solutions become more attractive for these missions. Due to the computational capacity enhancements provided by COTS technology, machine-learning and computer-vision applications are now being deployed on modern space missions. However, COTS electronics are highly susceptible to radiation environments. As a result, reliability in the underlying computations becomes a concern. Matrix multiplication is used in machine-learning and computer-vision applications as the main computation for decisions, making it a critical part of the application. Therefore, the large time and memory footprint of the matrix multiplication in machine-learning and computer-vision applications makes them even more susceptible to single-event upsets. In this paper, algorithm-based fault tolerance (ABFT) is investigated to mitigate silent data errors in machine learning and computer vision. ABFT is a methodology of data error detection and correction using information redundancy contained in separate data structures from the primary data. In matrix multiplication, ABFT consists of storing checksum data in vectors separate from the matrix to use for error detection and correction. Fault injection into a matrix-multiplication kernel was performed prior to irradiation. Irradiation was then performed on the kernel under wide-spectrum neutrons at Los Alamos Neutron Science Center to observe the mitigation effects of ABFT. Fault injections targeted towards the general-purpose registers show a 48 × reduction in data errors using data-error mitigation with ABFT with a negligible change in run-time. Cross-section results from irradiation...
(Show More)"
8978303,Smart Electronic Stick for Visually Impaired using Android Application and Google’s Cloud Vision,"Presently, the visually impaired use a simple stick, which is insufficient to help them perform their daily activities independently. To resolve this issue, the paper proposes a smart and effective solution in the form of an e-stick module integrated with voice controlled android application. The primary aim is to provide a simple and affordable solution for the visually impaired by keeping the stick structurally similar to the traditional stick used by them today, that is thin, lightweight and easy to handle. All these functionalities are being provided at a lower cost and using efficient Natural Language processing (NLP) features. Thus, the proposed method attempts to help the visually impaired to lead a normal life."
8862706,Computer Vision Based Turmeric Leaf Disease Detection and Classification: A Step to Smart Agriculture,"Disease identification plays a vital role in agricultural sector. Turmeric being a rhizomatous crop and well known for its therapeutic effects, monitoring such crops is crucial. The turmeric leaves are mainly exposed to diseases like Leaf Spot and Leaf Blotch. The paper develops an algorithm for detecting and preventing the spreading of diseases to the whole crop and results in high quality crop production. The data base of different leaf images was created and processed using k-Means image segmentation and leaf images textural analysis was carried out using GLCM. SVM classifier is used to classify the feature extracted images after ranking their attributes using an information gain algorithm. A GUI has been created to portray the various stages of the image processing algorithm and detect the two leaf diseases."
9474337,Hot-Spot Zone Detection to Tackle Covid19 Spread by Fusing the Traditional Machine Learning and Deep Learning Approaches of Computer Vision,"Corona Virus is a pandemic, and the whole world is affected due to it. Apart from the vaccine, the only cure for this drastic disease is to follow the rules and regulations that avoid further spread. There are different mechanisms like (Social Distancing, Mask Detection, Human occupancy etc.) through which we can able to stop the spread of the coronavirus. In this paper, we proposed hotspot zone detection using the computer vision techniques of deep learning. We have defined the hotspot area as the particular region on which the person touches more than some specified threshold. We further mark that area to some specific color to help the authority take necessary action and disinfect that particular place. To implement this algorithm, we have utilized the human-object interaction concept. We have extracted the dataset of person classes from the publicly available dataset for the person detection and the self-generated dataset to train the algorithm. Different experiments on object detection algorithms (YOLO-v3, Faster RCNN, SSD) for person detection have been performed in this work. We achieved the maximum accuracy in real-time on the YOLO-v3 for person detection. Whereas we have marked the specific area using the template matching algorithm of computer vision techniques. Our proposed algorithm detects the persons and extracts the region of interest points on which the user draws the rectangle. Then we find the intersection over union ratio between the detected person and the region of interest of the marked area to make the decision. We have achieved 88.72% accuracy on person detection in the local environment. Whereas, for the whole system of human-object interaction for detecting the hotspot area zone, we have achieved 86.7% accuracy using the confusion matrix."
8956633,A Novel Computer Vision System for Integrated Biomolecule and Cell Assays,"Computer vision (CV) is used throughout life sciences for tissue and cell imaging; this paper presents the first demonstration of CV as a biomolecular sensor. We present an integrated sensing and fluid-actuation platform that combines CV with bead-based chemistries to perform both biomolecular and cell-based assays. By rapidly detecting the size and shape of objects at 5-50 micron length scales, the CV system can accommodate a variety of assays that traditionally require separate analytical tools and workflows, providing a greater degree of data integration and automation. This paper demonstrates 3 initial use cases: 1) Semi-quantitation of a biomarker (IgG) using a clustering assay; 2) Enumeration of white blood cells expressing CD45 surface markers using functionalized nanoparticles; and 3) Cell viability detection using dyes. All assays are performed using a single CV platform."
9010257,Super-Resolution Spike Event-based Polarimetric Dynamic Vision Sensor p(DVS) Cognitive Imaging,"In this study, a novel experimental design of a cognitive imaging system aiming at enhancing the spatial and temporal resolution of neuromorphic vision sensors, is presented. Super resolution images of moving or semi-obstructed targets, by means of the “Polarimetric Dynamic Vision Sensor (pDVS)” system, were obtained. This system improves the contrast sensitivity and proves to be an efficient strategy for rapid scene analysis by making use of deep learning. The outcome of this study has a tremendous impact to contribute to the design of innovative bioinspired-based vision systems for rapid classification, identification, motion detection, and tracking."
9579855,Faster Region Based Convolutional Neural Network and VGG 16 for Multi-Class Tyre Defect Detection,"This paper intends to create a deep-learning architecture for training models over various defective tyre images until it has achieved intelligence to distinctly classify multi-class tyre defects and tell apart various types of defects in tyres using computer vision. This paper aims to create a system that can detect defects and allow professionals to spend more time trying to find a fix for it instead of spending time diagnosing it. It wishes to provide an automated Inspection system that is both reliant and fast with improved business sustainability at a lower cost. The models will be created and compared using Faster R-CNN and VGG16, the two architectures of ConvNets primarily used to perform computer vision tasks like recognition, classifications. We performed the multi-class tyre defect detection using a self-made dataset consisting of 970 images."
8929393,Filipino Braille One-Cell Contractions Recognition Using Machine Vision,"Braille is one of the major tools for teaching the visually impaired. Sufficient number of teachers engaged in special education involved in Filipino Braille is not available. One of the possible approaches to address this problem is the use of computers in automation of extracting information in Braille that can facilitate teaching. Other countries have taken their initiative to develop similar technology capable of teaching Braille however the Filipino Braille code including its contractions, and the Filipino language per se has features that are distinct to other languages. This research proposes a system that use machine vision in recognizing one-cell Filipino Braille contractions. Scanned Braille images undergo image processing and HOG feature extraction to train the system classifier thru SVM. Performance evaluation results reflect a high accuracy of recognition."
9248275,Research on Target 3D Reconstruction and Measurement Technology based on Binocular Vision and Lidar,"At present, the research on target 3D reconstruction and pose measurement is hot issue in the field of computer vision. This paper proposes a method of target 3D reconstruction and pose measurement. It is considered that the 3D information of scene can be recovered by using the visible binocular camera according to the geometric relationship of corresponding points between images. The amount of information is large and the details are rich. However, due to the image as the input data, it brings sensitivity to the change of environmental light Sense, the weak texture region is difficult to match. Lidar can directly measure the three-dimensional information of the scene by emitting laser, which has the advantages of accurate measurement and little influence by external environment changes, but it has the disadvantages of sparse data and low scanning frequency. Based on the characteristics of binocular camera and lidar, aiming at the 3D reconstruction and pose measurement of the target, this paper studies the 3D structure restoration method of the spatial target based on lidar and visible light, and studies the pose measurement method by identifying the typical features of the target, so as to provide a solution for the 3D reconstruction and measurement of the object."
9630492,A Novel Computer Vision Approach to Kinematic Analysis of Handwriting with Implications for Assessing Neurodegenerative Diseases,"Fine motor movement is a demonstrated biomarker for many health conditions that are especially difficult to diagnose early and require sensitivity to change in order to monitor over time. This is particularly relevant for neurodegenerative diseases (NDs), including Parkinson’s Disease (PD) and Alzheimer’s Disease (AD), which are associated with early changes in handwriting and fine motor skills. Kinematic analysis of handwriting is an emerging method for assessing fine motor movement ability, with data typically collected by digitizing tablets; however, these are often expensive, unfamiliar to patients, and are limited in the scope of collectible data. In this paper, we present a vision-based system for the capture and analysis of handwriting kinematics using a commodity camera and RGB video. We achieve writing position estimation within 0.5 mm and speed and acceleration errors of less than 1.1%. We further demonstrate that this data collection process can be part of an ND screening system with a developed ensemble classifier achieving 74% classification accuracy of Parkinson’s Disease patients with vision-based data. Overall, we demonstrate that this approach is an accurate, accessible, and informative alternative to digitizing tablets and with further validation has potential uses in early disease screening and long-term monitoring.Clinical relevance— This work establishes a more accessible alternative to digitizing tablets for extracting handwriting kinematic data through processing of RGB video data captured by commodity cameras, such as those in smartphones, with computer vision and machine learning. The collected data has potential for use in analysis to objectively and quantitatively differentiate between healthy individuals and patients with NDs, including AD and PD, as well as other diseases with biomarkers displayed in fine motor movement. The developed system has potential applications including providing widespread screening systems for NDs in low-income...
(Show More)"
9711204,Generating Masks from Boxes by Mining Spatio-Temporal Consistencies in Videos,"Segmenting objects in videos is a fundamental computer vision task. The current deep learning based paradigm offers a powerful, but data-hungry solution. However, current datasets are limited by the cost and human effort of annotating object masks in videos. This effectively limits the performance and generalization capabilities of existing video segmentation methods. To address this issue, we explore weaker form of bounding box annotations.We introduce a method for generating segmentation masks from per-frame bounding box annotations in videos. To this end, we propose a spatio-temporal aggregation module that effectively mines consistencies in the object and background appearance across multiple frames. We use our predicted accurate masks to train video object segmentation (VOS) networks for the tracking domain, where only manual bounding box annotations are available. The additional data provides substantially better generalization performance, leading to state-of-the-art results on standard tracking benchmarks. The code and models are available at https://github.com/visionml/pytracking."
9441827,Contactless Elevator Based on Hand Gestures During Covid 19 Like Pandemics,"Hand movements in our world are the most popular non-vocal ways of communication that are of great significance. Gesture recognition is an interaction with human computers, commonly used for purposes of education, medicine, and entertainment. Different computer vision related algorithms have used cameras for hand gesture recognition, but sturdy and robust detection of hand gestures is still difficult. We used the CNN model in this paper to define gesture and control devices using particular gestures. Gesture recognition technology will be easy, powerful and secure in this covid-19 pandemic situation."
9339646,Camera Latency Review and Parameters Testing for Real-Time Object Detection Implementation,"In recent years, as computer vision and machine learning technologies continues to improve and become more available to general public, implementation of machine learning and computer vision based object detection also grown in popularity. Even more recently, advancement in computational power had enabled such systems to be implemented to process real time data and output real time decisions, such as surveillance, driving assistance, or pedestrian tracking. Computer vision-based object detection, in general, allows these implementations to be implemented on various kinds of cameras - USB Webcam, IP Camera, etc. While such implementations could do real-time processing and real-time decision making, but computer vision includes capturing process, which sometimes is caused by cameras itself. This paper reviews latencies caused by IP Camera and USB Webcam, by comparing captured stopwatch to real value of the stopwatch, and the data is used to compare those latencies to theoretical standard of real-time, imperceptible latency, to further compiled into a optimized parameters to be used in other implementations. Finally, this paper is used to test whether those latencies could be reduced by optimizing capture parameters so that computer vision implementations are true real-time from capture to output. These optimized settings could yield 176,67 milliseconds latency on USB Camera and 128,4 milliseconds on specific IP Camera."
9099900,Learning Dynamic Textures for Neural Rendering of Human Actors,"Synthesizing realistic videos of humans using neural networks has been a popular alternative to the conventional graphics-based rendering pipeline due to its high efficiency. Existing works typically formulate this as an image-to-image translation problem in 2D screen space, which leads to artifacts such as over-smoothing, missing body parts, and temporal instability of fine-scale detail, such as pose-dependent wrinkles in the clothing. In this article, we propose a novel human video synthesis method that approaches these limiting factors by explicitly disentangling the learning of time-coherent fine-scale details from the embedding of the human in 2D screen space. More specifically, our method relies on the combination of two convolutional neural networks (CNNs). Given the pose information, the first CNN predicts a dynamic texture map that contains time-coherent high-frequency details, and the second CNN conditions the generation of the final video on the temporally coherent output of the first CNN. We demonstrate several applications of our approach, such as human reenactment and novel view synthesis from monocular video, where we show significant improvement over the state of the art both qualitatively and quantitatively."
9064262,Automatic Encoder Combined with Nasnet in Histopathologic Cancer Detection,"Computer vision is a major branch of artificial intelligence algorithm. The algorithm of computer vision mainly consists of the processing of image and video, including image recognition and image detection etc. Practice has proved that computer vision is scientific and practical to a certain extent. In pace with the development of in-depth learning, computer vision has already been put to use well in all walks of life. However, it is still in exploring stage in the medical field, because the medical data is sensitive, which requires high accuracy of the algorithm. In this paper, images of PCam [1], [2] medical electron microscope are put to use for tumor detection, which is an task of image recognition and an automatic encoder is used to lower the dimensions of the data into low-dimensional vectors which are used as features in training. Then the vectors are added as features to the training, and the model is trained together with the original data set as the training features of NASnet. Because detection algorithms in the medical field pay more importance to the true positive rate and false positive rate. When the output is positive, it is necessary to be revalidated by SVM model trained by encoder. As a result, ROC curve is 0.98, which is 0.03 higher than Baseline."
9498540,Automated Assistance for Breast Cancer Identification on Mammograms using Computer Vision Algorithms,"one of the greatest health problems in the world is breast cancer. If these breast cancer abnormalities are identified early, there is a maximum chance of recovery. We can go for this early prediction. It is one of the most effective detection and screening strategies and is widely used. The basic goal of CAD systems is to support physicians in the process of diagnosis. CAD systems, however, are very expensive. Our emphasis is on developing a CAD system that is low-cost and effective. To categorize breast cancer as either benign or malignant, a computer-aided detection approach is suggested. The standard mammogram image corpus, Digital Database used for Screening Mammography, images are used for enhancement, segmented and GLCM, intensity and histogram methods are used to extract features. The work is carried out by effective multilayer perceptron classifier (MLP) and support vector machine (SVM). Compare the performance of the classifiers. The proposed approach achieved 96 % accuracy and 8% improvement in accuracy compared to previous approaches with same dataset [4]."
9200139,Object Detection and Tracking Turret based on Cascade Classifiers and Single Shot Detectors,"The involvement of embedded systems and computer vision is increasing day by day in various segments of consumer market like industrial automation, traffic monitoring, medical imaging, modern appliance market, augmented reality systems, etc. These technologies are bound to make new developments in the domain of commercial and home security surveillance. Our project aims to make contributions to the domain of video surveillance by making use of embedded computer vision systems. Our implementation, built around the Raspberry Pi 4 SBC aims to utilize computer vision techniques like motion detection, face recognition, object detection, etc to segment the region of interest from the captured video footage. This technique is superior as compared to traditional surveillance systems as it requires minimum human interaction and intervention at the control room of such security systems. The proposed system is capable of sensing suspicious events like detection of an unknown face in the captured video or motion detection/object detection in a closed section of a building. Moreover, with the help of the turret mechanism built using servo motors, the camera integrated in the system is capable of having 360° rotation and can track a detected face or object of interest within its range. Apart from automated tracking, the system can also be manually controlled by the operator."
9141068,Analysis of video surveillance images using computer vision in a controlled security environment,"Facial analysis using video camera images proves to be a useful tool in the field of both personal and industrial security. By means of the facial analysis it is possible to develop applications based on the use of algorithms developed in Python programming languages and even with the support of Haar Cascade from the OpenCV library it is possible to solve problems caused by unexpected factors when analyzing an image. These factors can greatly influence image quality, whether it be poor lighting, shadows or even hardware problems. This document details the development of a facial recognition application for images from surveillance cameras, with the aim of providing security in a controlled environment."
9209751,Computer Vision-Assisted Instant Alerts in 5G,"This paper introduces an innovative model which incorporates vehicle On-Board Unit (OBU) data and roadside video information to provide instant alert messages to drivers. We apply computer vision techniques to perform real-time danger event detection and to identify specific surrounding vehicles that should be alerted. Different from traditional broadcast-based alerting, we propose to send these instant alert messages to the target vehicles by unicast and geocast. To do so, an accurate method is required to analyze the spatial relation of vehicles. Also, to confine our alert messages to only those target vehicles, we rely on roadside cameras and apply a sensor fusion technique that can link a video object with its communication MAC address. Through this innovative idea, we integrate computer vision with 5G networks and enable transmitting instant alerts to precise vehicles without interfering irrelevant vehicles. How to incorporate our system with 3GPP V2X by setting proper transmission parameters is also addressed. To validate our idea, we present four common road danger events and show how our model works. To the best of our knowledge, this is the first work bringing computer vision to instant messaging."
9396953,Vision System Actualization for Object Localization Robot,"The accuracy of object localization by an autonomous robotic system depends on the efficiency of the vision system. The key issue for vision system is proper segmentation and detection of an object under noise and low light. A number of image enhancement algorithms have been proposed so far such as adaptive histogram equalization (AHE) and contrast limit adaptive histogram equalization (CLAHE) to combat the low light and high noise in the captured image. However, both of these algorithms have some advantages and disadvantages based on image quality. Therefore to incorporate the advantages of both algorithms, we design a vision system actualization (VSA) system based on automatic image tuning (AIT) system that selects the suitable algorithm for image enhancement on the basis of image noise variance and light intensity. The VSA system is implemented on a 3-DOF robotic system using embedded control system (ECS). The proposed method shows optimum results up to 92 percent and minimizes the processing times up to 0.005 seconds for the VSA system. In addition, we developed a human-robot interface (HRI) to control and optimize the vision performance."
9305715,"Applying Deep-Learning-Based Computer Vision to Wireless Communications: Methodologies, Opportunities, and Challenges","Deep learning (DL) has seen great success in the computer vision (CV) field, and related techniques have been used in security, healthcare, remote sensing, and many other areas. As a parallel development, visual data has become universal in daily life, easily generated by ubiquitous low-cost cameras. Therefore, exploring DL-based CV may yield useful information about objects, such as their number, locations, distribution, motion, etc. Intuitively, DL-based CV can also facilitate and improve the designs of wireless communications, especially in dynamic network scenarios. However, so far, such work is rare in the literature. The primary purpose of this article, then, is to introduce ideas about applying DL-based CV in wireless communications to bring some novel degrees of freedom to both theoretical research and engineering applications. To illustrate how DL-based CV can be applied in wireless communications, an example of using a DL-based CV with a millimeter-wave (mmWave) system is given to realize optimal mmWave multiple-input and multiple-output (MIMO) beamforming in mobile scenarios. In this example, we propose a framework to predict future beam indices from previously observed beam indices and images of street views using ResNet, 3-dimensional ResNext, and a long short-term memory network. The experimental results show that our frameworks achieve much higher accuracy than the baseline method, and that visual data can significantly improve the performance of the MIMO beamforming system. Finally, we discuss the opportunities and challenges of applying DL-based CV in wireless communications."
8862109,Influence of Computer Vision and IoT for Pipeline Inspection-A Review,"In recent years transmission is becoming one of the demanding ways of mobility all over the world. There are various pipeline systems built to carry water, gas and sewage water to reach out every nook and corner of the state. Unfortunately most of these resources are lost during the transmission only, due to the damages found in the pipelines. The advent of Computer Vision and Internet of Things (IoT) over the years has increased the scope of automation in every field. Being influenced by that, the existing inspection systems are getting smarter day by day. This paper gives an overall view about the existing techniques used in identification of the defects occurring in the pipelines. It discusses about the existing image processing techniques used to detect the defects present in the pipelines as quoted from various papers. It also briefs about the various sensors that are being used in the current scenarios for the continuous monitoring of the pipelines thus describing its pros and cons. Finally, the limitations of the existing methods and the scope of research in this domain have been outlined."
9211972,A template-free object motion estimation method for industrial vision system in aligning machine,"With the popularity of varied sensors as well as the development of communication technologies. Industrial vision systems have been deployed in many manufacture applications. Particularly, industrial vision systems coped with air nozzle are often adopted in aligning machines for object aligning. Since alignment efficiency depends on propriety of blown timing/pressure which is casually manifested in the motion of the observed blown objects, motion estimation method is critical to adopt. Unlike conventional methods typically estimate objects’ motions using prior prepared templates, this paper proposed a template-free object motion estimation method for industrial vision system in aligning machine. By virtue of the properties of industrial images, an observation area with bounding box are initialized for each object, then motion estimation is achieved by updating them following expectation-maximization principle. Experiments revealed that the proposed method is able to achieve continuous motion estimation in less processing time."
9141787,Effects of Faster Region-based Convolutional Neural Network on the Detection Efficiency of Rail Defects under Machine Vision,"To optimize the low efficiency of rail defect detection in China, a detection algorithm of Faster Region-based Convolutional Neural Network (Faster R-CNN) under machine vision is proposed in this research. First, the network structure of Faster R-CNN detection algorithm under machine vision, the realization of this algorithm, and the loss function and training steps are introduced. Then the structure of the proposed algorithm is studied. Finally, the network results of rail defect recognition and positioning and detection efficiency results of the algorithm are analyzed. The results show that the Faster R-CNN detection algorithm proposed in this research not only improves the detection accuracy, but also greatly optimizes the detection speed. In addition, the Faster R-CNN algorithm has a high accuracy rate in defect detection under machine vision. Accordingly, the results of this study are of great significance for realizing automatic surface defect detection of railway track in China."
9337602,.Product Quality Detection and Recognition based on Vision and Deep Learning,"In order to address the defective surface of products in traditional manufacturing, the detection efficiency and accuracy of defects should be improved to sort out products effectively. The combination of in-depth learning and machine vision technology can realize accurate detection of product quality. In the first place product data and images that were collected through machine vision and the illuminating system were uploaded to the cloud. And then, by analyzing such data and images, a convolutional neural network model was built and used to train and divide test sets through in-depth learning. At last, then this model was applied to check sample products for defective surfaces and classify types of defects with high accuracy and efficiency. Machine vision and cloud can collect and analyze quality data, while in-depth learning improves detection accuracy."
9209700,Toward Mobile 3D Vision,"In the past few years, the computer vision community has developed numerous novel technologies of 3D vision (e.g., 3D object detection and classification and 3D scene segmentation). In this work, we explore the opportunities brought by these innovations for enabling real-time 3D vision on mobile devices. Mobile 3D vision finds various use cases for emerging applications such as autonomous driving, drone navigation, and augmented reality (AR). The key differences between 3D vision and 2D vision mainly stem from the input data format (i.e., point clouds or 3D meshes vs. 2D images). Hence, the key challenge of 3D vision is that it is could be more computation intensive and memory hungry than 2D vision, due to the additional dimension of input data. For example, our preliminary measurement study of several state-of-the-art machine learning models for 3D vision shows that none of them can execute faster than one frame per second on smartphones. Motivated by these challenges, we present in this position paper a research agenda on offering systems support for real-time mobile 3D vision, focusing on improving its computation efficiency and memory utilization."
8984462,An IoT Reconfigurable SoC Platform for Computer Vision Applications,"The field of Internet of Things (IoT) and smart sensors has expanded rapidly in various fields of research and industrial applications. The area of IoT robotics has become a critical component in the evolution of Industry 4.0 standard. In this paper, we developed an IoT based reconfigurable System on Chip (SoC) robot that is fast and efficient for computer vision applications. It can be deployed in other IoT robotics applications and achieve its intended function. A Terasic Hexapod Spider Robot (TSR) was used with its DE0-Nano SoC board to implement our IoT robotics system. The TSR was designed to provide a competent computer vision application to recognize different shapes using a machine learning classifier. The data processing for image detection was divided into two parts, the first part involves hardware implementation on the SoC board and to provide real-time interaction of the robot with the surrounding environment. The second part of implementation is based on the cloud processing technique, where further data analysis was performed. The image detection algorithm for the computer vision component was tested and successfully implemented to recognize shapes. The TSR moves or reacts based on the detected image. The Field Programmable Gate Array (FPGA) part is programmed to handle the movement of the robot and the Hard Processor System (HPS) handles the shape recognition, Wi-Fi connectivity, and Bluetooth communication. This design is implemented, tested and can be used in real-time applications in harsh environments where movements of other robots are restricted."
9706887,InfographicVQA,"Infographics communicate information using a combination of textual, graphical and visual elements. This work explores the automatic understanding of infographic images by using a Visual Question Answering technique. To this end, we present InfographicVQA, a new dataset comprising a diverse collection of infographics and question-answer annotations. The questions require methods that jointly reason over the document layout, textual content, graphical elements, and data visualizations. We curate the dataset with an emphasis on questions that require elementary reasoning and basic arithmetic skills. For VQA on the dataset, we evaluate two Transformer-based strong baselines. Both the baselines yield unsatisfactory results compared to near perfect human performance on the dataset. The results suggest that VQA on infographics—images that are designed to communicate information quickly and clearly to human brain—is ideal for benchmarking machine understanding of complex document images. The dataset is available for download at docvqa.org"
9389466,Crack Detection and Localization with Real-Time Path Tracking using Stereo Vision for Autonomous Underwater Welding,"Oil and gas organizations across the globe rely heavily on underwater pipelines and water transportation systems. Due to exposure of pipe and ship surfaces to hostile environments like turbulence, water currents, and underwater vegetation, it is not uncommon to observe cracks on exposed material. Thus, regular inspection and maintenance are imperative. Such tasks need to be extremely precise which render the task to be time-consuming and the hostility of the environment renders the task to be dangerous and expensive. This calls for a robotic solution to the problem. In this paper, we propose a 7 DOF autonomous underwater welding arm along with autonomous crack detection, localization, and welding of the crack. We propose a real-time crack identification and segmentation technique using computer vision algorithms to facilitate real-time path generation for the arm, trajectory following, and motion planning. This methodology has been tested by mounting the designed arm on an AUV in the UnderWater Simulator and moving it using the MoveIt! package. The paper also implements a closed-loop control mechanism with an adaptive motion planner using pose tracking to overcome external disturbances during welding operations. The results indicate that the path tracking problem can be solved using computer vision algorithms in the absence of dynamic models."
9538440,A Computer Vision System for Power Transmission Line Inspection Robot,"Periodic inspections of power grids to prevent power outages and promptly handle potential risks are one of the most important tasks of the electricity industry. However, this is a tedious, time-consuming and dangerous job as all current inspection methods are carried out manually. This paper proposes a computer vision system that assists inspection robots working on overhead high voltage transmission lines to operate autonomously. Our system performs three main functions. The first is to detect obstacles by using YOLOv4, a state-of-the-art object detection technique so that the robot can determine how to properly overcome obstacles. The second is to estimate the distance to the obstacle by using linear regression technique so that the robot can determine the exact time to overcome the object. The third is to detect wire defects based on the wire edges by using image processing techniques. Our achieved performance of the system: detecting obstacles with mAP@0.5 equal to 98.65%, estimating distance to objects with average mean absolute error equal to 0.81cm in the range from 20cm to 100cm, and detecting wire defects with precision equal to 90.24% and recall equal to 86.05%. Our computer vision system is accurate and reliable, ready to integrate with the robot in real life. Inspection robots with this system will make the inspection of power lines faster and simpler, which saves time, maintenance costs and labor."
8964744,"Design of a Voice Control 6DoF Grasping Robotic arm Based on Ultrasonic Sensor, Computer Vision and Alexa Voice Assistance","The article presents a study to design a 6-degree of freedom robotic arm which can pick up objects in random positions on a 2D surface based on Arduino microcontroller, ultrasonic sensors and picamera. The robotic arm is able to recognise objects based on computer vision algorithm for shape detection. The ultrasonic sensor measures the distance between the objects and the robotic arm, and the position of the objects in the real world will be detected by its mass centre in the image to improve the accuracy of the pick-up movement. Arduino microcontroller will calculate the rotation angles for the joints of the robotic arm by using inverse kinematics algorithms. The movement of the robotic arm also can be controlled by an Amazon Alexa voice assistance device. The experiment of applying the artificial neural network to control the robotic arm pick-up movement is achieved. The artificial neural network can manipulate the position of the robotic arm to pick up objects after training using the values which are calculated by inverse kinematics equations. The Raspberry Pi is used for processing the computer vision data, and voice commands from Alexa Voice Service based on cloud service."
8451919,The ParallelEye Dataset: A Large Collection of Virtual Images for Traffic Vision Research,"Dataset plays an essential role in the training and testing of traffic vision algorithms. However, the collection and annotation of images from the real world is time-consuming, labor-intensive, and error-prone. Therefore, more and more researchers have begun to explore the virtual dataset, to overcome the disadvantages of real datasets. In this paper, we propose a systematic method to construct large-scale artificial scenes and collect a new virtual dataset (named “ParallelEye”) for the traffic vision research. The Unity3D rendering software is used to simulate environmental changes in the artificial scenes and generate ground-truth labels automatically, including semantic/instance segmentation, object bounding boxes, and so on. In addition, we utilize ParallelEye in combination with real datasets to conduct experiments. The experimental results show the inclusion of virtual data helps to enhance the per-class accuracy in object detection and semantic segmentation. Meanwhile, it is also illustrated that the virtual data with controllable imaging conditions can be used to design evaluation experiments flexibly."
9059031,Long-Range Binocular Vision Target Geolocation Using Handheld Electronic Devices in Outdoor Environment,"Binocular vision is a passive method of simulating the human visual principle to perceive the distance to a target. Traditional binocular vision applied to target localization is usually suitable for short-range area and indoor environment. This paper presents a novel vision-based geolocation method for long-range targets in outdoor environment, using handheld electronic devices such as smart phones and tablets. This method solves the problems in long-range localization and determining geographic coordinates of the targets in outdoor environment. It is noted that these sensors necessary for binocular vision geolocation such as the camera, GPS, and inertial measurement unit (IMU), are intergrated in these handheld electronic devices. This method, employing binocular localization model and coordinate transformations, is provided for these handheld electronic devices to obtain the GPS coordinates of the targets. Finally, two types of handheld electronic devices are used to conduct the experiments for targets in long range up to 500m. The experimental results show that this method yields the target geolocation accuracy along horizontal direction with nearly 20m, achieving comparable or even better performance than monocular vision methods."
9418255,A Survey on Human Activity Recognition using Sensors and Deep Learning Methods,"One of the most difficult challenges in the field of computer vision is human activity recognition (HAR). The main purpose of intelligent video system is to determine the actions and activities of the individual. This action monitoring system can be used in a variety of settings, which include human-computer interaction, tracking, security, and health monitoring. Detecting activity in an uncircumscribed territory remains a challenging task with multiple challenges, despite ongoing efforts in this area. Throughout this article, some recent research papers that include different approaches are analyzed to detect different human activities. Wearable devices and phone sensors are required for this task. The vision-based approach has become a prevalent HAR technique, according to the researchers."
8759189,A Semi-Supervised CNN Learning Method with Pseudo-class Labels for Atherosclerotic Vascular Calcification Detection,"The recent rapid success of deep convolutional neural networks (CNN) on many computer vision tasks largely benefits from the well-annotated Pascal VOC, ImageNet, and MS COCO datasets. However, it is challenging to get ImageNet-like annotations (1000 classes) in the medical imaging domain due to the lack of clinical training in the lay crowdsourcing community. We address this problem by presenting a semi-supervised training method for neural networks with true-class and pseudo-class (un-annotated class) labels on partially annotated training data. The true-class labels are supervised annotations from clinical professionals. The pseudo-class labels are unsupervised clustering of un-annotated data. Our method rests upon the hypothesis of better coherent annotations with discriminative classes leading to better trained CNN models. We validated our method on extra-coronary calcification detection in low dose CT scans. The CNN trained with true-class and 10 pseudo-classes achieved an 85.0% sensitivity at 10 false positives per patient (0.3 false positive per slice), which significantly outperformed the CNN trained with true-class only (sensitivity =56.0% at 10 false positives per patient)."
9035162,Multi-column Deep Neural Network Based On Particle Swarm Optimization,"Many architectures for computer vision use of Convolutional Neural Networks (CNN) are biologically inspired by the receptive fields in the visual cortex. A modified deep architecture Thresholding Convolution Neural Network (ThCNN) progresses in this paper. This method is simple and effective to; regularize features map in the early layers of Convolution Neural Network; Compute the optimal global threshold to determine the features that are passed to the next layers; improve the speed performance of the suggested structure. The present paper proposed that each feature map in the convolution layer is connected to half feature maps in the previous layer. Another tendency in current applications is to combine the results of multiple networks, the particle swarm optimization method (PSO) is used to determine the influence of each network in the combining model. The proposed method was evaluated using the MNIST datasets. The experimental results showed that the error rate has decreased (up to 26.98 percent). Furthermore, combining model has decreased the number of the networks with the percent of 20%, and decreased error rate to 78.26 % of error rate of the best single model and to 88.89% of error of the averaging model."
9256252,A Vision-Based Path Planning and Object Tracking Framework for 6-DOF Robotic Manipulator,"Industrial robots are widely used for repetitive, humanly unmanageable, and hazardous tasks. Hence, an improvement in the production efficiency of industrial robot manipulators is of prime concern. This can be achieved through machine vision and path planning techniques with a focus on localization and shortest path calculation. In particular, this is important for manufacturing and bottle filling industries which extensively use robotic manipulators to place/displace bottles during production and post refill placements. This is even more challenging when soft, fragile, or opaque objects have to be detected, since it is significantly difficult for robot vision to focus on their indistinguishable features. To this end, we present an ensemble robot framework with a stereo vision system for tracking colored objects which are sensed using blob analysis. An ensemble robotic framework with neural networks is proposed for predicting and thereby overcoming the inbuilt geometric error present in stereo vision systems. Moreover, we have simplified 2-D correspondence problem to 1-D by using a non-rectified stereo camera model and object tracking by applying the triangulation technique in 3D stereo vision coordinate system (SVCS). Subsequently, the SVCS is transformed into robot stereo vision coordinate system for tracking the object centroid by using an RGB marker placed on the object. Finally, in the learning model we have combined color region tracking with machine learning to achieve high accuracy. The outcomes are in accordance with the designed model and successfully achieve path prediction with up to 91.8% accuracy."
9395822,Lesion Extraction of Endometriotic images using Open Computer Vision,"The Endometriosis is caused by the endometriotic tissue that streaks the uterus from outside. A total of 6% of women under child bearing age is affected by the endometriosis disorder. The presence of the tissue can be identified though MRI, Transvaginal Ultra Sound Scan (TVUS), Laparoscopic images and pathological slides. Image based applications are processed using Image Pre-processing, Image Segmentation, Image Enhancement and Feature Extraction techniques. The Open Computer Vision (Open CV) is used to extract the features from Laparoscopic Endometriosis Images (LPI). This paper proposed a method to identify the endometriotic tissue by using the LPI images. The proposed method uses several image processing techniques of OpenCV includes Adaptive Threshold, Contour Mask to extract the lesion area from the Laparoscopic images. The designed algorithm has been experimented and the results yielded 53.5 % mean value intensity in identifying the lesion area."
9563427,Fruit Quality Analysis using modern Computer Vision Methodologies,"The field of agriculture is one of the most profitable fields for a country. The produce of this industry i.e., fruits and vegetables, is tremendous and thus the quality insurance of these products are of utmost importance. Evaluation of fruits can be done manually but due to inconsistent results and huge time consumption, it is necessary to have the automated systems to perform the quality tests. In this study, computer vision has been used to build an architecture that is competent to detect whether the fruit is rotten or fresh. VGG16 CNN (Convolutional Neural Network) model is employed to extract the features from the images of Apples, Bananas, Guava and Oranges. With the help of extracted features, the classification is performed through Decision Tree, Support Vector Machines (SVM), and Logistic regression models. Support Vector Machine performed the best classification with an accuracy of 99%"
9039280,Intelligent Navigation System Based on Computer Vision Optical System,A research project on the development of an intelligent autonomous navigation system is considered. The system is implemented on the basis of an artificial neural network. The system makes a decision by analyzing the data of the optical system of computer vision.
9256697,Towards Robust Certification of Computer-Vision-Based Detect and Avoid in UAS,"Computer-vision-based Detect and Avoid (DAA) promises to enable large-scale unmanned BVLoS drone flights in the near future, which would have enormous economic benefits. However, quite a bit of work remains to be done before it can be certified. We describe AirMap's ongoing research into computer-vision-based DAA."
9723748,System design of picking robot based on computer vision,"Picking robots are widely used in the harvesting of various agricultural products, which can significantly reduce the labor cost in agricultural production and solve the problem of labor shortage in agricultural production. Computer vision technology can identify ripe melons and fruits from their natural growth environment, which is the key for picking robot to achieve target positioning and coordinate manipulator. In this paper, the method of designing robot picking experiment platform based on computer vision is proposed, and intelligent picking is realized by constructing robot picking experiment platform. The experimental results show that the robot picking experimental platform based on computer vision can solve the problem of low picking efficiency of the previous platform, and can realize the efficient picking of different fruits and melons, which provides a research basis for the picking robot to better meet the actual demand of agricultural production."
9108000,A Machine Vision Framework for Autonomous Inspection of Drilled Holes in CFRP Panels,"This paper presents a fully autonomous framework for the inspection of drilled holes in planar carbon fiber composite panels used in the aerospace and automotive industries. The proposed framework can automatically recognize a part and extract the geometrical information from an existing library of DXF files. It then determines the location and orientation of the part with respect to the motion platform without a need for explicit programming of the part coordinate system. Visual servoing and optimal motion planning techniques are used to autonomously move the end-effector's camera to each hole to capture high resolution images. Image processing techniques are used to determine the geometrical errors and delamination factors for each hole. All of the proposed computer vision modules have been implemented in Python and OpenCV, which are open source and thus readily available to the industry. Experimental results prove that the proposed framework can efficiently and autonomously inspect drilled holes in composite panels with minimal programming required of the end-user."
9194647,Single Object Trackers in OpenCV: A Benchmark,"Object tracking is one of the fundamental tasks in computer vision. It is used almost everywhere: human-computer interaction, video surveillance, medical treatments, robotics, smart cars, etc. Many object tracking methods have been published in recent scientific publications. However, many questions still remain unanswered, such as, which object tracking method to choose for a particular application considering some specific characteristics of video content or which method will perform the best (quality-wise) and which one will have the best performance? In this paper, we provide some insights into how to choose an object tracking method from the widespread OpenCV library. We provide benchmarking results on the OTB-100 dataset by evaluating the eight trackers from the OpenCV library. We use two evaluation methods to evaluate the robustness of each algorithm: OPE and SRE combined with Precision and Success Plot."
8967025,Accurate Cost Estimation of Memory Systems Utilizing Machine Learning and Solutions from Computer Vision for Design Automation,"Hardware/software co-designs are usually defined at high levels of abstractions at the beginning of the design process in order to provide a variety of options on how to realize a system. This allows for design exploration which relies on knowing the costs of different design configurations (with respect to hardware usage and firmware metrics). To this end, methods for cost estimation are frequently applied in industrial practice. However, currently used methods oversimplify the problem and ignore important features, leading to estimates which are far off from real values. In this article, we address this problem for memory systems. To this end, we borrow and re-adapt solutions based on Machine Learning (ML) which have been found suitable for problems from the domain of Computer Vision (CV). Based on that, an approach is proposed which outperforms existing methods for cost estimation. Experimental evaluations within an industrial context show that, while the accuracy of the state-of-the-art approach is frequently off by more than 20 percent for area estimation and more than 15 percent for firmware estimation, the method proposed in this article comes rather close to the actual values (just 5-7 percent off for both area and firmware). Furthermore, our approach outperforms existing methods for scalability, generalization, and decrease in manual effort."
8905144,A Review of Stereo-Photogrammetry Method for 3-D Reconstruction in Computer Vision,"3-D reconstruction is one big concern since the first days of computer vision (CV) development. By going through the history of 3-D reconstruction, we show the contribution of this field into society and the necessity of improving the system's complexity, speed and accuracy. Although various algorithms and methods have been introduced, stereo-photogrammetry that applies the binocular vision principle is the closest to human vision system. Originated from the motivation of improving the stereo-photogrammetry and applying for robot manipulation, the method is reviewed with all three known-to-date categorizes local, global and semi-global. In addition, it is advisable to rectify the stereo images before matching to find depth and reconstruct 3-D models. Therefore, some of the rectification methods are demonstrated. Finally, we discussed our findings and future work relates to stereo-photogrammetry."
8993075,Vision-Based Trainable Robotic Arm for Individuals with Motor Disability,"This paper presents a trainable robotic arm with vision-based guidance capability. The arm can be trained to perform object manipulation tasks such as picking up an object from a location. The vision-based guidance, that utilizes a low-cost integrated webcam, augments the trainable arm's capability to pick up objects; it assists in situations in which the object to be manipulated is displaced from the location the arm was originally trained to pick up. The experimental results from 10 trails demonstrate the vision-based trainable arm's potential to be utilized as a robotic assistant for individuals with physical functioning difficulties."
9165205,A Stream Algebra for Performance Optimization of Large Scale Computer Vision Pipelines,"There is a large growth in hardware and software systems capable of producing vast amounts of image and video data. These systems are rich sources of continuous image and video streams. This motivates researchers to build scalable computer vision systems that utilize data-streaming concepts for processing of visual data streams. However, several challenges exist in building large-scale computer vision systems. For example, computer vision algorithms have different accuracy and speed profiles depending on the content, type, and speed of incoming data. Also, it is not clear how to adaptively tune these algorithms in large-scale systems. These challenges exist because we lack formal frameworks for building and optimizing large-scale visual processing. This paper presents formal methods and algorithms that aim to overcome these challenges and improve building and optimizing large-scale computer vision systems. We describe a formal algebra framework for the mathematical description of computer vision pipelines for processing image and video streams. The algebra naturally describes feedback control and provides a formal and abstract method for optimizing computer vision pipelines. We then show that a general optimizer can be used with the feedback-control mechanisms of our stream algebra to provide a common online parameter optimization method for computer vision pipelines."
9429420,Deep Pre-trained Models for Computer Vision Applications: Traffic sign recognition,"Objects detection and Recognition are an important task for computer vision field and intelligent transportation systems. Generally, these tasks remain challenging for the artificial machines due to the need of pre-learning phase in which the machine acquires an intelligent brain. Some researchers have shown that deep learning tools work well in computer vision, image processing, and pattern recognition. To solve such tasks, this paper focuses on deep Convolutional Neural Network (CNN) and its architectures, such as, VGG16, VGG19, AlexNet, and Resnet50. An overview for the techniques and schemes used for computer vision applications such as Road Sign Recognition will be introduced. Then by customizing the hyperparameters for each pre-trained models, we re-implement these models for the traffic sign recognition application. In the experiments, these pre-trained CNN classifiers are trained and tested with the German Traffic Sign Recognition Benchmark dataset (GTSRB). Experimental results show that the proposed scheme achieved a good performance results in terms of evaluations metrics of traffic signs recognition. A performance comparison analysis between the selected pre-trained models for traffic sign recognition confirmed that the AlexNet model outperforms all other implemented models."
9578372,Understanding and Simplifying Perceptual Distances,"Perceptual metrics based on features of deep Convolutional Neural Networks (CNNs) have shown remarkable success when used as loss functions in a range of computer vision problems and significantly outperform classical losses such as L1 or L2 in pixel space. The source of this success remains somewhat mysterious, especially since a good loss does not require a particular CNN architecture nor a particular training method. In this paper we show that similar success can be achieved even with losses based on features of a deep CNN with random filters. We use the tool of infinite CNNs to derive an analytical form for perceptual similarity in such CNNs, and prove that the perceptual distance between two images is equivalent to the maximum mean discrepancy (MMD) distance between local distributions of small patches in the two images. We use this equivalence to propose a simple metric for comparing two images which directly computes the MMD between local distributions of patches in the two images. Our proposed metric is simple to understand, requires no deep networks, and gives comparable performance to perceptual metrics in a range of computer vision tasks."
9578772,Natural Adversarial Examples,"We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets’ real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called IMAGENET-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called IMAGENET-O, which is the first out-of-distribution detection dataset created for ImageNet models. On IMAGENET-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on IMAGENET-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models."
9444889,Remote Attacks on Drones Vision Sensors: An Empirical Study,"Vision systems applied to drones, automatic vehicles, and robots have become an increasingly popular sensing method. However, vision sensors that make up these systems are vulnerable to malicious input attacks, which can lead to serious consequences. Prior work on attacking cameras of automatic vehicles shows that lasers can cause failure of camera-based functionalities, but it lacks analysis of the results and does not conduct experiments in the actual scenarios.In this paper, a laser-based attack on cameras and binocular vision sensors of drones is presented. First, we propose a threat model that describes how an adversary attacks the drone then perform feasibility analysis of the attack from theory and practice. Next, we design multi-variable experiments in the lab to systematically study the effectiveness of the attack, and further analyze how each variable affects the results. To get intuitive and fine-grained results, multidimensional image similarity is used to measure the effects. In particular, experiments in the actual scenarios are carried out, and results show that the attack can make obstacle avoidance, target recognition and tracking completely failed. Finally, lightweight countermeasures based on hardware and software are proposed to improve sensor resilience against the attack."
9697152,Computer Vision for Monitor and Control of Vertical Farms Using Machine Learning Methods,"Vertical farms have become increasingly popular in today’s society. Its popularity owes to the increasing relevance of vertical farms as a panacea for the effect of desertification and urbanization as proposed by experts and researchers. Urbanization is increasing annually, as at the middle of year 2020 according to statistica.com the rate of urbanization was at 56 percent, and it has been estimated that by the year 2050 about 70-90% of global population will live in cities. The tripod effect of urbanization, desertification and climate change has made vertical farms a convenient alternative. While vertical farm is helping to solve this problem, the interface of computers with vertical farms has exponentially increased the efficiency, it has helped to create convenient environments which can be controlled, as such facilitating and all-around production of food crops all through the year despite changing environmental conditions. In this paper, we are taking a step further to see how computer vision can help in this process. So far IoT has been used to monitor the farm extrinsic factors, and get relevant data, the problem with that method is that only the external factors are being monitored, in this paper we will be exploring how computer vision can monitor intrinsic factors, but beyond that, we will also explore how computer vision and machine learning methods could be used together with IoT for the control of vertical farms as well to create favorable conditions for the planting of vertical farms."
9626736,Vision-based fall detection and prevention for the elderly people: A review & ongoing research,"Falls are the most leading cause of accidental injury deaths worldwide, therefore, it poses a real challenge for the prevention of life-threatening conditions in geriatrics. The most damaged community is the ever-growing aging population. For this reason, there are considerable demands to distinguish a dangerous posture such as fall in real-time. Here we provide a literature review of conducted work on elderly fall detection and prediction mentioning the main methods to recognize human posture including computer vision-based and wearable sensor-based. We approached these perspectives: sensor fusion, Datasets, approaches proposition. In conclusion, our survey summarizes the progress achieved in the five past years to help the researchers in this field to spot areas where further effort would be beneficial and innovative."
9569217,Evaluating the Effect of Poor Contrast Ratio in Simulated Sensor-Based Vision Systems on Performance,"Sensor-based vision systems enable approaches to altitudes closer to the runway in low visibility conditions. These systems utilize imaging sensors capable of providing forward vision of the runway and the resulting imagery is displayed on a head-up display. Previous studies were primarily limited to nominal cases. Off-nominal cases assessments are limited only to display failures. The sensors integrated to these systems have known limitations and can produce degraded display output with respect to atmospheric conditions. An evaluation is conducted to assess the potential human factors implications of a simulated display of poor sensor output, operationalized as a difference in contrast ratio of the display output for simulated approach and landing operations. Pilots fly six different approach and landing scenarios in a simulator for two visibility levels and three levels of display information quality (none, poor, and good display output). Measures of performance include approach and landing performance, attention allocation, workload, and decision-making. The experiment results indicate that landing performance and decision making are negatively impacted by poor display output, while there is no evidence of an impact on workload or situation awareness."
9609496,Third Vision for Safeguarding Women with A Live Surrounding Update using Deep-Learning for Computer Vision,"The Large deployment of deep learning is safeguarding women all over the world. The main motive is to discover a few specific design necessities for women’s safety. Women’s safety is the most crucial issue in every country. Women are harassed and distressed, and on occasion, whilst pressing assistance is needed, there may be no required place for women so that people can help. It’s vital that everyone is privy to the significance of women’s safety; all need to examine that they ought to be properly protected. The formerly existing methodology enables tracking down the victim’s place after the crime has been committed. Whenever she comes in touch with any individual out of doors, the person’s activities and the phrases they talk about can be monitored constantly. If the person’s feelings vary, ensuing in any dangerous motion, and their speech results are abusive and vulgar, then the system will encounter it and process the captured image. It will forward it to the own circle of relatives that the user has given with GPS place tracked from the IP address in the form of SMS to their phone number and email. Thus the undertaking enables in saving womankind and safeguarding her in the current state of affairs."
9465458,A Multi-View Stereoscopic Video Database With Green Screen (MTF) For Video Transition Quality-of-Experience Assessment,"We introduce a multi-view stereoscopic video database with a green screen, called MTF, for the usages in computer vision applications, in particular for free navigation, free-viewpoint television, and video transition quality-of-experience (QoE) assessment. The MTF contains full-HD videos of real storytelling made up of 3 scenes. One particularity of this dataset is that to understand its storytelling, users must change their point of view in the scene at a given time. To this end, we usually need to generate a transition to link two points of view in the same scene. Computer vision techniques that enable such transitions like view synthesis methods, rely on a set of images of the scene to render some new views from different viewpoints of this scene. However, these methods may have many failure cases that lead to artifacts in the final rendered video transition. In most view synthesis QoE tests, the contents are not designed to make the transition between two points of view useful or interesting for the viewers, e.g. they don't need to make a transition to capture more information to better understand the content. We thus, assume that participants will harshly judge artifacts and imperfections in the rendered transition. Thus, the MTF is expected to enable a better analysis of the visual impact of persistent artifacts in the final rendered transition. In our dataset, all the scenes are recorded in a green screen studio, which is often used to superimpose special effects and scenery during editing according to specific needs. Our dataset also presents a wide baseline camera-setup, a challenging constraint for view synthesis techniques. Finally, The MTF can also be used as a complementary dataset with others in literature in various computer vision applications, such as video compression, 3D video content, immersive virtual reality environment, optical flow estimation..."
9336310,TagAttention: Mobile Object Tracing With Zero Appearance Knowledge by Vision-RFID Fusion,"We propose to study mobile object tracing, which allows a mobile system to report the shape, location, and trajectory of the mobile objects appearing in a video camera and identifies each of them with its cyber-identity (ID), even if the appearances of the objects are not known to the system. Existing tracking methods either cannot match objects with their cyber-IDs or rely on complex vision modules pre-learned from vast and well-annotated datasets including the appearances of the target objects, which may not exist in practice. We design and implement TagAttention, a vision-RFID fusion system that achieves mobile object tracing without the knowledge of the target object appearances and hence can be used in many applications that need to track arbitrary un-registered objects. TagAttention adopts the visual attention mechanism, through which RF signals can direct the visual system to detect and track target objects with unknown appearances. Experiments show TagAttention can actively discover, identify, and track the target objects while matching them with their cyber-IDs by using commercial sensing devices in complex environments with various multipath reflectors. It only requires around one second to detect and localize a new mobile target appearing in the video and keeps tracking it accurately over time."
8966136,Utilization of the open source datasets for semantic segmentation in automotive vision,"Great advancements in deep-learning-based machine vision are boosting the automotive industry potential to accurately recognize vehicle environment. The need for high quality datasets is of critical importance when training neural networks which are used to detect and classify objects in front of the vehicle. Available open source datasets for semantic segmentation can be used by a wide community of researchers to develop next generation self-driving functions. Those datasets have severe limitations such as class imbalance, unobserved objects, erroneous labelling and limited number of covered scenarios. In this paper, we propose a sequence of steps to combine and manipulate existing open source datasets to maximize the inference performance. Those steps include relabeling with outlier removal, class-driven balancing of validation and training datasets, as well as targeted image manipulation for scarce classes. Our evaluation indicates the improved inference accuracy when compared to the usage of most common open source datasets."
9486505,Development of machine vision module for intelligent portable bowling system,"This design analyzes the functional requirements of portable bowling system, and designs the overall framework of machine vision module based on the whole bowling system. The UP Board was chosen as the master chip, and Intel Realsense D435 as the data acquisition device. The HOG (Histogram of Oriented Gradient) feature and Adaboost algorithm were combined to detect the status of the bowling pins. We integrated all algorithms into the program, and completed the software design of the whole module, and debugged and improve the hardware. Finally, the software and hardware of the machine vision module are fully combined, running stably and realizing normal functions, which meets all the requirements of this design."
8834160,Research on veneer correction system based on machine vision,"In the process of laminated veneer lumber, it requires manual adjustment of the position of the veneer to accurately position it, which is inaccurate and inefficient. Aiming at these problems, a veneer correction algorithm on machine vision is designed and applied to the production process of laminated veneer lumber. First, the veneer image is subjected to a preprocessing operation. Second, the canny operator is used to detect edges, and the Hough transform is used to detect veneer straight lines. Finally, the deviation angle and displacement deviation of the veneer are calculated. The experimental results show that the veneer image processing and the correction algorithm on machine vision perform well in measuring the deviation and adjusting the position of the board."
9085190,Design of Ceramic Gasket State Detection System Based on Machine Vision,"Machine vision has been widely used in industrial component production and processing due to its high accuracy, high efficiency and non-contact measurement. The electronic water pump of vehicle includes a gasket, which is sleeved on the rotor between pump body and the cylinder head joint surface and has the characteristics of stamping, sealing, high temperature resistance and so on. Therefore, in the process of pump production, gasket detection is very important link. The traditional manual visual inspection has some shortcomings, such as strong subjective factors, and the component is vulnerable to secondary damage. On this basis, this paper design a ceramic gasket state detection system based on machine vision, including software and hardware platforms, which can detect the state of the current components in real time and feed back to the inspection workers, reducing the subjectivity brought by manual operation, reducing the probability of components damage and improving the production efficiency."
8865698,Extraction of Color Characteristics of Pesticide Detection Card Based on Machine Vision,"In this paper, aiming at the pesticide residue in water resources, considering the necessity of rapid detection on site, a method based on machine vision for pesticide residue detection is proposed. The method combines machine vision and image processing technology to quickly extract the RGB three-component eigenvalues of the pesticide residue detection card. Each RGB channel has 256 brightness levels, which can overcome the shortcomings of low brightness level and indistinct image, and improve the accuracy of detection. The results show that the method is simple, fast and efficient, it is suitable for quickly extracting the characteristic values of pesticide residue detection cards."
9258498,Target height measurement method based on stereo vision,"The key to using binocular stereo vision technology to achieve target height measurement is to obtain accurate predicted box of target. We propose a disparity-method and an edge-method to correct the original predicted box of the target. The main idea of the disparity-method is to use the disparity discontinuity between the target area and the background to find the location of the highest point of the object, so as to correct the upper edge of the original predicted box. The main idea of the edge-method is to find the boundary between the target and the ground based on the edge map, so as to correct the lower side of the original predicted box. It can be seen from the experiment that the corrected predicted box has higher accuracy than the original predicted box obtained by Yolo. Finally, we give a specific method for measuring the height of the object based on binocular stereo vision."
9559816,A Machine Vision Approach for Copper Plating Quality Control System Development,"In a plating process, its quality control process depends on its employees' skills and experience. In long work hours of the quality control: the QC process causes fatigue and the employees' visual problem. Additionally, lacking experience, skills, leaving the job, and even personal issues of the employees who work in the QC process can harm the company's quality control. This research is the area of research development. The authors focus on design and copper plating quality control system development to solve problems and enhance the plating industry's business benefits and reliability. The article presents a machine vision approach for quality control of copper plating. The authors apply the machine vision concept by using and color spectrum and color-matching techniques to solve the copper plating process's quality control problems from human errors. This system simultaneously identifies and classify oxidation defect on copper plating of pole pieces of microwave ovens. In this work, the paper presents 1. oxidation defect problems in a copper plating industry 2. color spectrum and color matching concept, 3. system architecture, 4. the experiment, and the conclusion of from result of the experiment."
9526043,Error Correcting System for Volleyball Smashing Based on Machine Vision,"In order to overcome the problem of low accuracy of motion detection and error correction in traditional volleyball smashing technology action correction system, a volleyball smashing technology action correction system based on machine vision is proposed and designed. The hardware part of the system, designs the motion image detection module of Volleyball Smash technology, completes the image detection with the support of the principle of image depth collection; the hardware part also includes the motion feature recognition module to extract the features of the smash technology action. System software part, based on the convolution neural network in machine vision, extracts the error correction features of volleyball smashing technology action image to complete the error correction of volleyball smashing technology action. The experimental results show that, compared with the traditional error correction system, the designed motion correction system of Volleyball Smash technology has higher accuracy of motion detection and error correction, and higher practical application value."
9527025,Single Image Super-Resolution Algorithm Based on Enhanced Generative Adversarial Network,"With the advancement of technology, the processing of increasing image data becomes particularly important. Computer vision can process the hidden information behind these data. Image super-resolution aims to use a low-resolution image to generate a corresponding high-resolution image through the convolutional neural network, improving the quality of the image and the display effect. This paper proposes an Enhanced Generative Adversarial Network (EGAN), designing a new loss function, so that the generated image has more texture details than the previous method. On three benchmark datasets, Set5, Set14 and BSD100, the proposed method has a great improvement on the peak signal-to-noise ratio and structural similarity of the generated images."
9523966,Nondestructive testing method for surface defects of mechanical parts based on machine vision,"In order to solve the problems of low detection accuracy and efficiency in the traditional surface defect detection methods of mechanical parts, a non-destructive testing method for surface defects of mechanical parts based on machine vision is proposed. This method designs an image acquisition architecture based on machine vision, and sets and selects camera parameters and light sources. According to the above framework, the distortion correction model is constructed by using inverse matrix method to complete the distortion correction processing of mechanical part image. Finally, the visual perception feature extraction of mechanical part image is carried out to complete the nondestructive testing of mechanical part surface defects. The experimental results show that, compared with the traditional detection methods, the proposed detection method has higher detection accuracy and efficiency, and is more practical."
9016057,Detection of Possible Illicit Messages Using Natural Language Processing and Computer Vision on Twitter and Linked Websites,"Human trafficking is a global problem that strips away the dignity of millions of victims. Currently, social networks are used to spread this crime through the online environment by using covert messages that serve to promote these illegal services. In this context, since law enforcement resources are limited, it is vital to automatically detect messages that may be related to this crime and could also serve as clues. In this paper, we identify Twitter messages that could promote these illegal services and exploit minors by using natural language processing. The images and the URLs found in suspicious messages were processed and classified by gender and age group, so it is possible to detect photographs of people under 14 years of age. The method that we used is as follows. First, tweets with hashtags related to minors are mined in real-time. These tweets are preprocessed to eliminate noise and misspelled words, and then the tweets are classified as suspicious or not. Moreover, geometric features of the face and torso are selected using Haar models. By applying Support Vector Machine (SVM) and Convolutional Neural Network (CNN), we are able to recognize gender and age group, taking into account torso information and its proportional relationship with the head, or even when the face details are blurred. As a result, using the SVM model with only torso features has a higher performance than CNN."
9501167,Robust Visual-Lidar Simultaneous Localization and Mapping System for UAV,"Obtaining 3-D data by LIDAR from unmanned aerial vehicles (UAVs) is vital for the field of remote sensing; however, the highly dynamic movement of UAVs and narrow viewpoint of LIDAR pose a great challenge to the self-localization for UAVs based on solely LIDAR sensor. To this end, we propose a robust simultaneous localization and mapping (SLAM) system, which combines the image data obtained by vision sensor and point clouds obtained by LIDAR. In the front-end of the proposed system, the more stable line and plane features are extracted from point clouds through clustering. Then the relative pose between two consecutive frames is computed by the least squares iterative closest point algorithm. Afterward, a novel direct odometry algorithm is developed by combining the image frames and sparse point clouds, where the relative pose is used as a prior. In the back-end, the pose estimation is refined and the 3-D map with texture information is built at a lower frequency. Extensive experiments show that our method can achieve robust and highly precise localization and mapping for UAVs."
9649021,Counting the Number of Moving Vehicles by Its Type Based on Computer Vision,"Traffic jam, lack of adequate information of the traffic low used for development of road infrastructure to reduce traffic jam, and possibility of human error in the execution of a heavy traffic survey become the primary background of this study. By using information technology, open up an opportunity for the development of monitoring system computer vision based. This study will calculate the total amount of moving vehicles based on its type with computer vision based with staged: ROI selection, image segmentation with Gaussian Mixture Model method, filtering process, blob detection and tracking, and vehicles classification with Fuzzy Clustering Means. Implementation of an application using visual studio 2010. The output comprises result of classification and total amount vehicles based on its type. The test application divided into a few test scenarios, namely test 1, test 2, test 3 and test 4. The accuracy obtained on each test are 36.27%, 50.47%, 60.75%, and 67.00% respectively."
9451762,Pothole Detection using Accelerometer and Computer Vision with Automated Complaint Redressal,"Road transport is the most widely used means of transportation around the world. With this high use of road transport, the safety of travellers' becomes the prime concern for any governing authority. While some safety concerns arise from driver errors and environmental factors, most cases are a result of poor maintenance of these roads. Potholes, specifically, are one of the leading causes of road accidents throughout the world and need to be taken care of immediately, by the authorities. This paper presents a solution that makes use of civilians' mobile sensors, along with image-based alternatives to detect potholes in real-time, using Machine Learning. The concerned authorities are then notified about the same through a web-based portal, to take the necessary action. The solution also incorporates pivoting existing complaints, location tagging and prioritization. Additionally, the solution provides a forecast of the likelihood of issues regarding potholes, constantly updating time series data of the locations."
9453978,Computer Vision and NLP based Multimodal Ensemble Attentiveness Detection API for E-Learning,"Attention is the fundamental element of effective learning, memory, and interaction. Learning however, with the evolvement of technologies in the modern digital age, has surpassed traditional learning systems to more convenient online or e-learning systems. Nevertheless, unlike in the traditional learning systems, attention detection of a student in an e-learning environment remains one of the barely explored areas in Human Computer Interaction. This study proposes a multimodal ensemble solution to detect the level of attentiveness of a student in an e-learning environment, with the use of computer vision, natural language processing, and deep learning to overcome the barriers in identifying user attention in e-learning. The proposed multimodal captures, processes, and predicts user attentiveness levels of individual students, which are subsequently aggregated through an ensemble model to derive an overall outcome of better accuracy than individual model outcomes. The final outcome of the ensemble model produces a range of percentages, within which the attentiveness level of the student lies during a single online lesson. This range is consequently delivered to the users through an Application Programming Interface."
9213771,A new 3D Viewer system based on hand gesture recognition for smart interaction,"The visualization of the 3D models is a scorching topic in computer vision and human-computer interaction. The demands for 3D models have been increased due to high involvement in animated characters, virtual reality and augmented reality. To interact with 3D models with the help of mouse and keyboard is a very hectic, less efficient and complex process because of multiple types of operations required by the models to view properly in all sides. So it is essential to improve the user interaction with the 3D system. In this paper, a new method is introduced by using the Microsoft Kinect v2 to detect the human body and joints. First, we trained the Kinect to understand the specific gestures, and then recognize to perform the specific task on an object in the proposed environment."
9406452,Stabilized adaptive sampling control for reliable real-time learning-based surveillance systems,"In modern security systems such as CCTV-based surveillance applications, real-time deep-learning based computer vision algorithms are actively utilized for always-on automated execution. The real-time computer vision system for surveillance applications is highly computation-intensive and exhausts computation resources when it performed on the device with a limited amount of resources. Based on the nature of Internet-of-Things networks, the device is connected to main computing platforms with offloading techniques. In addition, the real-time computer vision system such as the CCTV system with image recognition functionality performs better when arrival images are sampled at a higher rate because it minimizes missing video frame feeds. However, performing it at overwhelmingly high rates exposes the system to the risk of a queue overflow that hampers the reliability of the system. In order to deal with this issue, this paper proposes a novel queue-aware dynamic sampling rate adaptation algorithm that optimizes the sampling rates to maximize the computer vision performance(i.e., recognition ratio) while avoiding queue overflow under the concept of Lyapunov optimization framework. Through extensive system simulations, the proposed approaches are shown to provide remarkable gains."
9707073,PredStereo: An Accurate Real-time Stereo Vision System,"Stereo vision algorithms are important building blocks of self-driving applications. The two primary requirements of a self-driving vehicle are real-time operation and nearly 100% accuracy in constructing the 3D scene regardless of the weather conditions and the degree of ambient light. Sadly, most real-time systems as of today provide a level of accuracy that is inadequate and this endangers the life of the passengers; consequently, it is necessary to supplement such systems with expensive LiDAR-based sensors. We observe that for a given scene, different stereo matching algorithms can have vastly different accuracies, and among these algorithms there is no clear winner. This makes the case for a hybrid stereo vision system where the best stereo vision algorithm for a stereo image pair is chosen by a predictor dynamically, in real-time.We implement such a system called PredStereo in ASIC 1 that combines two diametrically different stereo vision algorithms, CNN-based and traditional, and chooses the best one at runtime. In addition, it associates a confidence with the chosen algorithm, such that the higher-level control system can be switched on in case of a low confidence value. We show that designing a predictor that is explainable and a system that respects soft real-time constraints is non-trivial. Hence, we propose a variety of hardware optimizations that enable our system to work in real-time. Overall, PredStereo improves the disparity estimation error over a state-of-the-art CNN-based stereo vision system by up to 18% (on average 6.25%) with a negligible area overhead (0.003mm 2 ) while respecting real-time constraints."
9668691,Practical application of pedestrian intelligent detection technology based on computer vision,"Pedestrian detection based on computer vision technology uses camera equipment mounted in vehicles to obtain driving image data, and then calculates whether pedestrians exist from the image or sequence and gives accurate positioning. It improves the safety of driverless vehicles to a large extent and greatly reduces the probability of accidents Pedestrian detection is the use of computer vision technology to determine whether there is a pedestrian in the image or sequence, and at the same time to locate the pedestrian technology. In this paper, first of all, the threshold method, image segmentation technology is described, and then the feature extraction method is divided into two different categories of bottom features and machine learning to make an introduction, and then the classifier construction method is summarized. Finally, the future research of pedestrian detection technology is prospected through the existing problems."
8822180,Survey of automated fabric inspection in textile industries,Defect review of fabric is also a method that accomplished with human visual scrutiny victimization semi machine-driven approach. To cut back time and value wastage because of defects the machine-driven review system for defect detection is employed for this purpose. The investment in machine-driven material defect detection is over economical once reduction parturient value and associated benefits square measure thought of the event of altogether machine-driven net examination system desires sturdy and economical material defect detection algorithms. The examination of real material defects is especially difficult thanks to the big vary of material defect categories that square measure characterized by their incomprehensibility and numerous techniques square measure developed to sight material defects and additionally the aim of this paper is to reason and or describe these algorithms. This paper makes a shot to gift the first survey on material defect detection techniques machine-driven and computer vision examination. Categorization of material detection of defect techniques is beneficial in evaluating the fabric qualities of well-known options. The characterization of real material surfaces victimization their structure and the primitive set have not nonetheless been prosperous. Automatic material examination victimization machine vision with pattern matching algorithmic rule can scale back the time for examination and human error than alternative defect identification and classification algorithmic.
9022078,Multi-Modal Pyramid Feature Combination for Human Action Recognition,"Accurate human action recognition remains a challenging task in the field of computer vision. While many approaches focus on narrow image features, this work proposes a novel multi-modal method that combines task specific features (action recognition, scene understanding, object detection and acoustic event detection) for human action recognition. This work encompasses two contributions: 1) The introduction of a feature fusion block that uses a gating mechanism to perform attention over features from other domains and 2) A pyramidal feature combination approach that hierarchically combines pairs of features from different tasks using the previous fusion block. The richer features generated by the pyramid are used for human action recognition. This approach is validated using a subset of the Moments In Time dataset, resulting in an accuracy of 35.43%."
8981018,Accurate Camera Syncronization Using Deep-Shallow Mixed Models,"To synchronize multiple cameras has been attracting more and more attentions since the surge of 360-degree view videos and VR/AR applications. Usually only the professional cameras with dedicated external-trigger interface can be synchronized easily and accurately. However, those cameras suffer from the high price and the complex operating procedure. For network cameras, some researchers have proposed synchronization method by using the NTP protocol. To our best knowledge, there is no effective method for synchronizing consuming cameras. In this work, we propose a method for synchronizing nearly all kinds of cameras given that they are connected into on PC. The experiment show that for average consuming webcams, our method can achieve the mean absolute difference (MSD) below 12 ms, which is totally meet the requirement of stereo vision or other multiple vision tasks."
8803338,Architecture-Aware Network Pruning for Vision Quality Applications,"Convolutional neural network (CNN) delivers impressive achievements in computer vision and machine learning field. However, CNN incurs high computational complexity, especially for vision quality applications because of large image resolution. In this paper, we propose an iterative architecture-aware pruning algorithm with adaptive magnitude threshold while cooperating with quality-metric measurement simultaneously. We show the performance improvement applied on vision quality applications and provide comprehensive analysis with flexible pruning configuration. With the proposed method, the Multiply-Accumulate (MAC) of state-of-the-art low-light imaging (SID) and super-resolution (EDSR) are reduced by 58% and 37% without quality drop, respectively. The memory bandwidth (BW) requirements of convolutional layer can be also reduced by 20% to 40%."
9583803,"TEyeD: Over 20 Million Real-World Eye Images with Pupil, Eyelid, and Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector, and Eye Movement Types","We present TEyeD, the world’s largest unified public data set of eye images taken with head-mounted devices. TEyeD was acquired with seven different head-mounted eye trackers. Among them, two eye trackers were integrated into virtual reality (VR) or augmented reality (AR) devices. The images in TEyeD were obtained from various tasks, including car rides, simulator rides, outdoor sports activities, and daily indoor activities. The data set includes 2D&3D landmarks, semantic segmentation, 3D eyeball annotation and the gaze vector and eye movement types for all images. Landmarks and semantic segmentation are provided for the pupil, iris and eyelids. Video lengths vary from a few minutes to several hours. With more than 20 million carefully annotated images, TEyeD provides a unique, coherent resource and a valuable foundation for advancing research in the field of computer vision, eye tracking and gaze estimation in modern VR and AR applications. Data and code at DOWNLOAD LINK."
9498502,A Deep Learning Based Diabetic Retinopathy Detection from Retinal Images,"Diabetes is increased tremendously due to metabolism. Lack of early detection, prolonged diabetics might lead to medical complications such as heart problems, eye vision problems, skin issues etc. Diabetic retinopathy (DR) is a frequent abnormality of diabetics. In this paper, we propose computer vision based technique to analyze and predict diabetes from the retinal input images. This helps in an early stage detection of DR. In this image processing steps such as pre-processing, segmentation, feature extraction steps are applied. After the image processing steps, machine learning based classification step is performed. For experimental results, we used python programming language for better results. For experimental results platform, we use jupyter for developing the coding. The framework developed was evaluated on open access public repository datasets, achieving an accuracy of 98.50% using CNN as compared to the accuracy of 87.40% achieved by SVM. These results perform better than several advanced unsupervised ML techniques. It results in decrease of procedural complexity and improved assessment metrics, hence making it suitable to be used in the diagnosis of DR using retinal image analysis."
9339425,4x4 Census Transform,"This paper proposes a novel 4x4 census transform (4x4CT) to encourage further research in computer vision and visual computing. Unlike the traditional census transform (3x3CT) which only uses a nine pixels kernel or 3x3 window size, the proposed 4x4CT uses a sixteen pixels kernel or 4x4 window size with four overlapped groups of 3x3 kernel size. In each overlapping group, a reference input pixel profits from its nearest eight pixels to produce an eight bits binary string convertible to a grayscale decimal integer corresponding to the 4x4CT's output pixel. Preliminary subjective assessments were more promising with the 4x4CT transformed images than 3x3CT transformed images."
8813449,Research on Rice Grain Shape Detection Method Based on Machine Vision,"The grain shape of rice is the most visual index to identify rice type, variety, grade. At present, the rice grain shape is mainly detected by artificial naked eye recognition, so the efficiency is low and the error is large. Computer vision technology is widely used in the quality detection of agricultural products. This paper presents the steps of rice image preprocessing by computer vision technology. According to the grain shape characteristics of rice, the detection method of broken rice or whole grain rice in rice using neural network technology is studied. The experimental results show that the correct rate of identifying the whole broken rice is over 95%."
8660470,SVSoC: Speculative Vision Systems-on-a-Chip,"Frame latency in continuous vision significantly impacts the agility of intelligent machines that interact with the environment via cameras. However, today's continuous vision systems limit the frame latency due to their fundamental sequential execution model. We propose a speculative execution model along with two mechanisms that enable practical vision speculation. We present SVSoC, a new mobile Systems-on-a-chip (SoC) architecture that augments conventional mobile SoCs with the speculation capability. Under the same energy budget, SVSoC achieves 14.3 to 35.4 percent latency reduction in different scenarios."
9641588,Sunflower Diseases Recognition Using Computer Vision-Based Approach,"Sunflower (Helianthus annuus) is a plant categorized as a low to medium drought-sensitive crop. It adds a significant value to the agricultural-based economy. But nowadays worldwide sunflower production is in crisis due to its many diseases. But if proper action is not adopted earlier, many serious diseases will have affect plants. Consequently, it will reduce the productivity, quantity, and quality of sunflower. Manual identification of disease is a very tedious task or perhaps impossible at times. Nowadays, computer vision-based technique has gained its popularity in the field of object recognition. In this paper, we proposed an approach for sunflower disease recognition. A total of 650 images were used to accomplish this work. The image data processing techniques such as resizing, contrast, and color enhancement have also been used. We have used k-means clustering for segmenting the diseases affected region and then extracted features from the segmented images. The classification has performed using five classifiers. We calculated the seven performance evaluation metrics for the performance measurement of each classifier. The highest average accuracy of 90.68% has been obtained for the Random Forest classifier that outperformed others."
9423129,Improving Robustness and Uncertainty Modelling in Neural Ordinary Differential Equations,"Deep learning models such as Resnets have resulted in state-of-the-art accuracy in many computer vision problems. Neural ordinary differential equations (NODE) provides a continuous depth generalization of Resnets and overcome drawbacks of Resnet such as model selection and parameter complexity. Though NODE is more robust than Resnet, we find that NODE based architectures are still far away from providing robustness and uncertainty handling required for many computer vision problems. We propose novel NODE models which address these drawbacks. In particular, we propose Gaussian processes (GPs) to model the fully connected neural networks in NODE (NODE-GP) to improve robustness and uncertainty handling capabilities of NODE. The proposed model is flexible to accommodate different NODE architectures, and further improves the model selection capabilities in NODEs. We also find that numerical techniques play an important role in modelling NODE robustness, and propose to use different numerical techniques to improve NODE robustness. We demonstrate the superior robustness and uncertainty handling capabilities of proposed models on adversarial attacks and out-of-distribution experiments for the image classification tasks."
9270424,Application Research of Interactive Packaging Design Based on Computer Graphics Technology and GIS Model,"With the development of computer network technology and computer graphics processing technology, computer graphics processing technology based on GIS models has received more and more attention from professionals in the field of graphic image design. This paper studies the construction of visual data mining model, combined with the specific requirements of packaging design, and the rational use of computer graphics technology, is conducive to enhancing the rationality of packaging design. Based on this, this article builds a GIS model to process two-dimensional and three-dimensional graphics, and discusses the relationship between computer graphics technology and packaging design as necessary to provide the necessary reference for related research"
9706907,Neural Radiance Fields Approach to Deep Multi-View Photometric Stereo,"We present a modern solution to the multi-view photometric stereo problem (MVPS). Our work suitably exploits the image formation model in a MVPS experimental setup to recover the dense 3D reconstruction of an object from images. We procure the surface orientation using a photometric stereo (PS) image formation model and blend it with a multi-view neural radiance field representation to recover the object’s surface geometry. Contrary to the previous multi-staged framework to MVPS, where the position, iso-depth contours, or orientation measurements are estimated independently and then fused later, our method is simple to implement and realize. Our method performs neural rendering of multi-view images while utilizing surface normals estimated by a deep photometric stereo network. We render the MVPS images by considering the object’s surface normals for each 3D sample point along the viewing direction rather than explicitly using the density gradient in the volume space via 3D occupancy information. We optimize the proposed neural radiance field representation for the MVPS setup efficiently using a fully connected deep network to recover the 3D geometry of an object. Extensive evaluation on the DiLiGenT-MV benchmark dataset shows that our method performs better than the approaches that perform only PS or only multi-view stereo (MVS) and provides comparable results against the state-of-the-art multistage fusion methods."
8709956,Automated Vision-Based High Intraocular Pressure Detection Using Frontal Eye Images,"Glaucoma, the silent thief of vision, is mostly caused by the gradual increase of pressure in the eye which is known as intraocular pressure (IOP). An effective way to prevent the rise in eye pressure is by early detection. Prior computer vision-based work regarding IOP relies on fundus images of the optic nerves. This paper provides a novel vision-based framework to help in the initial IOP screening using only frontal eye images. The framework first introduces the utilization of a fully convolutional neural (FCN) network on frontal eye images for sclera and iris segmentation. Using these extracted areas, six features that include mean redness level of the sclera, red area percentage, Pupil/Iris diameter ratio, and three sclera contour features (distance, area, and angle) are computed. A database of images from the Princess Basma Hospital is used in this work, containing 400 facial images; 200 cases with normal IOP; and 200 cases with high IOP. Once the features are extracted, two classifiers (support vector machine and decision tree) are applied to obtain the status of the patients in terms of IOP (normal or high). The overall accuracy of the proposed framework is over 97.75% using the decision tree. The novelties and contributions of this work include introducing a fully convolutional network architecture for eye sclera segmentation, in addition to scientifically correlating the frontal eye view (image) with IOP by introducing new sclera contour features that have not been previously introduced in the literature from frontal eye images for IOP status determination."
9709933,A New Journey from SDRTV to HDRTV,"Nowadays modern displays are capable to render video content with high dynamic range (HDR) and wide color gamut (WCG). However, most available resources are still in standard dynamic range (SDR). Therefore, there is an urgent demand to transform existing SDR-TV contents into their HDR-TV versions. In this paper, we conduct an analysis of SDRTV-to-HDRTV task by modeling the formation of SDRTV/HDRTV content. Base on the analysis, we propose a three-step solution pipeline including adaptive global color mapping, local enhancement and highlight generation. Moreover, the above analysis inspires us to present a lightweight network that utilizes global statistics as guidance to conduct image-adaptive color mapping. In addition, we construct a dataset using HDR videos in HDR10 standard, named HDRTV1K, and select five metrics to evaluate the results of SDRTV-to-HDRTV algorithms. Furthermore, our final results achieve state-of-the-art performance in quantitative comparisons and visual quality. The code and dataset are available at https://github.com/chxy95/HDRTVNet."
9116870,An Efficient Approach for Using Expectation Maximization Algorithm in Capsule Networks,"Capsule Networks (CapsNets) are brand-new architectures that have shown ground-breaking results in certain areas of Computer Vision (CV). In 2017, Hinton and his team introduced CapsNets with routing-by-agreement in ""Sabour et al"" and in a more recent paper ""Matrix Capsules with EM Routing"" they proposed a more complete architecture with Expectation-Maximization (EM) algorithm. Unlike the traditional convolutional neural networks (CNNs), this architecture is able to preserve the pose of the objects in the picture. Due to this characteristic, it has been able to beat the previous state-of-the-art results on the smallNORB dataset, which includes images with various view points. Also, this new architecture is more robust to white box adversarial attacks. However, CapsNets have two major drawbacks. They can’t perform as well as CNNs on complex datasets and, they need a huge amount of time for training. We try to mitigate these shortcomings by finding optimum settings of EM routing iterations for training CapsNets. Unlike the past studies, we use un-equal numbers of EM routing iterations for different stages of the CapsNet. We manage to achieve higher accuracies than the original CapsNet while training the network up to three times faster. For our research, we use three datasets: Yale face dataset, Belgium Traffic Sign dataset, and Fashion-MNIST dataset."
9051269,Small Object Detection in Aerial Imagery using RetinaNet with Anchor Optimization,"Deep Learning has successfully solved many computer vision problems sometimes in conjunction with traditional computer vision methods and sometimes by replacing them. In this paper, we aim to solve the problem of object detection by employing different methods from deep learning as well as computer vision. Significant amount of work is done in the domain of generic object detection, where usually objects (foreground) cover majority of image space as compared to background. In this paper we will focus on detecting small objects which constitute a tiny area as compared to background such as aerial imagery where desired objects such as people, cars etc. tend to appear relatively small. Such images have an intrinsic imbalanced class problem because background samples dominate object samples. We propose to use an anchor optimization method which will help reduce unnecessary region proposals as well as it can generate customized anchors depending upon the dataset. It can be used in conjunction with any single stage object detection framework. Its empirically noted that this anchor optimization technique improves accuracy over baseline frameworks."
8993566,Intelligent Vision Systems – Bringing Human-Machine Interface to AR/VR,"AR/VR devices are believed to be the next generation mobile compute platform that will replace today's smart-phones. These devices will require new human-machine input (HMI) modalities that are intuitive, non-interruptive, and socially friendly and responsive in an all-day wearable glasses form factor. The candidate HMI modalities include voice, eye gazing, hand/head/body gestsures, and brain-computer interface (BCI). This paper describes computer vision based modalities along with the sensor and system specifications. It also proposes solutions to the extremely stringent power, form factor and performance challenges."
9183287,Helmet and Number Plate detection of Motorcyclists using Deep Learning and Advanced Machine Vision Techniques,"In today's world, the increasing use of Motorcycles has prompted increment in road accidents and injuries. Helmet not used by the motorcycle rider is one of the major cause. Currently, one procedure is to physically check use of helmet at the pavement junction or through the CCTV footage video, which requires human intervention to detect motorcyclists without helmet. The proposed framework presents a computerization machine structure to distinguish the motorcycle rider with or without helmet from images. The system extracts objects class based on feature extracted. The system uses You Only Look Once (YOLO)-Darknet deep learning framework which consists of Convolutional Neural Networks trained on Common Objects in Context (COCO) and combined with computer vision. YOLO's convolutional layers are modified to detect specified three classes and it uses a sliding-window process. The map (Mean Average Precision) on validation dataset achieved 81% by using training data."
9112409,Vision-based sign language recognition system: A Comprehensive Review,"Communication is an essential activity for a human being to express their feeling, ideas, work together, and for the overall development of society. A person with a hearing disability uses sign language for communication and this language develops naturally within them. However, it is not commonly understood by non-signer community and hence, there remains a communication gap between these two communities. To bridge this gap, a computer vision based recognition system can be used. Many researchers have been worked on this field to facilitate the ease of communication between these two communities. In this paper, the basic concept of sign language recognition system and review of its existing techniques along with their comparison presented. The main objective of presenting this survey is to highlight the importance of vision based method with a specific focus on sign language."
8698871,High-Precision Measurement of Binocular Telecentric Vision System With Novel Calibration and Matching Methods,"In stereo vision-based three-dimensional measurements, calibration, and stereo matching are the most challenging tasks for accurate three-dimensional reconstruction. The traditional binocular vision algorithm has low precision, and we propose a binocular vision system using a telecentric lens. In this paper, we propose a calibration and matching algorithm for the telecentric binocular vision system, which collects only two pictures of the calibration plate to complete the system calibration. The algorithm is a special application of 3D reconstruction. In this paper, if the measurement points are selected as points on the grid, the appropriate extension can be used for high-precision 3D reconstruction, which will be explained in detail in the experiments. In order to reduce the calibration error and obtain high maneuverability, the binocular vision system is only calibrated once with a two-dimensional calibration board, and the depth information is obtained through the parallax of the detection process, so as to effectively simplify the calibration process and reduce the errors introduced in the calibration process. For the matching of images in the right and left cameras, the polar theory of telecentric binocular vision system is constructed according to the characteristics of telecentric lens imaging model based on the traditional binocular vision system theory. The matching algorithm only needs to apply the two-dimensional calibration information and then calculates the results according to the parallax combined with the polar line matching algorithm, which simplifies the algorithm flow and reduces the error. In this paper, experiments show that the algorithm proposed is simple and has high precision, good stability, and high-practical value."
9706726,SIGNAV: Semantically-Informed GPS-Denied Navigation and Mapping in Visually-Degraded Environments,"Understanding the perceived scene during navigation enables intelligent robot behaviors. Current vision-based semantic SLAM (Simultaneous Localization and Mapping) systems provide these capabilities. However, their performance decreases in visually-degraded environments, that are common places for critical robotic applications, such as search and rescue missions. In this paper, we present SIGNAV, a real-time semantic SLAM system to operate in perceptually-challenging situations. To improve the robustness for navigation in dark environments, SIGNAV leverages a multi-sensor navigation architecture to fuse vision with additional sensing modalities, including an inertial measurement unit (IMU), LiDAR, and wheel odometry. A new 2.5D semantic segmentation method is also developed to combine both images and LiDAR depth maps to generate semantic labels of 3D mapped points in real time. We demonstrate that the navigation accuracy from SIGNAV in a variety of indoor environments under both normal lighting and dark conditions. SIGNAV also provides semantic scene understanding capabilities in visually-degraded environments. We also show the benefits of semantic information to SIGNAV’s performance."
9142266,"Robust Phase-Based Decoding for Absolute (X, Y, Θ) Positioning by Vision","Computer vision is a convenient noncontact tool for position control and thus constitutes an attractive multidirectional alternative to widely used single-direction sensors. However, to meet actual industry requirements, vision-based measurement methods must be sufficiently robust to comply with industrial environments. This article explores the robustness of an in-plane position measurement method based on a pseudoperiodic pattern and allowing a 10 8 range-to-resolution ratio in displacement and a 1-μ rad angular resolution over 2π rad. This article shows how the pattern phase can be used to maintain reliable measurements despite defocus, discrepancies in local contrast, nonuniform illuminations, or occlusions. The proposed method can be implemented at different size scales with unique capabilities combining high resolution, large measurement range, and robustness to diverse kinds of disturbances."
9231689,A Computer Vision Inspired Automatic Acoustic Material Tagging System for Virtual Environments,"This paper presents the ongoing work on an approach to material information retrieval in virtual environments (VEs). Our approach uses convolutional neural networks to classify materials by performing semantic segmentation on images captured in the VE. Class maps obtained are then re-projected onto the environment. We use transfer learning and fine-tune a pretrained segmentation model on images captured in our VEs. The geometry and semantic information can then be used to create mappings between objects in the VE and acoustic absorption coefficients. This can then be input for physically-based audio renderers, allowing a significant reduction in manual material tagging."
9650105,Bilateral Symmetry Detection in Perspective Based on Vanish Features,"Symmetry detection is an important research field in image analysis and computer vision. It plays an important role in object match, recognition and location. However, the change of perspective projection angle increases the difficulty of symmetry detection. Therefore, it is of great significance to detect the symmetry of objects under perspective projection. Current feature point-based methods mostly use multiple sets of matched point pairs to detect the symmetry axis line, which not only introduces massive calculation but also interference between feature point pairs. Hence, we propose a novel feature point match-based symmetry axes lines detection method of planar graphs in perspective. This approach detects matched pairs based on the BEBLID descriptor and AdaLAM mismatch removal, and adopts binocular cameras to get a potential symmetry axis line from a pair of matched points, avoiding the influence of other matched points. Experimental results show that this method can achieve accurate and efficient detection of symmetry axis."
9421942,Deep Learning in Image Classification: A Survey Report,"Recently, deep learning is emerging as a powerful tool and has become a leading machine learning tool in computer vision and image analysis. In this survey paper, we provide a snapshot of this fast-growing field, image classification, specifically. We briefly introduce several popular neutral networks and summarize their applications in image classification. In addition, we also discuss the challenge of deep learning in image classification."
8936015,A Vision-Based Detection and Tracking Algorithm for a Child Monitoring Robot,"Accidents have been found to be one of the leading causes of both fatal and non-fatal injuries to children. Though some accidents that occur are often unavoidable, more often than not these injuries can be prevented by giving the child proper attention. The researchers intend to address certain gaps in stationary monitoring solutions by adding abilities such as an insured way of continuously monitoring the test subject and a real time notification feature to a mobile spherical robot. This research presents the software division of a technological solution to child monitoring by developing a computer vision algorithm for following and monitoring children indoors utilizing an RGB-D camera. This algorithm will work hand in hand with a hardware design of a spherical robot that utilizes microcontrollers, RFID technology and GSM system. An Android application will also be created to provide the users the means of manually overriding the spherical robot, color calibration and location indicator as a part of the robot's notification system. The detection and tracking ability of the algorithm is tested by using objects with varying characteristics. The autonomous navigation testing of the robot is performed at two controlled test setups: living room and child's playroom."
9574403,Elements and construction principles of sports visual image system based on hierarchical semantics,"With the continuous development of Internet technology and multimedia technology, the application of digital images has penetrated into all aspects of social life. At the same time, computer science is also developing rapidly, and hardware and software equipment are constantly improving and innovating in function and performance. In this context, image understanding has become one of the research hotspots in the field of computer vision in recent years. This article analyzes the composition of the hierarchical semantic sports visual image system. Starting from the characteristics of the structure of sports images, a semantic labeling and hierarchical indexing method is analyzed and given. For a sports image data, the image data labeling can be automatically completed by matching with the feature database. Sports visual images have the characteristics of intuitiveness, metaphor and readability. It is mainly composed of visual content elements, symbolic meaning elements and subjective elements. In the process of building a more perfect sports visual image system, we should follow the principles of independence, integrity and innovation."
9508142,DeepFoveaNet: Deep Fovea Eagle-Eye Bioinspired Model to Detect Moving Objects,"Birds of prey especially eagles and hawks have a visual acuity two to five times better than humans. Among the peculiar characteristics of their biological vision are that they have two types of foveae; one shallow fovea used in their binocular vision, and a deep fovea for monocular vision. The deep fovea allows these birds to see objects at long distances and to identify them as possible prey. Inspired by the biological functioning of the deep fovea a model called DeepFoveaNet is proposed in this paper. DeepFoveaNet is a convolutional neural network model to detect moving objects in video sequences. DeepFoveaNet emulates the monocular vision of birds of prey through two Encoder-Decoder convolutional neural network modules. This model combines the capacity of magnification of the deep fovea and the context information of the peripheral vision. Unlike algorithms to detect moving objects, ranked in the first places of the Change Detection database (CDnet14), DeepFoveaNet does not depend on previously trained neural networks, neither on a huge number of training images for its training. Besides, its architecture allows it to learn spatiotemporal information of the video. DeepFoveaNet was evaluated in the CDnet14 database achieving high performance and was ranked as one of the ten best algorithms. The characteristics and results of DeepFoveaNet demonstrated that the model is comparable to the state-of-the-art algorithms to detect moving objects, and it can detect very small moving objects through its deep fovea model that other algorithms cannot detect."
9498764,A Solution of Human-Computer Remote Interaction with Tactile Feedback,"In human-computer interaction, we need to use human's multiple sensory channels and motion channels to improve its naturalness and efficiency. These channels include voice, handwriting, posture, vision, expression, etc. Visual-based human-computer interaction systems are often mentioned in the literature. The system uses a camera to monitor and track the operator's posture, and sets corresponding commands to control the movement of the robot. On top of that, we add tactile feedback to allow the robot to accomplish more tasks. In the vision system, first we obtain color images and depth images through a depth camera. Secondly, we estimate the human posture through OpenPose. Finally, we map the motion of the human arm to the motion of the robotic arm. In the tactile system, first we control the movement of the magic hand through the exoskeleton. Secondly, we obtain the force values of the fingers through the magic hand. Finally, we provide force feedback to each finger through the exoskeleton."
9290737,AI in Photography: Scrutinizing Implementation of Super-Resolution Techniques in Photo-Editors,"Judging the quality of a photograph from the perspective of a photographer we can ascertain resolution, symmetry, content, location, etc. as some of the factors that influence the proficiency of a photograph. The exponential growth in the allurement for photography impels us to discover ways to perfect an input image in terms of the aforesaid parameters. Where content and location are the immutable ones, attributes like symmetry and resolution can be worked upon. In this paper, I prioritized resolution as our cynosure and there can be multiple ways to refine it. Image super-resolution is progressively becoming a prerequisite in the fraternity of computer graphics, computer vision, and image processing. It's the process of obtaining high-resolution images from their low-resolution counterparts. In my work, image super-resolution techniques like Interpolation, SRCNN (Super-Resolution Convolutional Neural Network), SRResNet (Super Resolution Residual Network), and GANs (Generative Adversarial Networks: Super-Resolution GAN-SRGAN and Conditional GAN-CGAN) were studied experimentally for post-enhancement of images in photography as employed by photo-editors, establishing the most coherent approach for attaining optimized super-resolution in terms of quality."
9156875,Can Deep Learning Recognize Subtle Human Activities?,"Deep Learning has driven recent and exciting progress in computer vision, instilling the belief that these algorithms could solve any visual task. Yet, datasets commonly used to train and test computer vision algorithms have pervasive confounding factors. Such biases make it difficult to truly estimate the performance of those algorithms and how well computer vision models can extrapolate outside the distribution in which they were trained. In this work, we propose a new action classification challenge that is performed well by humans, but poorly by state-of-the-art Deep Learning models. As a proof-of-principle, we consider three exemplary tasks: drinking, reading, and sitting. The best accuracies reached using state-of-the-art computer vision models were 61.7%, 62.8%, and 76.8%, respectively, while human participants scored above 90% accuracy on the three tasks. We propose a rigorous method to reduce confounds when creating datasets, and when comparing human versus computer vision performance. Source code and datasets are publicly available."
9495379,Machine Vision Based Method for Measuring Single-Cell Biophysical Properties Using Dielectrophoresis Mobility,"Measurement on single-cell biophysical properties is of great significance for cell sorting, cytopathology tracking, etc. Dielectrophoresis (DEP) based single-cell measurement has attracted increasingly interests because of its non-invasive, label-free, and easy integration with other functional structures. However, the measurement accuracy and efficiency of DEP-based method highly depends on cell motion tracking. Here, we propose a machine vision-based DEP motion tracking method to realize accurate and fast measurements on single-cell biophysical properties."
9498867,Detection System of Truck Blind Area based on Machine Vision,"With the continuous development of cargo transport, road transport has become an indispensable part of the logistics industry today. Because of the multi-directional visual blind area of freight cars, drivers cannot fully observe the surrounding environment, which is prone to traffic accidents. Therefore, it is of great significance to develop a reliable and safe detection system for truck blind area for road vehicle traffic safety. In this paper, based on the machine vision technology and combined with the scientific research project, the gradient distribution of local area is extracted by HOG feature, so as to realize the detection of the accuracy and rapidity of the detection of the blind area of trucks from multiple angles and in all aspects, and timely alarm is given to provide real driving convenience for truck drivers and provide higher safety guarantee for highway traffic."
9237683,Method for Detecting Obstacles of Riceplanter Based on Machine Vision,"Rice planting machine in the paddy field automatic operation project, the realization of the automatic steering of the machine at the field stalk is the key technology to realize the autonomous navigation of the whole process of the rice planting machine. Aiming at the problem of discriminating the turning position in the visual navigation of the rice planter, this paper proposes a discriminating method of the rice field stem based on machine vision. Obtain distortion parameters through camera calibration to correct the original image, and calculate the deviation of the gray average value of the image line through python to determine whether the field stem appears. Binary and morphological processing of the field stem image, the processed image is scanned in the height direction to obtain the characteristic points of the field stem boundary line fitting, and finally the least square method is used to fit the feature points to draw the field stem borderline. The test results show that the discrimination rate of whether the field stem appears is not less than 90%, and the average pixel error of the field stem boundary is 4.954 pixels. It can meet the real-time and accuracy requirements of the rice planter during the operation process."
8947355,Detection of Nano-particles Based on Machine Vision,"Information extraction from images is a key problem. In this paper, a fast, high-precision, multi-target image processing method based on machine vision is proposed to detect nanoparticles. The proposed methods include the following steps: image pre-processing, feature extraction, and result output. An auxiliary-Calibration plate is used as the auxiliary need to determine the measurement accuracy. Through the analysis and calculation of the characteristic scales of nanoparticles, our results are highly consistent with the experiment, indicating the accuracy of our method and algorithm. From the input test images to the output results, the algorithm only takes less than a second. For the image including more complexed structures, it can also be accurately detected. Furthermore, the algorithm is transferable and extensible, and can be applied in wider image processing fields."
9308790,Research and development of intelligent headlight test system based on machine vision,"As an important component of automotive vehicles, automotive headlights have evolved from a traditional single-light headlight to an active safety lighting system. Intelligent headlight, such as matrix LED headlight has gradually become the mainstream of headlight lighting systems for their good lighting visibility and easy to control combined light forms. It is import to do research on the test method of intelligent headlight for quality assurance. In this paper, an automatic test system was designed, included light shape image processing and machine vision system development. The test results show that the test system can detect the headlight product characteristics well and efficiently."
9050157,Defect Inspection Algorithm of Metal Surface Based on Machine Vision,"During the production and post-processing of complex metal parts, surface defects such as scratches, stains, and pits will inevitably occur, which will reduce the yield of metal parts and cause serious economic losses. Complex metal surfaces have complex surface texture characteristics, which leads to the difficulty of detection during the detection process. Therefore, this paper presents a defect inspection algorithm of metal surface based on machine vision. The proposed surface defect inspection algorithm first uses improved bi-dimensional empirical mode decomposition (BEMD)-based extracting algorithm to perform initial extracting of surface defects through filtering out complex textures on the metal surface, while retaining as much effective information as defects as possible. Then, the inspection algorithm applies Canny edge detection operator to detect the edge information of these defects. Finally, the final acquisition of defect edges can be achieved by connecting edge breakpoints and image filling operations. Experimental results on inspection of a variety of the surface defects on parts with metal surface are reported to show the performance of the defect inspection algorithm."
9551313,Seismic stability monitoring system of building structure based on machine vision,"In order to overcome the problems of low detection accuracy and efficiency existing in the traditional seismic stability monitoring system of building structure, a new seismic stability monitoring system of building structure based on machine vision is proposed in this paper. The function and structure of the system are designed on LabVIEW platform. Based on this, the hardware of the system is designed, including building structure visual image acquisition equipment and wireless transmission module. In the software part of the system, the image algorithm is used to calculate the visual image of the building structure to complete the judgment of the instability factor. The experimental results show that, compared with the traditional monitoring system, the visual feature extraction accuracy and stability monitoring accuracy of the designed system are higher, and the practical application performance is stronger."
9467621,Warpage Measurements of Metal Plate Based on Machine Vision,"Aiming at the warping problem caused by the temperature difference between the upper and lower surfaces of the metal plate during heat treatment, a metal warping detection system based on machine vision is designed. The system extracts the feature information of calibration plate in the image as the dimension reference of metal warping detection. Then, the ROI(Region of Interest) of metal warping is selected for Blob analysis to obtain the metal warping area. The height of the minimum circumscribed rectangle is calculated as the maximum warping height of the metal, and the position of the metal when the maximum warping occurs is displayed, the height divided by the width of the minimum circumscribed rectangle is the warpage of the metal plate. The experimental results show that the system can detect the metal warping and display the position in real time, and has the advantages of fast operation speed and small error. The measurement accuracy can reach 0.1mm. It can provide data basis for the research and analysis of the cause of metal warping."
9526330,Navigation System for Olfactory Mobile Robot by Using Machine Vision System,"Combustion of fuels to produce heat or other forms of power has been the cornerstone of industrial processes. Liquefied Petroleum Gas (LPG) is an alternative fuel, which is used primarily as a fuel in most spark-ignited internal combustion engines. Therefore, the incidence of leakage in gas cylinders needs to be considered, in order to avoid fire, poisoning, and even death to those around it. Olfactory mobile robot can be used to detect gas content. Thus, the source of the gas leak in the industry and in the storage area for LPG gas cylinders can be identified. This paper emphasizes on robot navigation by using Machine Vision System (MVS) to speed up the process of finding the exact location of the gas leak source. Several tests were carried out on several aspects. The test results on the motor drivers show that the robot can move well. While the MVS testing shows that the programming algorithm for image processing is able to recognize track borders. Then, the results on gas sensors testing show that the robot can find the source of the gas leak and can adjust it to the required robot speed. The last test, which is on the whole system, shows that the duration needed to find the source of the gas leak is in accordance with the distance from the source of the gas leak."
8808980,Development of the Machine Vision System for Automated Inspection of Printed Circuit Board Assembl,"Due to the present, there are many automated optical inspection solutions for printed circuit board assembly inspection using in various companies but most of the solutions are high cost, large-size, limited function, etc., Also, the detection of defected sizes on the printed circuit board assembly has been decreasingly in sizes, i.e., micro-sizes. Such this size is getting difficult to detect by existing automated optical inspection solutions. Therefore, this research has been proposed to create an automated optical inspection system for detecting micro-size defects. The system application consists of LabVIEW programming with OpenCV Library in a kind of server connection between the Raspberry Pi board and the USB microscope camera. Thus, the system of the machine vision is conducted based on the algorithm design for automated monitoring on the leakage of copper along the edges of the printed circuit board assembly. The primary result presents the creation of the system prototype that can inspect the micro-size defects of the printed circuit board in real time monitoring and operations. Nevertheless, the purposed system is suitable for laboratory-scale testing only, the system application for automated manufacturing has been purposed and developed, and will be presented in the separate paper."
9467535,Design and Application of HMI System for Robot Based on Vision,"In view the common problems existing in traditional HMI technology, a HMI system for robot based on vision is proposed. The robot obtains the gesture image of human through camera, and uploads the image to Baidu AI server through PC to realize the understanding of gesture and make corresponding feedback action, so as to achieve the purpose of human-machine interaction. The application of robot in intelligent home service is taken as an example for technical application. The user interacts with the robot to request water pouring service through gestures, and the results showed that the system responded quickly, the gesture recognition rate was high, and the overall experience was good. The scheme makes the HMI more intelligent and natural, improves the robot experience, and is conducive to the robot into entertainment, office, assistance for the elderly and other areas of life. It has application prospects, and can provide reference for technicians in related fields."
9603095,Using Machine Vision Based of Preventive Maintenance and Management of Historic Buildings,"The 921 earthquake occurred in Taiwan in 1988. At that time, more than 700 old buildings were not designated as ""historical sites."" They were damaged in the earthquake without proper maintenance and management. This was a great impact on the national culture. Therefore, the government began to rescue cultural assets with high historical, artistic or scientific value, and registered and managed valuable historical buildings with reference to relevant foreign management methods, hoping to make up for regrets. However, it is not easy for the civilians who are the owners of private cultural assets to assist in maintenance and management if they do not have the professional knowledge and skills of maintenance and management. In view of this, this research applies machine vision technology in information technology to build a machine learning database to deal with the destruction of old buildings and provide owners, managers and enthusiasts of cultural assets. The non-destructive inspection method of image shooting can find a solution and contact relevant units for processing, which greatly reduces the difficulty of maintenance and management and increases the public's attention to the preservation of private cultural assets."
9459106,Intelligent Control System of Coal Mine Main Transportation Based on Machine Vision,"This paper develops a set of an intelligent control system of mine main transportation based on machine vision. The system can accurately measure the instantaneous coal quantity of the belt, collect the running speed of each belt, establish the mathematical model of coal quantity distribution of the whole coal flow transportation system, and establish an intelligent control system with a self-learning function based on the real-time distribution of coal quantity, to dynamically adjust the running speed of each belt and reduce the cost Reduce the times of belt pressing, improve the starting rate, and make the belt run at the most economical speed, to realize energy saving and green production."
9563710,Research on Self-calibration Strategy of Workpiece Processing Based on Machine Vision,"Self-calibration processing equipment is studied through measuring the precise size of the workpiece produced in the factory assembly line based on the machine vision measurement technology. Hough circle detection algorithm and subpixel edge algorithm are adopted. After several experiments, the measuring accuracy of the device can reach 0.05mm, which meets the design requirements of measurement accuracy. The self-calibration parameters are sent to the processing equipment through the transmission layer to reduce the rejection rate and improve the enterprise benefits."
8834354,Research on surface defect detection and grinding path planning of steel plate based on machine vision,"In the process of surface defect detection and grinding of steel plate, it needs a lot of manpower, not only low efficiency, but also not high detection accuracy. The surface defect detection of steel plate based on machine vision has been applied and paid more and more attention. In this paper, the image processing algorithm is studied to identify the surface defects of steel plate, and to obtain the location information of the defect on the steel. A method integrating with defect classification and merging and polishing path planning is proposed, and the coordinate information of each point on the path is obtained. Under the Windows system, a surface defect detection software is designed to detect the surface of the steel plate, and the detected defect location information is transmitted to the robot, and then the robot grinds the steel plate. Experimental results show that the algorithm is effective."
9075661,Defect detection system for optical element surface based on machine vision,"Machine vision is now an excellent tool for testing a wide range of industrial products such as textiles, printed circuit boards, electronic component tags, integrated circuits (ICs) and machine tools. In this paper, we propose an optical lens detection system based on machine vision, which can be widely used in the defect detection of camera lenses, eyeglass lenses and other related optical components. Firstly, the hardware composition of the entire system and the flow of the control system were introduced. Secondly, through the image preprocessing algorithm such as image denoising and contrast enhancement, combined with the algorithm of Canny operator and morphological processing, the reasonable image processing results were obtained. Finally, a decision tree classifier based on C4.5 algorithm was established according to the shape feature of the defect, which realized the automatic recognition and classification of surface defects on optical components. The experimental results show the effectiveness of this method in the detection of optical component defects, which provides a theoretical basis for the functional analysis of optical components in the future. Meanwhile, it can be extended to defect detection in many fields such as circuit boards and integrated circuits."
9134254,Embedded Image Processing and Video Analysis in Intelligent Camera-based Vision System,The paper discusses the development of a technical vision system based on intelligent cameras. The technical specifications of Axis intelligent CCTV cameras are described. The features of software implementation of image processing and video analysis algorithms on the embedded platform of these cameras are considered. Recommendations for the implementation of algorithms in order to provide embedded realtime data processing are formulated.
9359022,Implementation of Inverse Perspective Mapping for Camera-Vision Water-Level Measurements,"To prevent floods from endangering lives and property, surveillance cameras are widely used in places prone to floods, such as low-lying areas and rivers, to monitor water levels during typhoons. Recently, with the advancement of surveillance camera technologies, surveillance cameras have been widely applied to automatically measure water levels. In this study, an embedded system based on a single camera is proposed, where camera images are used to measure water levels. Digital image processing is used to identify the current water level from an image of the on-site staff gauge. The fact that the camera position typically results in a non-orthogonal angle between the camera's optical axis and the staff gauge plane on-site was taken into consideration due to the fact that it causes perspective distortion and leads to deviations in water level measurements. In this study, the inverse perspective mapping (IPM) method was applied in the system to overcome this problem. Using IPM to transform the coordinate system of images from one perspective plane to another rectified the perspective distortion. To simplify the image processing of every input frame, only the region of interest (ROI) underwent IPM transformation during the water level calculation in order to improve the system operating performance. The experimental results proved that the water level measurement system using the IPM method effectively reduced measurement deviations for confined space applications and effectively tracked real-world water level changes."
8923044,Deep Learning Assisted Smart Glasses as Educational Aid for Visually Challenged Students,"Computer Vision Technology has played a significant role in assisting visually challenged people to carry out their day to day activities without much dependency on other people. Smart glasses in one such solution which enables blind or visually challenged people to “read” images. This paper is an attempt in this direction to build a novel smart glass which has the ability to extract and recognize text captured from an image and convert it to speech. It consists of a Raspberry Pi 3 B+ microcontroller which processes the image captured from a webcam super-imposed on the glasses of the blind person. Text detection is achieved using the OpenCV software and open source Optical Character Recognition (OCR) tools Tesseract and Efficient and Accurate Scene Text Detector (EAST) based on Deep Learning techniques. The recognized text is further processed by Google's Text to Speech (gTTS) API to convert to an audible signal for the user. A second feature of this solution is to provide location-based services to the blind people by identifying locations in an academic building using the RFID technology. This solution has been extensively tested in a university environment for aiding visually challenged students. The novelty of the implemented solution lies in providing the desired computer vision functionalities of image/text recognition which is economical, small-sized, accurate and uses open source software tools. This solution can be potentially used for both educational and commercial applications."
9408935,Deep learning based vehicle violation detection system,"Due to the limited road resources and the ever-increasing number of vehicles and persons, more frequent traffic violations and higher management costs have resulted. It is crucial to propose a more intelligent and less cost scheme to solve the traffic management problem. In this paper, we design and implement a vehicle violation detection system based on deep learning, which includes the detection, tracking and recognition of vehicles. On this basis, the detection and real-time alarm of common violations, such as red light running and impolite pedestrian, are also supported. Compared with the traditional detection and monitoring based on physical equipment, our system is completely based on computer vision, where the cutting-edge achievements of deep learning have been improved and applied. The system is not only more intelligent, but also can reduce the cost to a greater extent. Experiments illustrate that the system can meet the needs of the intelligent management of urban traffic through real-time monitoring and data analysis of the traffic scenes."
9489224,IoT Based Weapons Detection System for Surveillance and Security Using YOLOV4,"Due to the increase in crime and terrorism in most parts of the world, security surveillance is becoming increasingly important. A computer vision-based system for detecting weapons for real-time security surveillance is designed in this work. For identification, detection, and notifying the appropriate authorities, the system employs the YOLO V4 (You Only Look Once) algorithm. This neural network can be trained using images, videos, and live streaming videos. This model incorporates Internet-of-Things (IoT) smart devices that are interconnected and automated in weapon detection. This model's accuracy varies depending on the quality of the images and videos used in the detection process. Here, the proposed research work has discovered that the detection process is affected by the type of hardware that has been utilized to run the algorithm, ranging from low-quality image/video detection with 70% accuracy to high-quality image/video detection with 95% accuracy."
9302500,"Deep Learning Based, Real-Time Object Detection for Autonomous Driving","One of the active research topics that maintains its popularity in the field of Computer Vision is the problem of object detection in autonomous cars. Since object detection is a difficult problem, high performance solutions do not work very quickly. Similarly, real-time solutions make compromise on performance. However, due to the nature of autonomous driving, object detection systems must perform in real time and high performance. In this study, Tiny YOLOv3, one of the most successful object detection architectures, was combined with one of the classical object tracking methods, the Kalman filter. A small and real-time object detection system, which increases the model's accuracy without losing its speed, is proposed."
9137359,Real-time drip infusion monitoring through a computer vision system,"Intravenous (IV) infusion is one of the most common therapies in hospitalized patients. Monitoring the flow rate of the fluid that is being administered to the patient is therefore very important for his safety, considering that both over-infusion and under-infusion can cause serious health problems. In this document, a novel method for monitoring the flow rate in IV infusions is presented, that is based on deep learning computer vision techniques. Basically, the drip chamber is filmed with a camera and object detection is used to count drops. The proposed method is therefore less invasive than other ones developed for this purpose. Experimental results show that it can produce an accurate real-time estimate of the instantaneous flow rate of the drip. For these reasons, the proposed method can be effectively adopted to implement monitoring and control systems for health facilities."
9360968,Research on Design and Application of Brand Vision Inspection and Sorting System Based on Image Processing,"Visual inspection technology has been more and more widely used in industrial production, which is crucial to improve production efficiency and production intelligence. In order to monitor the sorting equipment more effectively, it is necessary to carry out accurate real-time detection of the operation parameters of the important actuators and functional components in the sorting process. Although computer vision technology does not touch the measuring object, it can accurately obtain a lot of information from the acquired images, and then analyze and process the information to obtain specific information such as object size, surface defects, chromaticity, etc. In the sorting operation of traditional assembly line, manual sorting is the main method. The workers' repeated sorting for a long time can not meet the requirements, which greatly affects the efficiency and accuracy of work. This paper proposes to add visual detection system based on image processing on the basis of the original sorting system, so as to change the whole control system into a closed-loop from the structure, effectively solve the problems in the process of loading bags, and improve the accuracy of sorting."
8935657,"TAG: Nucleus Detection in Colorectal Adenocarcinomas Histology Images using Local Texture, Appearance, and Gradient Features","This work considers the problem of detecting nuclei in H&E stained images. The high intra-class and inter-image variability of nuclei of the images itself calls for a more robust method that is able to handle this high variability. Recently, deep learning has become the most popular method in computer vision with many systems achieving state-of-the-art results for a wide array tasks. However, as a trade-off, deep learning methods require tons of data and computational resources that some may not posses. Thus, this work proposes a traditional computer vision pipeline along with TAG, a local feature-based approach that combines texture, appearance, and gradient features which is used in the task of pixel-per-pixel prediction in the virtue of semantic segmentation. Based from the results, the method was shown to be effective despite the challenges of high variability in the dataset."
9672049,Understanding the state of the Art in Animal detection and classification using computer vision technologies,"This work presents the results of a survey through the analysis of studies published between January 2017 and May 2021, aiming to compose a broader view of the state of the art in the field of animal detection and classification using computer vision technologies in urban environments, and also the majors researches gaps available to address. We conducted an automatic search through two digital knowledge bases identifying 146 studies in the subject, among them 20 were selected for our analysis and data extraction. Further, the 20 studies were classified into 6 categories: (i) studies using SVM, (ii) studies using HOG, (iii) studies using SIFT, (iv) studies using PCA, (v) studies using CNN, and (vi) DFDL. As a result, it can be noted that the use of CNN is predominant concerning other approaches and that there are also combinations to improve the accuracy of classification models. In conclusion, it is possible to observe that the state-of-the-art approaches have been used in different situations, however, in the context of animal detection and classification in intelligent urban environments, there is still a lack of specific architectures to improve results."
9677045,An Embedded Deep Learning Computer Vision Method for Driver Distraction Detection,"Driver distraction is a modern issue when operating automotive vehicles. It can lead to impaired driving and potential accidents. Detecting driver distraction most often relies on analyzing a photo or video of the driver being distracted. This involves complex deep learning models which often can only be ran on computers too powerful and expensive to implement into automobiles. This paper presents a method of detecting driver distraction using computer vision methods within an embedded environment. By taking the deep learning architecture SqueezeNet, which is optimized for embedded deployment, and benchmarking it on a Jetson Nano embedded computer, this paper demonstrates a viable method of detecting driver distraction in real time. The method shown here involves making slight modifications to SqueezeNet to be trained on the AUC Distracted Driver Dataset, yielding accuracies as high as 93% when detecting distracted driving."
9263449,Machine Vision-Based Urban Farming Growth Monitoring System,"Precision agriculture integrates information technology and agricultural systems to support efficiency, productivity, and agricultural profitability. This paper presents an approach to the growth of the crops for urban farming using machine vision methods. The approach employs Improved Background Subtraction technique to detect the intruder or unwanted objects, and the Green Channel Otsu filtering to detect the growth rate of the crops. The calculated values are then being sent to a server hosted in the cloud for further analysis. The analytics result summarizes in a reporting dashboard to give insight on the growth of the crops."
9711600,Security of Containerized Computer Vision Applications,"Computer vision processing requires many resources that are easily accessible in the Cloud. However, with the growth of the Internet of Things (IoT), sending all the data to the cloud to analyze is undesirable due to delay in response, high bandwidth consumption, reliability issues, and privacy concerns. Edge computing is an attractive solution to address these issues by processing the data at edge nodes. Well-defined nature of IoT applications enable lightweight containers at the edge to run the computer vision applications. However, security of containers running computer vision applications is not studied in detail. In this paper, we investigate security of containerized computer vision application using Docker containers using multiple tools and we design our own containers that provide better security for computer vision applications running on the edge nodes."
8772209,Deep Learning-Based System for Automatic Recognition and Diagnosis of Electrical Insulator Strings,"This paper presents a complete system for automatic recognition and the diagnosis of electrical insulator strings which efficiently combines different deep learning-based components to build a versatile solution to the automation problem of the power line inspection process. To this aim, the proposed system integrates one component responsible for insulator string segmentation and two components in charge of its diagnosis. The insulator string segmentation component consists of a novel fully convolutional network (FCN) architecture, termed Up-Net, which enhances the capabilities of the state-of-the-art U-Net network by introducing new skip connections at certain levels of the architecture. Furthermore, we propose a second variant of the Up-Net network by training it within a generative adversarial network (GAN) framework. The capabilities of the proposed Up-Net variants are incremented by the application of data augmentation and transfer learning techniques, achieving accurate segmentation of the insulator string elements (i.e., discs and caps). Regarding the insulator string diagnosis, we design a convolutional neural network (CNN) which takes as input the mask generated by the insulator string segmentation component and is capable of identifying the absence of a variable number of discs. The second diagnosis component consists of a novel strategy which integrates a Siamese convolutional neural network (SCNN) designed for modeling the similarity between adjacent discs and allowing the detection of several types of disc defects using the same model. The proposed system has been extensively evaluated in several video sequences from real aerial inspections of high-voltage insulators, showing robust insulator recognition and diagnosis capabilities."
9461386,Computer vision and stochastic approach for LEO objects attitude determination,"A few objects larger than one-meter re-enter the Earth's atmosphere every year. In total, about 75% of all the largest objects ever launched have already returned. Two current examples of large objects returned to the atmosphere are those of the Tiangong-1 Space Lab (April 2, 2018) and heavy-lift launch vehicle Long March 5B (May 9, 2021). The threat posed by space debris has become a global issue, and Space Situational Awareness (SSA) is a prerequisite for any space activity. Optical systems provide valuable assistance in identifying and monitoring such objects. The Space Systems and Space Surveillance Laboratory of University of Rome “La Sapienza” (S5Lab) carries out activities for space surveillance purposes and develop algorithms and software for analysing the data collected by optical systems. This article focuses on determining the attitude of space debris from their optical observation. For this purpose, a simulator of the space environment and a genetic algorithm were used through which it is possible to derive the presumed attitude parameters (Euler angles and angular velocities) that the object had at the time of acquisition."
9633455,Research on Head-up and Down Behavior Computer Detection by Deep Learning and Artificial Intelligence,"The use of deep learning technology to recognize human behavior in video scenes is one of the current research hotspots in the field of computer vision. In recent years, it has received extensive attention from academia and engineering. Human behavior recognition related technologies are used in intelligent monitoring, human-computer interaction, and virtual Reality and content-based video retrieval and interpretation have broad application prospects and potential economic value. At the same time, in the classroom scene, identifying students' classroom behavior has important reference significance for the evaluation of students' learning status, learning quality and teachers' teaching effects. This article applies deep learning technology to college classroom scenes, from the perspective of students looking down and raising their heads, to detect students and identify their behavioral states."
9270518,Analysis of Computer Information Processing Technology Under the Background of Intelligent Big Data,"In recent years, the development trend of computers has been rapid. With the increase of more digital information technology methods, the flow of data information has been growing, and the methods of information processing have been continuously updated. The technical requirements have also been constantly improved. The rise of the Internet has brought convenience to people's production and life, but security issues such as hacker attacks and computer viruses have become increasingly prominent. This paper analyzes according to the current trend of computer information processing technology, and combines the intelligent big data information technology means to explore the scientific and reasonable use, which is intended to provide more computers for technical staff and users. The perspective aims to promote the technology in the context of big data."
9230869,A Comprehensive Survey on Computer Vision Based Approaches for Moving Object Detection,"Moving object detection has achieved a noticeable attention in many computer vision applications. The research community have contributed lot of works for dealing with major challenges of moving object detection in real-world scenarios. The paper presents a comprehensive review on different moving object detection techniques classified into four categories: Background Modeling Based techniques; Frame Difference Based techniques; Optical Flow Based techniques and Deep Learning Based techniques. Moreover, detailed descriptions of various methods in each of this category are also provided."
9259705,Music panel: An application for creating and editing music using OpenCV and JFugue,"Music is the product of human creativity and the mastery of musical instruments. There are a large number of musical instruments, whether electric, acoustic, wind or percussion. Some areas of research explore the creation of music through the human-computer interaction. This paper proposes a way to generate sounds with the help of a wall-installed panel with notes drawn on it. The notes are recognized with a webcam and computer vision algorithms, and then played back to the user in real time. The application was created with the aim of giving people who want to express their creativity through music, the chance to do so without the need for musical instruments, in a simple and novel way."
9648917,VCSELs for 3D Sensing and Computer Vision,"stract:
This chapter briefly explains the background and differences among 3D sensing and imaging technologies, namely stereo vision, time‐of‐flight, and structured light. In the last two years, the 3D sensing products using 2D laser arrays especially in smartphones has revolutionized the VCSEL industry. The authors briefly explain 2D VCSEL arrays, their key features for sensing functions, and some of the manufacturing challenges. A few VCSEL chips used for commercial products are shown as examples. A special topic on computer vision is explained as a combination of real‐world and virtual objects for enhanced human 3D vision, in particular on augmented reality and intelligence. Finally, a brief note on 3D sensing (mobile and camera) market is presented."
9457215,Fingerprint Presentation Attack Detector Using Global-Local Model,"The vulnerability of automated fingerprint recognition systems (AFRSs) to presentation attacks (PAs) promotes the vigorous development of PA detection (PAD) technology. However, PAD methods have been limited by information loss and poor generalization ability, resulting in new PA materials and fingerprint sensors. This article thus proposes a global-local model-based PAD (RTK-PAD) method to overcome those limitations to some extent. The proposed method consists of three modules, called: 1) the global module; 2) the local module; and 3) the rethinking module. By adopting the cut-out-based global module, a global spoofness score predicted from nonlocal features of the entire fingerprint images can be achieved. While by using the texture in-painting-based local module, a local spoofness score predicted from fingerprint patches is obtained. The two modules are not independent but connected through our proposed rethinking module by localizing two discriminative patches for the local module based on the global spoofness score. Finally, the fusion spoofness score by averaging the global and local spoofness scores is used for PAD. Our experimental results evaluated on LivDet 2017 show that the proposed RTK-PAD can achieve an average classification error (ACE) of 2.28% and a true detection rate (TDR) of 91.19% when the false detection rate (FDR) equals 1.0%, which significantly outperformed the state-of-the-art methods by ~10% in terms of TDR (91.19% versus 80.74%)."
8934612,Design and Development of Hand Gesture Based Virtual Mouse,"The technique of establishing a process of interaction between human and computer is evolving since the invention of computer technology. The mouse is an excellent invention in HCI (Human-Computer Interaction) technology. Though wireless or Bluetooth mouse technology is invented still, that technology is not completely device free. A Bluetooth mouse has the requirement of battery power and connecting dongle. Presence of extra devices in a mouse increases the difficulty to use it. The proposed mouse system is beyond this limitation. This paper proposes a virtual mouse system based on HCI using computer vision and hand gestures. Gestures captured with a built-in camera or webcam and processed with color segmentation & detection technique. The user will be allowed to control some of the computer cursor functions with their hands which bear colored caps on fingertips. Primarily, a user can perform left clicks, right clicks, and double clicks, scrolling up or down using their hand in different gestures. This system captures frames using a webcam or built-in cam and processes the frames to make them track-able and after that recognizes different gestures made by users and perform the mouse function. So the proposed mouse system eliminates device dependency in order to use a mouse. Therefore it can be proved beneficial in order to develop HCI technology."
9156546,Varicolored Image De-Hazing,"The quality of images captured in bad weather is often affected by chromatic casts and low visibility due to the presence of atmospheric particles. Restoration of the color balance is often ignored in most of the existing image de-hazing methods. In this paper, we propose a varicolored end-to-end image de-hazing network which restores the color balance in a given varicolored hazy image and recovers the haze-free image. The proposed network comprises of 1) Haze color correction (HCC) module and 2) Visibility improvement (VI) module. The proposed HCC module provides required attention to each color channel and generates a color balanced hazy image. While the proposed VI module processes the color balanced hazy image through novel inception attention block to recover the haze-free image. We also propose a novel approach to generate a large-scale varicolored synthetic hazy image database. An ablation study has been carried out to demonstrate the effect of different factors on the performance of the proposed network for image de-hazing. Three benchmark synthetic datasets have been used for quantitative analysis of the proposed network. Visual results on a set of real-world hazy images captured in different weather conditions demonstrate the effectiveness of the proposed approach for varicolored image de-hazing."
9241787,RFID-Pose: Vision-Aided Three-Dimensional Human Pose Estimation With Radio-Frequency Identification,"In recent years, human pose tracking has become an important topic in computer vision (CV). To improve the privacy of human pose tracking, there is considerable interest in techniques without using a video camera. To this end, radio-frequency identification (RFID) tags, as a low-cost wearable sensor, provide an effective solution for 3-D human pose tracking. In this article, we propose RFID-Pose, a vision-aided realtime 3-D human pose estimation system, which is based on deep learning assisted by CV. The RFID phase data are calibrated to effectively mitigate the severe phase distortion, and high accuracy low rank tensor completion is employed to impute the missing RFID data. The system then estimates the spatial rotation angle of each human limb, and utilizes the rotation angles to reconstruct human pose in realtime with the forward kinematic technique. A prototype is developed with commodity RFID devices. High pose estimation accuracy and realtime operation of RFID-Pose are demonstrated in our experiments using Kinect 2.0 as a benchmark."
9397533,Visual Sketching: From Image Sketches to Code,"Writing code is difficult and time consuming. This vision paper proposes Visual Sketching, a synthesis technique that produces code implementing the likely intent associated with an image. We describe potential applications of Visual Sketching, how to realize it, and implications of the technology."
9022009,Towards Generalizable Distance Estimation By Leveraging Graph Information,"Approximating the distance of objects present in an image remains an important problem for computer vision applications. Current SOTA methods rely on formulating this problem to convenience depth estimation at every pixel; however, there are limitations that make such solutions non-generalizable (i.e varying focal length). To address this issue, we propose reformulating distance approximation to a per-object detection problem and leveraging graph information extracted from the image to potentially achieve better generalizability on data acquired at multiple focal lengths."
8782524,"Comparing Energy Efficiency of CPU, GPU and FPGA Implementations for Vision Kernels","Developing high performance embedded vision applications requires balancing run-time performance with energy constraints. Given the mix of hardware accelerators that exist for embedded computer vision (e.g. multi-core CPUs, GPUs, and FPGAs), and their associated vendor optimized vision libraries, it becomes a challenge for developers to navigate this fragmented solution space. To aid with determining which embedded platform is most suitable for their application, we conduct a comprehensive benchmark of the run-time performance and energy efficiency of a wide range of vision kernels. We discuss rationales for why a given underlying hardware architecture innately performs well or poorly based on the characteristics of a range of vision kernel categories. Specifically, our study is performed for three commonly used HW accelerators for embedded vision applications: ARM57 CPU, Jetson TX2 GPU and ZCU102 FPGA, using their vendor optimized vision libraries: OpenCV, VisionWorks and xfOpenCV. Our results show that the GPU achieves an energy/frame reduction ratio of 1.1-3.2× compared to the others for simple kernels. While for more complicated kernels and complete vision pipelines, the FPGA outperforms the others with energy/frame reduction ratios of 1.2-22.3×. It is also observed that the FPGA performs increasingly better as a vision application's pipeline complexity grows."
9025533,Study on Fashion Image Retrieval Methods for Efficient Fashion Visual Search,"Fashion image retrieval (FIR) is a challenging task, which requires searching for exact items accurately from massive collections of fashion products based on a query image. Despite recent advances, FIR still has limitations for application to real-world visual searches. The main reason for this is not only the trade-off between the model complexity and performance, but also the common nature of fashion images captured under uncontrolled circumstances (e.g. varying viewpoints and lighting conditions). In particular, fashion images are vulnerable to shape deformations and suffer from inconsistency between the user's query images and refined product images. Moreover, multiple fashion objects can be present simultaneously within a single image. In this paper, we considered an FIR method that is optimized for the fashion domain. We investigated training strategies and deep models to improve the retrieval performance. The experimental results on three benchmarks from DeepFashion dataset show that considered methods could achieve the significant improvements compared to the previous FIR methods."
9344869,Smart Elevator Cotrol System Based on Human Hand Gesture Recognition,"The rapid development of computer vision technology makes human-computer interaction possible, which has a wide range of application prospects. In this paper, we propose a gesture recognition system that can be applied to the operation of smart elevators. It can recognize different gestures of people without touching the buttons and reach the designated floor. The training data set used to train the hand gesture recognition consists of pictures and real-time frames taken by the camera. We utilize gesture segmentation, gesture tracking and other methods to preprocess the image. Then we use CNN to train the preprocessed pictures. At last, We design the user interface for computer and human interaction. The experiment shows 98.1% accuracy of static images."
9553684,Deep Vision Transformers for Remote Sensing Scene Classification,"In this paper, we present a scene classification method based on vision transformers. These types of networks, which are now the standard models in natural language processing (NLP) do not rely on convolution block as in convolutional neural networks (CNNs). Alternatively, they are based on a mechanism known as multi-head self-attention (MSA), which captures the contextual relations between image pixels regardless of their spatial distance. At the first step, the images under analysis are split into patches, then converted to sequence by flattening and embedding. The embedding position is encoded and added to the sequence to preserve the order of the patches. Then, the resulting sequence is fed to several MSA layers for generating the final representation. To increase the classification performance, we employed several data augmentation strategies to expand the size and the diversity of the training data. Additionally, we show experimentally that we can compress the network by pruning half of its layers while keeping the competing performance. We further investigate the performance of the data-efficient image transformers (DeiT), a version of the model that is trained by knowledge distillation with less amount of data. Experimental results on two remote sensing datasets show that vision transformers can outperform state-of-the-art methods based on CNNs."
9396492,Watermelon Quality Determining from a Photo Using Machine Vision,"The article discusses methods for determining the quality of watermelon using machine vision, as well as the implementation of these methods in the form of an application. The article describes experiments on using these methods on photographs of real watermelons under various circumstances. To write algorithms, the existing methods for determining the ripeness of a watermelon were studied. In the course of experiments, the problem of implementing these methods using machine vision algorithms was solved. A ready-made solution was obtained that correctly determines the quality of a watermelon from a photograph."
8958356,Optimization of Functional Tasks for a Vision System of a Mobile Robot,"This paper analyzes features of a computer vision of mobile robotic systems. One of the features of mobile robot vision systems is the limited computing resources. This is because, due to the requirements for cost and overall characteristics of the studied type of robots, their design uses relatively cheap computing devices with low speed and memory size. Therefore, it is necessary to simplify tasks that we set for the vision system, focusing on existing equipment. In addition, it is necessary to choose correct recognition algorithms for the vision system and adapt them for use in mobile versions of robotic systems. The paper discusses recognition algorithms in terms of requirements for vision systems of mobile robots. On the example of prototypes of mobile robots, optimal combinations of hardware and software solutions are shown, taking into account a simplification of functional tasks of vision systems. It is proposed to provide the requirements for computer vision systems under consideration by a rational combination of software and hardware capabilities."
9053761,Attention Guided Region Division for Crowd Counting,"Crowd counting has drawn more and more attention in computer vision. There are two mainstream approaches to deal with crowd counting tasks, regression and detection. Regression-based methods usually overestimate the count in sparse areas, while detection-based methods tend to underestimation in dense areas. In this paper, we propose a two-branch network combining regression and detection. We introduce the attention mechanism to make the network adaptively divide dense and sparse areas and employ appropriate methods on them respectively. The regression branch predicts density map in extremely dense areas. An improved detection network is applied to detect multi-scale heads in relatively sparse areas. Our method is able to obtain precise head bounding boxes in sparse areas with ensuring counting accuracy in dense areas. Experimental results show that our method achieves state-of-the-art on challenging public crowd counting datasets."
9489066,Optimized Image Edge Detection Approach using Fractional Order Calculus,"In Computer Vision, Segmentation plays a vital step in identifying the edges of the objects which helps in object recognition. This paper proposes a new optimized edge detection algorithm which uses a fractional order mask for convolution to demarcate the edges. One of the important steps in edge detection is using a low-pass filter for smoothening to minimize the effect of noises. This bears the overheads of altering true edges and additional computational time used in identifying the edges. The characteristic of this algorithm is the fact that it is resistant to noise and can adapt to efficient detection of edges in any type of image. The quality of the edges detected varies based on the order of the fractional parameter. In most Image Processing techniques, smoothing process accounts for computational time overhead and bears side effects of modifying actual edges. To tackle this problem, this paper proposes an edge detection algorithm which skips the smoothing operation."
9417910,Plant Disease Detection Using Deep Learning,"In recent years, use of Convolutional neural networks has been explored in a wide range of applications whether its image classification, feature extraction or image segmentation. One of those applications is plant disease detection, since plant disease is one of the most significant factors that leads to poor yield in the agricultural sector. Over the period of time various deep learning approaches have been used to solve this problem of identification and classification of plant disease. But then to there are some limitation to these approaches. The recent application of transformer networks in computer vision tasks has shown great promise. This paper compares these approaches with traditional CNN approaches in the task of plant disease detection. The best validation accuracy that our transformer model achieves is 97.98%."
9156626,Select to Better Learn: Fast and Accurate Deep Learning Using Data Selection From Nonlinear Manifolds,"Finding a small subset of data whose linear combination spans other data points, also called column subset selection problem (CSSP), is an important open problem in computer science with many applications in computer vision and deep learning. There are some studies that solve CSSP in a polynomial time complexity w.r.t. the size of the original dataset. A simple and efficient selection algorithm with a linear complexity order, referred to as spectrum pursuit (SP), is proposed that pursuits spectral components of the dataset using available sample points. The proposed non-greedy algorithm aims to iteratively find K data samples whose span is close to that of the first K spectral components of entire data. SP has no parameter to be fine tuned and this desirable property makes it problem-independent. The simplicity of SP enables us to extend the underlying linear model to more complex models such as nonlinear manifolds and graph-based models. The nonlinear extension of SP is introduced as kernel-SP (KSP). The superiority of the proposed algorithms is demonstrated in a wide range of applications."
9297424,Computer-aided decision-making System of Legal Provisions of Water Ecological Compensation System based on Character Recognition,"Computer-aided decision-making system for legal provisions of water ecological compensation system based on the character recognition is designed in this paper. The information of the network is realized by the interaction between neurons. The storage of knowledge and information is a distributed physical relation between the interconnection of network components. The learning and recognition of the network are determined by the dynamic evolution of the connection weights of each core neuron, hence, this core characteristics are considered to then construct the efficient compensation system. The pattern recognition models are applied to analyze the performance. The data mining and computer vision models are combined to the construct the efficient model. The experimental results have proven the overall effectiveness."
9025479,Surrogate Contrastive Network for Supervised Band Selection in Multispectral Computer Vision Tasks,"Computer vision techniques that operate on hyper-and multispectral imagery benefit from the additional amount of spectral information relative to those that exploit traditional RGB or monochromatic visual data. However, the increased volume of data to be processed brings about additional memory, storage and computational requirements. In order to address such limitations, a wide range of techniques for dimensionality reduction have been introduced by previous work. In this paper, we propose a framework for spectral band selection that is highly data-and computationally efficient. The method leverages a convolutional siamese network learned by optimizing a contrastive loss, and performs band selection based on the low-dimensional data embeddings produced by the network. We empirically demonstrate the efficacy of the method on an object detection task from aerial multispectral imagery. The results show that, in spite of the method's frugality, it produces very competitive band selection results against the evaluated competing techniques."
9650329,Overview of image mosaic technology by computer vision and digital image processing,"Image mosaic technology has become an important research object in the fields of computer vision, computer graphics and digital image processing technology, and has achieved some results. This paper introduces the image mosaic technology and its application fields; Image mosaic technology includes three basic parts: image preprocessing technology, image registration technology and image fusion technology. This paper mainly introduces the basic algorithm of image preprocessing technology, image registration technology and image fusion technology commonly used algorithm, and it is analyzed and summarized; At the end of the paper, the existing problems of image Mosaic technology are put forward, and the development trend of image Mosaic technology is prospected."
9277813,Computer Vision based Automatic Power Equipment Condition Monitoring and Maintenance: A Brief Review,"In recent years, with the progress of science and technology, network communication technology, information processing technology, artificial intelligence technology, big data technology, navigation technology, power electronics technology and other rapid development, for the development of highly intelligent with independence. The autonomous navigation technology, which integrates computer vision, artificial intelligence, sensor fusion and other disciplines, will provide technical support for the practicality and popularization of the new type of power inspection robot. On the other hand, using robots instead of manual inspection is bound to be the future development direction. Research and development of substation inspection robot with autonomous navigation ability can sense the substation road environment through its own sensor system, plan the driving route and control the vehicle to reach the predetermined destination, which can greatly improve the inspection efficiency of the robot, broaden its operation range and improve the efficiency of the robot inspection To improve its working ability, this will make the power inspection robot to the practical direction of a big step forward."
9450731,"A pipeline framework for robot maze navigation using computer vision, path planning and communication protocols","Maze navigation is a recurring challenge in robotics competitions, where the aim is to design a strategy for one or several entities to traverse the optimal path in a fast and efficient way. To do so, numerous alternatives exist, relying on different sensing systems. Recently, camera-based approaches are becoming increasingly popular to address this scenario due to their reliability and given the possibility of migrating the resulting technologies to other application areas, mostly related to human-robot interaction. The aim of this paper is to present a pipeline methodology towards enabling a robot solving maze autonomously, by means of computer vision and path planning. Afterwards, the robot is capable of communicating the learned experience to a second robot, which then will solve the same challenge considering its own mechanical characteristics which may differ from the first robot. The pipeline is divided into four steps: (1) camera calibration (2) maze mapping (3) path planning and (4) communication. Experimental validation shows the efficiency of each step towards building this pipeline."
8353133,Egocentric Meets Top-View,"Thanks to the availability and increasing popularity of wearable devices such as GoPro cameras, smart phones, and glasses, we have access to a plethora of videos captured from first person perspective. Surveillance cameras and Unmanned Aerial Vehicles (UAVs) also offer tremendous amounts of video data recorded from top and oblique view points. Egocentric and surveillance vision have been studied extensively but separately in the computer vision community. The relationship between these two domains, however, remains unexplored. In this study, we make the first attempt in this direction by addressing two basic yet challenging questions. First, having a set of egocentric videos and a top-view surveillance video, does the top-view video contain all or some of the egocentric viewers? In other words, have these videos been shot in the same environment at the same time? Second, if so, can we identify the egocentric viewers in the top-view video? These problems can become extremely challenging when videos are not temporally aligned. Each view, egocentric or top, is modeled by a graph and the assignment and time-delays are computed iteratively using the spectral graph matching framework. We evaluate our method in terms of ranking and assigning egocentric viewers to identities present in the top-view video over a dataset of 50 top-view and 188 egocentric videos captured under different conditions. We also evaluate the capability of our proposed approaches in terms of temporal alignment. The experiments and results demonstrate the capability of the proposed approaches in terms of jointly addressing the temporal alignment and assignment tasks."
9176632,Quantification of Advanced Dementia Patients’ Engagement in Therapeutic Sessions: An Automatic Video Based Approach using Computer Vision and Machine Learning,"Most individuals with advanced dementia lose the ability to communicate with the outside world through speech. This limits their ability to participate in social activities crucial to their well-being and quality of life. However, there is mounting evidence that individuals with advanced dementia can still communicate non-verbally and benefit greatly from these interactions. A major problem in facilitating the advancement of this research is of a practical and methodical nature: assessing the success of treatment is currently done by humans, prone to subjective bias and inconsistency, and it involves laborious and time consuming effort. The present work is the first attempt at exploring if automatic (artificial intelligence based) quantification of the degree of patient engagement in Adaptive Interaction sessions, a highly promising intervention developed to improve the quality of life of nonverbal individuals with advanced dementia. Hence we describe a framework which uses computer vision and machine learning as a potential first step towards answering this question. Using a real-world data set of videos of therapeutic sessions, not acquired specifically for the purposes of the present work, we demonstrate highly promising results."
9075571,The Method of Thread Defect Detection Based on Machine Vision,"To resolve the problem of low accuracy and difficulty in detecting threads, this paper proposes a method of detecting based on machine vision. The image of the threaded part is collected online by an industrial CCD camera, and the image is gray scaled, enhanced, binary and de-noised. The de-noising method selects the mathematical morphology operation. Mathematical morphology of corrosion, expansion, open and closed operations effectively preserves thread shape characteristics and edge information of the threads. The algorithm detects the position of the thread defect by locating the defect of the thread using a subtraction operation. The experimental results show that the algorithm has strong ability to identify and detect defects and can effectively identify appearance defects such as no thread, less thread and irregular pitch, which meets the basic design requirements."
9118580,Driving State Recognition for Ego-central Driving Videos,"In computer vision filed, route-planning plays a vital role in guiding vehicles. Efficient recognition the motion states of driving vehicles may contribute to an effective advanced driving assistance system. In this paper, we propose an end-to-end driving state recognition method for ego-central driving videos based on NVIDIA's frame-level steering angle generation and 3D ConvNet (C3D) based video-level action recognition tasks, we propose an end-to-end driving state recognition method for ego-central driving videos. A C3D based deep learning architecture, named C3D Multi-classification (CM), is proposed to recognize the multiple driving vehicle states. In particular, we train the network with large amounts of video sequences, force it to generate the category of the given video batch. The generated results can be utilized in a high-level decision-making progress. The efficiency of the proposed method is evaluated on Canpus20 and NVIDIA driving datasets separately. The verified results show that, the proposed network can construct the internal representation (content and dynamic) of ego-central driving videos using its effective spatiotemporal feature learning ability. Experimental results on the real-world captured dataset prove that the proposed method has good generalization."
9408740,Research on Automatic Recognition Algorithm of Pointer Meter Based on Machine Vision,"Pointer meters can directly reflect the changing trend of measured values, so they are widely used in industry field measurements. The digital reading of pointer meters is conductive to real-time data monitoring and management. The paper firstly describes the structural characteristics of the pointer meter, and briefly describes the preprocessing algorithm of the meter picture, and then uses the algorithm to complete the position of the dial and the pointer location relative to the dial. The pointer positioning is achieved by monitoring the edges on both sides of the pointer. Finally, the pointer reading is determined according to the position of the pointer scale relative to the minimum scale. The algorithm is not affected by the tilt of the dial, and the final recognition accuracy is one-fifth of the smallest scale."
9528213,Computer Vision Based Eye Gaze Controlled Virtual Keyboard for People with Quadriplegia,"Disabilities or physical impairments caused by genetic or traumatic injury hold people from proclaiming their basic needs, feelings, or ideas. This paper presents an eye gaze controlled virtual keyboard which is an assistive system for disable and able-bodied people. Disability such as quadriplegia restrain people from using their four limbs, virtual keyboards proposed in this paper can be used for expressing their thoughts. The proposed virtual keyboard has 40 keys including the delete key, English fonts, numerical digits, and several Latin symbols where each key lights up sequentially in the forward or backward direction. The user’s eye gaze navigates the activation of the key and eye blinking is used for typing the active key. The proposed keyboard has salient features such as sound is played when a key has been pressed and delete key eliminates unintentionally typed letters. The Character Per Minute (CPM), Word Per Minute (WPM), total error rate have been calculated from the tests conducted by several users which show an improved result than the existing models. The proposed virtual keyboard is a low-cost assistive system as it requires only a webcam for performing."
9528150,Hand Gesture-Based Character Recognition Using OpenCV and Deep Learning,"Fast, accurate, and user-friendly human-computer interaction (HCI) requires both processing and intelligence. Understanding signs, and symbols is already possible by computers but recognizing symbols drawn live by a human in front of a camera is still a new concept. Many attempts have been made to achieve this already by using different sensors like Time of Flight (ToF) camera, Kinect sensor, etc., by using special metric systems or by special algorithms. Our research proposes doing such work using normal cameras that almost every computer has already. In this work, we tried to approach the problem from two directions. We left the detection, tracking and drawing tasks on mathematics-based algorithms like Accumulated Weight, CSRT (The Channel and Spatial Reliability) Tracker and OpenCV (Open Computer Vision) library. The recognition relies on deep learning. Our model can classify different characters of English alphabet and numerals so that when a user draws that, it can predict that. Our deep learning model is 98.56% accurate in classifying symbols which is more accurate than previous methods while not requiring any special sensors."
9096726,A Survey On Face Recognition Algorithms,"Facial Recognition System is a computer technology that uses a variety of algorithms that identify the human face in digital images, identify the person and then verify the captured images by comparing them with the facial images stored in the database. Facial recognition is an important topic in computer vision, and many researchers have studied this topic in many different ways; it is important especially in some applications such as surveillance systems. The main objective of this survey paper is to compare the multiple algorithms used for facial recognition."
9710127,Deep Reparametrization of Multi-Frame Super-Resolution and Denoising,"We propose a deep reparametrization of the maximum a posteriori formulation commonly employed in multi-frame image restoration tasks. Our approach is derived by introducing a learned error metric and a latent representation of the target image, which transforms the MAP objective to a deep feature space. The deep reparametrization allows us to directly model the image formation process in the latent space, and to integrate learned image priors into the prediction. Our approach thereby leverages the advantages of deep learning, while also benefiting from the principled multi-frame fusion provided by the classical MAP formulation. We validate our approach through comprehensive experiments on burst denoising and burst super-resolution datasets. Our approach sets a new state-of-the-art for both tasks, demonstrating the generality and effectiveness of the proposed formulation."
9578730,CompositeTasking: Understanding Images by Spatial Composition of Tasks,"We define the concept of CompositeTasking as the fusion of multiple, spatially distributed tasks, for various aspects of image understanding. Learning to perform spatially distributed tasks is motivated by the frequent availability of only sparse labels across tasks, and the desire for a compact multi-tasking network. To facilitate CompositeTasking, we introduce a novel task conditioning model – a single encoder-decoder network that performs multiple, spatially varying tasks at once. The proposed network takes an image and a set of pixel-wise dense task requests as inputs, and performs the requested prediction task for each pixel. Moreover, we also learn the composition of tasks that needs to be performed according to some CompositeTasking rules, which includes the decision of where to apply which task. It not only offers us a compact network for multitasking, but also allows for task-editing. Another strength of the proposed method is demonstrated by only having to supply sparse supervision per task. The obtained results are on par with our baselines that use dense supervision and a multi-headed multi-tasking design. The source code will be made publicly available at www.github.com/nikola3794/composite-tasking."
9577843,Drafting and Revision: Laplacian Pyramid Network for Fast High-Quality Artistic Style Transfer,"Artistic style transfer aims at migrating the style from an example image to a content image. Currently, optimization-based methods have achieved great stylization quality, but expensive time cost restricts their practical applications. Meanwhile, feed-forward methods still fail to synthesize complex style, especially when holistic global and local patterns exist. Inspired by the common painting process of drawing a draft and revising the details, we introduce a novel feed-forward method named Laplacian Pyramid Network (LapStyle). LapStyle first transfers global style patterns in low-resolution via a Drafting Network. It then revises the local details in high-resolution via a Revision Network, which hallucinates a residual image according to the draft and the image textures extracted by Laplacian filtering. Higher resolution details can be easily generated by stacking Revision Networks with multiple Laplacian pyramid levels. The final stylized image is obtained by aggregating outputs of all pyramid levels. Experiments demonstrate that our method can synthesize high quality stylized images in real time, where holistic style patterns are properly transferred."
9156964,Semantic Drift Compensation for Class-Incremental Learning,"Class-incremental learning of deep networks sequentially increases the number of classes to be classified. During training, the network has only access to data of one task at a time, where each task contains several classes. In this setting, networks suffer from catastrophic forgetting which refers to the drastic drop in performance on previous tasks. The vast majority of methods have studied this scenario for classification networks, where for each new task the classification layer of the network must be augmented with additional weights to make room for the newly added classes. Embedding networks have the advantage that new classes can be naturally included into the network without adding new weights. Therefore, we study incremental learning for embedding networks. In addition, we propose a new method to estimate the drift, called semantic drift, of features and compensate for it without the need of any exemplars. We approximate the drift of previous tasks based on the drift that is experienced by current task data. We perform experiments on fine-grained datasets, CIFAR100 and ImageNet-Subset. We demonstrate that embedding networks suffer significantly less from catastrophic forgetting. We outperform existing methods which do not require exemplars and obtain competitive results compared to methods which store exemplars. Furthermore, we show that our proposed SDC when combined with existing methods to prevent forgetting consistently improves results."
9577992,GANmut: Learning Interpretable Conditional Space for Gamut of Emotions,"Humans can communicate emotions through a plethora of facial expressions, each with its own intensity, nuances and ambiguities. The generation of such variety by means of conditional GANs is limited to the expressions encoded in the used label system. These limitations are caused either due to burdensome labelling demand or the confounded label space. On the other hand, learning from inexpensive and intuitive basic categorical emotion labels leads to limited emotion variability. In this paper, we propose a novel GAN-based framework that learns an expressive and interpretable conditional space (usable as a label space) of emotions, instead of conditioning on handcrafted labels. Our framework only uses the categorical labels of basic emotions to learn jointly the conditional space as well as emotion manipulation. Such learning can benefit from the image variability within discrete labels, especially when the intrinsic labels reside beyond the discrete space of the defined. Our experiments demonstrate the effectiveness of the proposed framework, by allowing us to control and generate a gamut of complex and compound emotions while using only the basic categorical emotion labels during training. Our source code is available at https://github.com/stefanodapolito/GANmut."
9578839,Efficient Conditional GAN Transfer with Knowledge Propagation across Classes,"Generative adversarial networks (GANs) have shown impressive results in both unconditional and conditional image generation. In recent literature, it is shown that pre-trained GANs, on a different dataset, can be transferred to improve the image generation from a small target data. The same, however, has not been well-studied in the case of conditional GANs (cGANs), which provides new opportunities for knowledge transfer compared to unconditional setup. In particular, the new classes may borrow knowledge from the related old classes, or share knowledge among themselves to improve the training. This motivates us to study the problem of efficient conditional GAN transfer with knowledge propagation across classes. To address this problem, we introduce a new GAN transfer method to explicitly propagate the knowledge from the old classes to the new classes. The key idea is to enforce the popularly used conditional batch normalization (BN) to learn the class-specific information of the new classes from that of the old classes, with implicit knowledge sharing among the new ones. This allows for an efficient knowledge propagation from the old classes to the new ones, with the BN parameters increasing linearly with the number of new classes. The extensive evaluation demonstrates the clear superiority of the proposed method over state-of-the-art competitors for efficient conditional GAN transfer tasks. The code is available at: https://github.com/mshahbazi72/cGANTransfer"
9607717,Object Detection in Cluttered Environments with Sparse Keypoint Selection,"In cases such as mobile robotic applications with limited computational resources, traditional approaches might be preferred over neural networks. However, open source solutions using traditional computer vision are harder to find than neural network implementations. In this work we address the task of object detection in cluttered environments in point clouds from RGB-D cameras. We compare several open source implementation available in the Point Cloud Library and present a novel and superior solution for this task. We further propose a novel sparse key-point selection approach that combines the advantages of uniform sampling and a dedicated keypoint detection algorithm. Our extensive evaluation shows the validity of our approach, which also improves the results of the compared methods. All code is available on our project repository: https://github.com/vseib/point-cloud-donkey."
9212994,Incorporating the Knowledge Distillation to Improve the EfficientNet Transfer Learning Capability,"Due to the Deep Learning requirement for a large training dataset, Transfer Learning has become a central method in the field of Computer Vision, which heavily used Deep Learning. Since the adoption of transfer learning in the field, the performance of the models in computer vision is significantly upgraded. The common transfer learning practice in computer vision research is to use state-of-the-art architectures in the ImageNet dataset as the backbone network. The best performing architecture in ImageNet is currently held by EfficientNet, which is a set of architectures that were progressively grown from a baseline architecture. By observing the EfficientNet development approach, it is natural to hypothesize that the smaller EfficientNet architecture is capable to contain the knowledge of the larger architecture. This hypothesis comes from the fact that the larger EfficientNet architecture was grown from the smaller architecture. Therefore, in this study, we experimented if it is beneficial to transfer the knowledge from the larger architecture to the smaller architecture of the EfficientNet. To achieve this goal, we proposed a transfer learning method that uses Knowledge Distillation as the knowledge transfer mechanism from the larger to smaller architecture. We found that the proposed method is able to upgrade the performance of each of the EfficientNet architecture. Several smaller architectures are even able to outperform the larger architecture that was trained using the standard transfer learning method."
9482067,Advanced Driver Assistance System based on Machine Vision,"In view of the frequent occurrence of traffic problems such as urban traffic congestion and traffic accidents, people pay more and more attention to traffic safety. This paper takes the key technical problems such as front vehicle detection and identification and pre-collision detection in the advanced driving assistance system as the research object, this paper puts forward a computer vision solution based on YOLOv5 algorithm and monocular camera distance calibration, which provides more comprehensive driving environment information for the advanced driving assistance system and improves the active safety of the vehicle. Finally, a case study is given to verify the superiority of the algorithm proposed in this paper."
9157412,Learning to Optimize on SPD Manifolds,"Many tasks in computer vision and machine learning are modeled as optimization problems with constraints in the form of Symmetric Positive Definite (SPD) matrices. Solving such optimization problems is challenging due to the non-linearity of the SPD manifold, making optimization with SPD constraints heavily relying on expert knowledge and human involvement. In this paper, we propose a meta-learning method to automatically learn an iterative optimizer on SPD manifolds. Specifically, we introduce a novel recurrent model that takes into account the structure of input gradients and identifies the updating scheme of optimization. We parameterize the optimizer by the recurrent model and utilize Riemannian operations to ensure that our method is faithful to the geometry of SPD manifolds. Compared with existing SPD optimizers, our optimizer effectively exploits the underlying data distribution and learns a better optimization trajectory in a data-driven manner. Extensive experiments on various computer vision tasks including metric nearness, clustering, and similarity learning demonstrate that our optimizer outperforms existing state-of-the-art methods consistently."
9702298,Automatic Counting System Using IoT and Computer Vision,"The number of frequenters is a significant intimation of success of every public or private mass- market space. In those spaces multiplex guests don't buy from their first visit and not all guests find their choice by just visiting. We propose in this paper a smart guest counter for the shopping centers and mass-market spaces. We will have four rudimentary tracks in fetching and analyzing APC statistics which is ordinary to all APC's. Those are information-accession, Statistics-recording and store, Information-fetching to processing and scrutiny. This method operates on the basis of tackle and software members. Counting detectors at each entry, for starters, dredge person exertion. Information retrieval machine on board take accounts of entries and exits using a logic computation. Usually, such entries and exits are kept in a fashion that gives smooth counting stop-referencing. It is accomplished by keeping track of time as well as distance measurements. The information is then redirected from the on-board storage device to a central computing installation using some kind of ministry. Finally, the info is entered into computer programs, and the requested info is presented."
9111940,Artificial Object Images Synthesis in Underwater Robot Vision System,"the article discusses the features of the synthesis of images of artificial objects with the aim of their search and recognition in the vision system of an underwater robot. The results of research and development to given: a database - a classifier of features that allows you to store information about the reference underwater objects; software module for generating images of reference objects and their insertion in a sonar image file. In the modeling program for the synthesis of underwater objects in the HBO image, the algorithms for introducing objects used by taking into account the peculiarities of the formation of HBO images by sonar. The program allows implementing several objects of spherical, cylindrical and cubic shapes with specified parameters, which allows simulating the situation of search for an artificial object according to given geometric features against the background of other objects. The developed software allows simulating underwater objects according to the characteristics close to real objects with the help of their preliminary processing and subsequent implementation in real HBO images"
9258756,Analysis of Meat Color Change using Computer Vision,"Sensory parameters are crucial for making a purchase decision in meat products. Thus, consumers will guide their choice based on their color. They will seek cherry red meat; when the meat turns brown due to the myoglobin's oxidation, the product is no longer desired. Therefore, the food industry must have a system that could be effective and give just-in-time information regarding changes in color to maintain the quality during the shelf life that consumers expect. This research aims to present a methodology based on computer vision to analyze the change of color in meat. We used images taken of different beef cuts and tested on different days. The Euclidean distance on the average of colors could be used. However, the method proposed in this study is the use of Kullback Leibler divergence, which takes the meat not only at one color point but as a cloud of points. The results were obtained with the Kullback Leibler divergence demonstrated that it is possible to calculate differences in meat images when passing the days. The practical application for this type of analysis would be in the retail industry in order to give just in time information about the quality of meat."
9522807,Reconsidering CO2 emissions from Computer Vision,"Climate change is a pressing issue that is currently affecting and will affect every part of our lives. It’s becoming incredibly vital we, as a society, address the climate crisis as a universal effort, including those in the Computer Vision (CV) community. In this work, we analyze the total cost of CO2 emissions by breaking it into (1) the architecture creation cost and (2) the life-time evaluation cost. We show that over time, these costs are non-negligible and are having a direct impact on our future. Importantly, we conduct an ethical analysis of how the CV-community is unintentionally overlooking its own ethical AI principles by emitting this level of CO2. To address these concerns, we propose adding ""enforcement"" as a pillar of ethical AI and provide some recommendations for how architecture designers and broader CV community can curb the climate crisis."
8998032,Research on Unmanned Aerial Vehicle vision-aid landing with Dynamic vision sensor,"Compared with the conventional frame-based image sensor, Dynamic Visual Sensor (DVS) only sensitive to the intensity changes in the scene, and then serially send out these changes in the form of Activity Event (AE). Taking advantage of real-time, low redundancy and small data volume of DVS, we exploit a novel method for Unmanned Aerial Vehicle (UAV) vision-aid landing. In our method, discrete AEs within a varying period are accumulated to form an event map at first. Then the template matching technology is employed to locate the landing mark. Based on the recognized landing mark, local optical flow estimation and real speed projection are conducted to provide more aux information for accurate landing. In the experiments, a simulation platform is setup verify the effectiveness of our methods under various external environments. The experimental results show that the proposed algorithm can accurately measure the location and relative speed of landing marks."
9395836,Mobile Attendance based on Face Detection and Recognition using OpenVINO,"The OpenVINO toolkit enables versatile computer vision with an Intel® Movidius ™ Neural Compute Stick 2 connected to a Raspberry Pi. This small portable platform provides new opportunities for innovative solutions in computer vision applications and beyond. This paper investigates its feasibility for mobile attendance systems for settings such as classrooms or other scenarios that require headcount or roll call. Related studies of face-based systems are explored, while the advantages of the proposed system are highlighted. Although there are some positioning constraints, the proof-of-concept system processes an approximate average of five faces per second. That means it can take attendance in a lecture room of 90 students in about 18 seconds. A recognition accuracy of 98.1% with an f-score of 96.9% was yielded on a private classroom dataset captured with a modest RPi camera. These promising results were achieved using a tiny ResNet-18 architecture, producing significantly better results than MobileNet. Furthermore, it outperformed the recognition accuracy of other `lightweight' methods used in the literature that do not run off embedded devices on publicly available datasets."
9491913,"Visually Impaired Indoor Navigation using YOLO Based Object Recognition, Monocular Depth Estimation and Binaural Sounds","This paper presents the development of a real-time spatial audio generating software to assist visually impaired people in indoor navigation using computer vision techniques and binaural sound generations. Our computer vision techniques utilize YOLO (You Only Look Once) based algorithm to detect objects, monocular depth estimation techniques to derive the depth map from a single captured image, and linear interpolation to obtain the azimuth and elevation angles of the detected objects. Based on the obtained results, binaural sounds are generated by HRTF (Head Related Transfer Function), where the intensity of the generated spatial audio is varied according to the distance of the detected object. Our test results show the real-time generated binaural sounds were able to accurately specify the position of the object in 2D space to avoid collisions and to provide surrounding information for navigating visually impaired people."
8883232,Self-Supervised Domain Adaptation for Computer Vision Tasks,"Recent progress of self-supervised visual representation learning has achieved remarkable success on many challenging computer vision benchmarks. However, whether these techniques can be used for domain adaptation has not been explored. In this work, we propose a generic method for self-supervised domain adaptation, using object recognition and semantic segmentation of urban scenes as use cases. Focusing on simple pretext/auxiliary tasks (e.g. image rotation prediction), we assess different learning strategies to improve domain adaptation effectiveness by self-supervision. Additionally, we propose two complementary strategies to further boost the domain adaptation accuracy on semantic segmentation within our method, consisting of prediction layer alignment and batch normalization calibration. The experimental results show adaptation levels comparable to most studied domain adaptation methods, thus, bringing self-supervision as a new alternative for reaching domain adaptation. The code is available at this link. https://github.com/Jiaolong/self-supervised-da."
9298161,Multi-level Random Sample Consensus Method for Improving Structured Light Vision Systems,"The paper proposes a structured light vision system equipped with multi-cameras and multi-laser emitters for object height measurement or 3D reconstruction. The proposed method offers a better accuracy performance over a single camera system. To tackle the intersections produced by laser emitters in the projected image plane, we propose a multi-level random sample consensus (MLRANSAC) algorithm to separate the intersection points instead of using the traditional methods such as time division and color division techniques. Our experiments demonstrate that the MLRANSAC algorithm can perform effectively."
9490245,SPEED: Spiking Neural Network With Event-Driven Unsupervised Learning and Near-Real-Time Inference for Event-Based Vision,"A fully event-based image processing pipeline containing neuromorphic vision sensors and spiking neural network has the potential to achieve high throughput, low latency and high dynamic range vision processing. In this work, we present an end-to-end SNN unsupervised learning inference framework to achieve near-real-time processing performance. The design uses fully event-driven operations that significantly improve learning and inference speed: over 100× increase of inference throughput on CPU and near-real-time inference on GPU for neuromorphic vision sensors can be achieved. The event-driven processing method supports unsupervised spike-timing-dependent plasticity learning of convolutional SNN. When labels are limited, it achieves higher accuracy than supervised training approaches. In addition, the proposed method improves robustness for low-precision SNN as it reduces spiking activity distortion and achieves higher learning accuracy than regular discrete-time simulated low-precision networks."
8889765,Effects of Image Degradation and Degradation Removal to CNN-Based Image Classification,"Just like many other topics in computer vision, image classification has achieved significant progress recently by using deep learning neural networks, especially the Convolutional Neural Networks (CNNs). Most of the existing works focused on classifying very clear natural images, evidenced by the widely used image databases, such as Caltech-256, PASCAL VOCs, and ImageNet. However, in many real applications, the acquired images may contain certain degradations that lead to various kinds of blurring, noise, and distortions. One important and interesting problem is the effect of such degradations to the performance of CNN-based image classification and whether degradation removal helps CNN-based image classification. More specifically, we wonder whether image classification performance drops with each kind of degradation, whether this drop can be avoided by including degraded images into training, and whether existing computer vision algorithms that attempt to remove such degradations can help improve the image classification performance. In this article, we empirically study those problems for nine kinds of degraded images—hazy images, motion-blurred images, fish-eye images, underwater images, low resolution images, salt-and-peppered images, images with white Gaussian noise, Gaussian-blurred images, and out-of-focus images. We expect this article can draw more interests from the community to study the classification of degraded images."
8988193,Multi-Feature View-Based Shallow Convolutional Neural Network for Road Segmentation,"This study presents a shallow and robust road segmentation model. The computer-aided real-time applications, like driver assistance, require real-time and accurate processing. Current studies use Deep Convolutional Neural Networks (DCNN) for road segmentation. However, DCNN requires high computational power and lots of labeled data to learn abstract features for deeper layers. The deeper the layer is, the more abstract information it tends to learn. Moreover, the prediction time of the DCNN network is an important aspect of autonomous vehicles. To overcome these issues, a Multi-feature View-based Shallow Convolutional Neural Network (MVS-CNN) is proposed that utilizes the abstract features extracted from the explicitly derived representations of the input image. Gradient information of the input image is used as additional channels to enhance the learning process of the proposed deep learning architecture. The multi-feature views are fed to a fully-connected neural network to accurately segment the road regions. The testing accuracy demonstrates that the proposed MVS-CNN achieved an improvement of 2.7% as compared to baseline CNN consisting of only RGB inputs. Furthermore, the comparison of the proposed method with the popular semantic segmentation network (SegNet) has shown that the proposed scheme performs better while being more efficient during training and evaluation. Unlike traditional segmentation techniques, which are based on the encoder-decoder architecture, the proposed MVS-CNN consists of only the encoder network. The proposed MVS-CNN has been trained and validated with two well-known datasets: the KITTI Vision Benchmark and the Cityscapes dataset. The results have been compared with the state-of-the-art deep learning architectures. The proposed MVS-CNN outperforms and shows supremacy in terms of model accuracy, processing time, and segmentation accuracy. Based on the experimental results, the proposed architecture can be considered as an efficient road ...
(Show More)"
9094943,Fingertip Detection Based on the Chord-to-Point Distance Accumulation,"Fingertip detection is an important part of vision-based human-computer interaction. Due to the complexity of background and the real-time capability of system, there still exist many problems in the accuracy of fingertip positioning. In view of these problems, this paper proposed a simple and efficient fingertip detection method. First, the input video stream is preprocessed, and then the hand contour is extracted. The obtained hand contour is processed by the Chord-to-Point Distance Accumulation algorithm, so as to detect the fingertip in the image. The experimental results show that the method has high accuracy and stability in fingertip recognition."
9527014,Stereo Matching Based on Visual Sensitive Information,"The area of computer vision is one of the most discussed topics amongst many scholars, and stereo matching is its most important sub fields. After the parallax map is transformed into a depth map, it can be applied to many intelligent fields. In this paper, a stereo matching algorithm based on visual sensitive information is proposed by using standard images from Middlebury dataset. Aiming at the limitation of traditional stereo matching algorithms regarding the cost window, a cost aggregation algorithm based on the dynamic window is proposed, and the disparity image is optimized by using left and right consistency detection to further reduce the error matching rate. The experimental results show that the proposed algorithm can effectively enhance the stereo matching effect of the image providing significant improvement in accuracy as compared with the classical census algorithm. The proposed model code, dataset, and experimental results are available at https://github.com/WangHewei16/Stereo-Matching."
9362722,LED instrument screen character recognition detection based on machine vision,"At present, more and more kinds of instruments are used, and more and more LED screen instruments are used. The recognition and detection of led instrument screen information is very important for a series of problems, such as the current LED instrument screen character information recognition is difficult, the detection is difficult, the screen tells the splash screen and so on. A screen character recognition detection method of LED instrument based on machine vision is proposed. Firstly, the character area of the LED instrument screen is recognized and located, and the minimum region containing character information is segmented, then the feature convolution operation is carried out, and the character feature pixel distribution is determined according to the convolution operation results. Finally, the segmented character features are recognized and detected. The experimental results show that this method can effectively detect the screen character recognition of LED instrument, and has a certain practical application value."
9270451,The Application of Computer-supported Mind Map in College English Reading Teaching,"This paper starts with the analysis of the essence of computer-supported mind maps and traditional mind maps, and then makes an in-depth analysis of the application of computer-supported mind maps in English teaching, including detailed discussion and planning on the development of mind maps, scaffolding teaching mode and brainstorming teaching strategies. Finally, the essential differences between English reading teaching and reading are elaborated in detail. Starting from the reality of college English reading teaching, while improving the development of college English teaching, the rapid development of computer-supported mind map in college English reading teaching is promoted."
9384824,Initial Development of Image Processing Based Computer Vision Technology on Robotic Arm Manipulator for Tool Wear Monitoring on Micro-milling,"Tool wear monitoring needs high accuracy that can be done with electron microscope which needs long period of time. Instead, this research is to simplified the tool wear monitoring with image processing-based computer vision using Dino-Lite attached to robotic arm manipulator. The development uses OpenCV on Python with the following steps: (1) gathering the new and the broken tool images using Dino-Lite; (2) importing the image to Python and convert to HSV; (3) giving a noise reduction using Gaussian Blur; (4) giving a color detection to obtain masking of the HSV thresholding variable adjustment; (5) uses image Canny to detect contour area from the thresholding; (6) the new and the broken tool face area will be displayed; (7) these two values will be compared and generate the wear percentage. The image processing calculates the tool face area and the experiment uses the variation of Gaussian Blur for noise reduction, with the given values of 0, 1, 3, 5, 7, 9 11, 13, 15, 17. Few data cannot be obtained due to the unsupported image condition. The results show that the tool area on the images is more potential to be detected due to the increasing number of Gaussian Blur value."
9665913,Neural Disparity Refinement for Arbitrary Resolution Stereo,"We introduce a novel architecture for neural disparity refinement aimed at facilitating deployment of 3D computer vision on cheap and widespread consumer devices, such as mobile phones. Our approach relies on a continuous formulation that enables to estimate a refined disparity map at any arbitrary output resolution. Thereby, it can handle effectively the unbalanced camera setup typical of nowadays mobile phones, which feature both high and low resolution RGB sensors within the same device. Moreover, our neural network can process seamlessly the output of a variety of stereo methods and, by refining the disparity maps computed by a traditional matching algorithm like SGM, it can achieve unpaired zero-shot generalization performance compared to state-of-the-art end-to-end stereo models."
8526309,Neuromorphic LIF Row-by-Row Multiconvolution Processor for FPGA,"Deep Learning algorithms have become state-of-theart methods for multiple fields, including computer vision, speech recognition, natural language processing, and audio recognition, among others. In image vision, convolutional neural networks (CNN) stand out. This kind of network is expensive in terms of computational resources due to the large number of operations required to process a frame. In recent years, several frame-based chip solutions to deploy CNN for real time have been developed. Despite the good results in power and accuracy given by these solutions, the number of operations is still high, due the complexity of the current network models. However, it is possible to reduce the number of operations using different computer vision techniques other than frame-based, e.g., neuromorphic event-based techniques. There exist several neuromorphic vision sensors whose pixels detect changes in luminosity. Inspired in the leaky integrate-and-fire (LIF) neuron, we propose in this manuscript an event-based field-programmable gate array (FPGA) multiconvolution system. Its main novelty is the combination of a memory arbiter for efficient memory access to allow row-by-row kernel processing. This system is able to convolve 64 filters across multiple kernel sizes, from 1 × 1 to 7 × 7, with latencies of 1.3 μs and 9.01 μs, respectively, generating a continuous flow of output events. The proposed architecture will easily fit spike-based CNNs."
9456137,"Detection of Pedestrian, Lane and Traffic Signal for Vision Based Car Navigation","In the recent times, the self-driving cars or the autonomous cars are getting more attention, as they are proved to be more convenient and require less human effort for travelling. The main things to be considered when driving are the traffic signals, the lane in which it moves and the surrounding environment which includes the pedestrians. The object detection is one of the main components in the autonomous cars. With the combination of the computer vision and machine learning, a system is built for the vehicle detection, lane detection and pedestrian detection. With this system using an ACF algorithm, the vehicle detection can be done and we also estimate the distance between the detected vehicle. The detection of the lane, vehicles and the pedestrians is achieved. The results are compared with the existing methods and are seen to be better than the existing methods."
9549691,Vision Based Real-time High-accuracy Automatic Counting with Applications for Smart Pharmacy,"At present, many industrial applications urgently need a real-time high-accuracy automatic counting algorithm to realize the counting function of various objects. Some scholars are studying the use of deep learning algorithms to count objects in industrial applications. However, facing closely arranged and different objects, deep learning algorithms cannot achieve this function. Aiming at the application background of regular cuboids counting with a wide range of requirements, this paper provides a real-time high-accuracy automatic counting algorithm based on a deep understanding of the scene and a comprehensive use of multiple computer vision technologies. The algorithm can meet the stringent requirements of industrial applications and realize real-time high-accuracy automatic counting of closely arranged cuboids. And there is no requirement for whether the surfaces of the cuboids are the same, and there is no need to prepare the template pictures of the cuboids in advance. The algorithm mainly uses the characteristics of gaps between cuboids, and adopts improved Hough line detection to identify gaps, and finally realizes the counting function. The algorithm proposed in this paper has been verified on the counting of pillboxes in the industrial application of smart pharmacies. Compared with the deep learning algorithm, it has the characteristics of good interpretability, good generalization, fast speed and high accuracy."
9445205,Vision-Based Real-time Human Malicious Behavior Detection,"Human detection and behavior analysis from surveillance videos is an active area of research in computer vision. Authorities and security administrators need a system that can detect human malicious behavior to take immediate necessary actions. In this paper, we propose an approach to detect the anomalous/malicious behavior of humans in the surveillance videos. The proposed approach models the human behavior using human joint motion information from skeleton sequence. We have divided the proposed approach into four sub-modules i.e. human detection and skeleton estimation, human ID assignment, feature extraction and classification. The proposed approach is evaluated on publicly available CASIA dataset in offline mode and accuracy of 90.81% has been achieved. The experimental results indicates that it can be exploited in real-time applications with low computational cost of 18 frames per second."
8740862,Computer Vision System for Automatic Counting of Planting Microsites Using UAV Imagery,"Mechanical site preparation by mounding is often used by the forest industry to provide optimal growth conditions for tree seedlings. Prior to planting, an essential step consists in estimating the number of mounds at each planting block, which serves as planting microsites. This task often requires long and costly field surveys, implying several forestry workers to perform manual counting procedure. This paper addresses the problem of automating the counting process using computer vision and UAV imagery. We present a supervised detection-based counting framework for estimating the number of planting microsites on a mechanically prepared block. The system is trained offline to learn feature representations from semi-automatically annotated images. Mound detection and counting are then performed on multispectral UAV images captured at an altitude of 100 m. Our detection framework proceeds by generating region proposals based on local binary patterns (LBP) features extracted from near-infrared (NIR) patches. A convolutional neural network (CNN) is then used for classifying candidate regions by considering multispectral image data. To train and evaluate the proposed method, we constructed a new dataset by capturing aerial images from different planting blocks. The results demonstrate the efficiency and validity of the proposed method under challenging experimental conditions. The methods and results presented in this paper form a promising cornerstone to develop advanced decision support systems for planning planting operations."
9544703,Stereo Music System Control using Vision based Static Hand Gesture Recognition,Human gesture plays a very crucial role for communication with the person and even with machines. Now a day human gesture is very famous for assisting deaf and dumb person but it can also be used to control machines in an efficient manner. This paper is taking the only example of the machine as a stereo music system for controlling it with the help of seven different human static hand gestures. This approach uses vision-based static hand gesture recognition. The wavelet transform is used for feature extractor from the image and neural network is used for classification purpose. This system is trained and tested with three different databases of images and thus the result is derived that is 97.14% which is highly satisfactory.
8782477,Automated Inspection of Monopole Tower Using Drones and Computer Vision,"Drones are used in a wide range of applications such as manual inspection of mobile towers, transmission lines, and Search and Rescue operations. Traditional methods of using drones for manual inspections can be time and cost consuming. Skilled labor is also required for controlling the drone. Several 'crack detection algorithms' have been developed for detecting cracks but there are still problems with accuracy. In this paper, we propose a computer vision algorithm under the robot operating system (ROS) platform that can detect the Region of Interest (ROI) and analyze images in real time. Drone airtime is reduced as a result of this method. This newly developed system will inspect towers and detect cracks and rusts therein. This method also considers the challenges that occur in manual methods as well as drone capabilities. This system takes the measurement of the detected cracks, and classify the types of rusts found using Deep Learning Techniques."
9473651,Vision-Aided Dynamic Blockage Prediction for 6G Wireless Communication Networks,"Unlocking the full potential of millimeter-wave and sub-terahertz wireless communication networks hinges on realizing unprecedented low-latency and high-reliability requirements. The challenge in meeting those requirements lies partly in the sensitivity of signals in the millimeter-wave, and sub-terahertz frequency ranges to blockages. One promising way to tackle that challenge is to help a wireless network develop a sense of its surrounding using machine learning. This paper attempts to do that by utilizing deep learning and computer vision. It proposes a novel solution that proactively predicts dynamic link blockages. More specifically, it develops a deep neural network architecture that learns from observed sequences of RGB images and beam-forming vectors how to predict possible future link blockages. The proposed architecture is evaluated on a publicly available dataset that represents a synthetic dynamic communication scenario with multiple moving users and blockages. It scores a link-blockage prediction accuracy in the neighborhood of 86%, a performance that is unlikely to be matched without utilizing visual data."
9169651,Long-Term Monitoring for Track Slab in High-Speed Rail via Vision Sensing,"Track slab deformation has become a challenging issue in high-speed rail (HSR) operation in recent years. This article proposed a novel approach for track slab deformation monitoring based on computer vision techniques. The basic principle of visual measurement of track slab displacement is first introduced. Then the detailed process of slab displacement calculation from the on-site images is presented, including region of interest (ROI) extraction, determination of the target edge, and displacement calculation. In this process, considering the actual operation environment of in-service HSR lines, an improved Canny algorithm, which can adaptively extract the location information of the target is proposed and employed in the image processing. Based on the modular design method, an online monitoring system for the displacement of the track slab is established. The devised system is installed on an in-service HSR line for long-term slab deformation monitoring. The performance of the proposed system is verified by one-year monitoring data of slab displacement. The measurement results of the proposed system are compared with an existing linear variable differential transformer (LVDT) system, demonstrating that it can accurately report track displacement with lower cost and easier instrumentation. This research is expected to provide insights to the railway maintenance-of-way department for better management and maintenance of slab deformation, especially under high temperature."
9423040,CPM R-CNN: Calibrating Point-guided Misalignment in Object Detection,"In object detection, offset-guided and point-guided regression dominate anchor-based and anchor-free method separately. Recently, point-guided approach is introduced to anchor-based method. However, we observe points predicted by this way are misaligned with matched region of proposals and score of localization, causing a notable gap in performance. In this paper, we propose CPM R-CNN which contains three efficient modules to optimize anchor- based point-guided method. According to sufficient evaluations on the COCO dataset, CPM R-CNN is demonstrated efficient to improve the localization accuracy by calibrating mentioned misalignment. Compared with Faster R-CNN and Grid R-CNN based on ResNet-101 with FPN, our approach can substantially improve detection mAP by 3.3% and 1.5% respectively without whistles and bells. Moreover, our best model achieves improvement by a large margin to 49.9% on COCO test-dev. Code is available at https://github.com/zhubinQAQ/CPM-R-CNN."
8997540,Subway 3D Bound Limitation Visualization Method Based on Stereo Vision,"During subway driving, foreign objetcs such as rolling stones, pedestrians may invade subway driving safety bound, which brings safety hazards to subway normal running. One potential method to solve this problem is foreign body intrusion detection technology with 3D bound limitation visualization module. In the paper, we utilize deep learning and stereo vision technology to extract subway track from binocular camera videos, calculate 3D coordinates of subway track and the vertex coordinates of subway safty boundary section at each horizontal position. By connecting these vertices along with subway track's direction, the bounding section can be obtained. Finally, multiple sections are combined to form a complete safety bound limitation. The proposed method releases a more intuitive and accurate intrusion determination implementation and provides a potential safty solution for subway assisting driving."
9607674,VisDrone-DET2021: The Vision Meets Drone Object detection Challenge Results,"Object detection on the drone faces a great diversity of challenges such as small object inference, background clutter and wide viewpoint. In contrast to traditional detection problem in computer vision, object detection in bird-like angle can not be transplanted directly from common-in-use methods due to special object texture in sky‘s view. However, due to the lack of a comprehensive data set, the number of algorithms that focus on object detection using data captured by drones is limited. So the VisDrone team gathered a massive data set and organized Vision Meets Drones: A Challenge (VisDrone2021) in conjunction with the IEEE International Conference on Computer Vision (ICCV 2021) to advance the field. The collected dataset is the same as the previous dataset object detection challenge. Specifically, the team needed to predict the bounding boxes of the objects of ten predefined classes. We received results from a number of teams using different approaches, and this article describes the 8 team’s approach. We conducted a detailed analysis of the assessment results and summarized the challenges. More information can be found at: http://www.aiskyeye.com/."
9151037,Match or No Match: Keypoint Filtering based on Matching Probability,"Keypoints that do not meet the needs of a given application are a very common accuracy and efficiency bottleneck in many computer vision tasks, including keypoint matching and 3D reconstruction. Many computer vision and machine learning methods have dealt with this issue, trying to improve keypoint detection or the matching process. We introduce an algorithm that filters detected keypoints before the matching is even attempted, by predicting the probability of each point to be successfully matched. This is realised using a flexible and time efficient Random Forest classifier. Experiments on stereo and multi-view datasets of building facades show that the proposed method decreases the computational cost of a subsequent keypoint matching and 3D reconstruction, by correctly filtering 50% of the points that wouldn't be matched while preserving 73% of the matchable keypoints. This enables a subsequent processing with minimal mismatches, provides reliable matches, and point clouds. The presented filtering leads to an improved 3D reconstruction of the scene, even in the hard case of repetitive patterns and vegetation."
9432349,Ripe-Unripe: Machine Learning based Ripeness Classification,Agro and Food Processing industries have seen tremendous growth in a short period. These industries daily process the bulk of farm produce to make a variety of products. This paper aims to propose technology for automating the process of fruits based on the ripeness using Machine Learning and Computer Vision technology. The system will also be able to log the data of the fruits in processing using which the output of the product can be estimated. The system uses a CNN algorithm to classify the fruits and ripeness. This work could also be used to analyze large fields for cultivation using drones or robots.
9216922,Robust Localization of a Self-Driving Vehicle in a Lane,"Localization is still one of the most challenging tasks in autonomous driving on city roads. Further development and improvement of automatic functions of vehicles in urban conditions are not possible without overcoming the problem of significant degradation of GNSS signal quality. The proposed approach to localization can provide information about vehicle position on the road in different operational conditions. Desired stability and quality are achieved by using the combination of conventional computer vision, neural networks and Kalman filtering."
8943857,Kernel Density Estimation for Foreground Detection in Dynamic Video Processing for Unmanned Aerial Vehicle Application,The paper considers aspects of Unmanned Aerial Vehicle video stream foreground detection based on computer vision and artificial intelligence. Nonparametric approach for background subtraction based on kernel model was used as the most appropriate for usage in dynamic environment. One of the key elements of the kernel density estimation is bandwidth selection that significantly effects results of image segmentation and object detection. Several approaches of bandwidth selection and their impact on object detection performance is investigated in the paper. Otsu's method was used for threshold selection for image segmentation.
9288457,TGA: Two-level Group Attention for Assembly State Detection,"Assembly state detection, i.e., object state detection, has a critical meaning in computer vision tasks, especially in AR assisted assembly. Unlike other object detection problems, the visual difference between different object states can be subtle. For the better learning of such subtle appearance difference, we proposed a two-level group attention module (TGA), which consists of inter-group attention and intro-group attention. The relationship between feature groups as well as the representation within each feature group is simultaneously enhanced. We embedded the proposed TGA module in a popular object detector and evaluated it on two new datasets related to object state estimation. The result shows that our proposed attention module outperforms the baseline attention module."
8786410,Low-cost and Real-time Hardware Implementation of Stereo Vision System on FPGA,"In this work, one of local stereo vision algorithms named SAD approach, which is used in image depth estimation, has been surveyed, and an efficient and real-time new hardware implementation has been proposed. The proposed method has been verified and tested using C implementation. The acceptable simulation results along with the detailed explanation of numerous pre-processing steps are also presented. Our innovations could be divided into two sections: architecture and algorithm. In architecture section, by using a specific architecture, memory access has been lowered and therefore, speed has been increased. In algorithm section, a part of local algorithm, known as refinement, has been substituted with a simpler and more efficient algorithm. Cyclone IV has been utilized as our hardware platform. In this article, this point would demonstrate how to use an exact but less complicated controller, which results in less area, and how to use pipeline architecture and remove repetitive and redundant memory accesses, which conduct our stereo vision system to meet the real-time constraints. Suggested hardware implementation could reach to 53fps processing speed with 100MHz clock. No processing IP core has been used. In comparison with related work, our proposed method is more efficient in logic elements usage, and accordingly in power consumption."
9266138,Detecting road potholes using computer vision techniques,"The number of road potholes is growing day by day due to the increase in vehicles. This is also increasing the number of vehicle accidents caused by lack of road maintenance. In this study, a road pothole detection system using computer vision techniques was conducted. Object detection algorithms such as the YOLO, and SSD were selected. The YOLOv3-SPP model obtained the best mAP of 68.83%, while the YOLOv3-tiny inferred an image in just 0.01s. Testing was also done on an Android device and a Raspberry Pi in order to evaluate performance on embedded systems. The SSDLiteMobileNet v2 converted to a TensorFlow Lite model outperformed all the YOLOv3 models when time is compared. In this paper we show that the model with the best performance is YOLOv3 SPP, but if time is a priority, the SSD TFLite model is the right fit."
9010449,A Calibration Method for the Vision System of Go-Robot,"In order to solve the problem of high calibration failure rate of go-robot vision system caused by factors such as the poor background environment and poor shooting conditions, an advanced chessboard corner detection algorithm was proposed for calibration. Firstly, the outer border of chessboard is identified based on watershed segmentation algorithm. Then construct a chessboard reference model and use the projection transformation to extract the complete board from the complex background based on the four corners of the outer border. Secondly, detect the chessboard lines by Hough Transform and record the intersection points. Finally, all the intersection points are transformed into the chessboard reference model, keep the correct corner points that are matched, and all the correct corner points are projected into the real image. The experimental results show that the proposed method is superior to the popular FAST, improved SUSAN, and Harris chessboard corner detection methods in terms of detection accuracy and computational efficiency. And it can be used for the calibration of the go-robot vision system."
9580011,An Efficient Computer Vision Approach for Rapid Recognition of Poisonous Plants by Classifying Leaf Images using Transfer Learning,"Livestock poisoning by several kinds of poisonous plants causes grievous economic losses to the livestock industry. Poisonous plants are also a fatal threat to humans, ingesting these plants can cause several side effects in the body because of their toxicity. Hence, it is essential to develop a rapid approach to recognize poisonous plants efficiently. This paper addresses a recognition approach for eighteen poisonous plants using poisonous plants leaf (PPL) dataset which has been generated using image augmentation techniques that contains 54000 training, 27000 validation, and 9000 testing images. Six different state-of-the-art deep learning models have been used in this study such as Xception, ResNet152V2, InceptionResNetV2, MobileNetV2, DenseNet201, and NASNetLarge for classifying leaf images of poisonous plants. Xception has shown more significant performance than other models, achieved 99.71% training and 99.37% testing accuracy. NASNetLarge and InceptionResNetV2 have achieved 96.89% and 95.18% test accuracy, respectively, and MobileNetV2 achieved the lowest test accuracy."
9395961,Distinguishing Pneumonia and COVID-19: Utilizing Computer Vision to Mimic Clinician Efficacy,"Corona Virus has caused a disruption to the normalcy in the world and require thorough patient management. Looking at the present scenario and the kind of pandemic that it has turned out to be, this paper aims at providing an enhancement to the RT-PCR way of testing and uses the chest X-Rays to detect the presence and severity of the corona virus in a body to successfully differentiate between pneumonia and the Covid-19. The main motive is to help doctors and medical experts with an advance aid to the nursing of critically affected patients. This is feasible because the X-Ray machines are widely available throughout the country and they can assist in the advanced detection of the disease. Around 6000 images of the three kinds of chest X-Rays of patients with pneumonia, Covid, as well as completely normal patients, were used in the process. The paper concludes with the explicit comparison of all the models and their results. Primarily, a simple CNN model was opted for the scrutiny and then later on VGG-16, VGG-19, ResNet50, MobileNet and MobileNetV2 pre-trained models were utilised for anatomizing Covid-19 with respect to pneumonia and normal cases. In case of CNN, the maximum accuracy that was attained was 95.30% whereas, for the VGG-16, VGG-19, ResNet50, MobileNet and MobileNetV2, the maximum accuracies in correctly predicting the diseases were 95.63%, 96.02%, 94.82%, 95.23% and 93.39% respectively."
9266249,Automatic Vision Inspection Solution for the Manufacturing Process of Automotive Components Through Plastic Injection Molding,"In the automotive industry, vehicle components can be obtained through the process of plastic injection molding. The components can be fixed in the vehicle by using metal bushings, which are placed on the injection mold before the beginning of the plastic injection process. The incorrect placement or absence of the bushings leads to a defective product. Object classification has been a long-tackled problem in Computer Science, and the breakthroughs in Artificial Intelligence allows us to solve this problem with a high degree of accuracy and precision. In the context of industrial inspection, ensuring high-quality products is a matter of utmost importance. The automated vision inspection process facilitates the creation of a high volume of products, which are in conformity with the quality standards, in a short amount of time and can save the manufacturing company money. In this paper we propose a solution that automatically detects the injection mold and classifies the positioning of the bushings, warning the operator in case the current positioning can lead to a defective product. Furthermore, the bushing detection is optimized in such a way that, it is mandatory for the mold to be visible only in the first frame, thus reducing the running time of the whole pipeline. The results are validated by using a data set covering multiple possible working scenarios. Moreover, we compare the implemented classifier with other classifiers, highlighting the performance of the proposed solution with respect to running time and classification accuracy."
9112002,Approach to Implementation of Transport and Physical Levels of Distributed Computer Vision System,"The option of construction of physical and transport levels of the system of distributed computer vision system with application of intermediate PoE-switch is considered. The possibility of achievement of the branching ratio equal to 3 in case of using the Class I terminal technique is shown. It is proved that it is possible to increase the length of the cable channel up to 350 m using the PoE technique and using 600 mPoE+ technique with the use of class 1 TV cameras with the power, not exceeding 3 W. It is established that effective means of increasing the length of the channel are cables with an increased diameter of the twisted pair insulation electric copper conductor."
9501446,Hierarchical Attention Approach in Multimodal Emotion Recognition for Human Robot Interaction,"The ability to perceive human emotions is one of the key elements that may promise a natural, genuine and more reliable human robot interaction. Though emotional perception in human robot interaction has been challenged by many difficulties, the lack of contextual understanding is can be attributed as the biggest hurdle in this regard. Most of the literature refers to the datasets developed in controlled environment to validate the performance of their systems which happens to be really good. Still those systems are very far from achieving that kind of performance in real-life scenarios. In this paper a multimodal emotion recognition strategy is presented, that uses voice features in addition to the facial expressions to determine the emotional state of the user. A hierarchical attention layer is used to for feature fusion purpose. The final system is end-to-end trainable, multimodal approach makes it more resilient to the changes in environment. The achieved 76.3% accuracy for eNterface05 video dataset, which is higher than the any single modality approach in the comparison."
8892275,"Identification of Wear Products in the Automotive Tribotechnical System Using Computer Vision Methods, Artificial Intelligence and Big Data","It has been emphasizing the importance of wear products identification during the automotive tribotechnical system operation. Some examples and concepts of combining and applying of computer vision, artificial intelligence, and big data techniques to identify the wear products has presented. Several examples of proposed technology for recognizing wear products are presented. The following algorithm was used to obtain the surface shape of the wear particles. On the original image, an area corresponding to the studied object is allocated. In this case, the high reflectivity of wear particles was used compared with the background and the methods of threshold separation. Then a sharp object diagram (on the basis of which a distance map was calculated) was developed. The distance map has constructed in such way that the brighter the pixel of the image, the closer in this place the surface of the object to the observer. For example, the range map contains information on the shape of the surface of the wear particle. The software of the tribotechnical system was written in the Object Pascal programming language in the Delphi visual programming environment. It has been presented some examples and concepts for combining and applying computer vision, artificial intelligence, and big data techniques to identify wearing products. Further development of the industry and modern technologies will promote the introduction and improvement of such expert technology."
9050914,Generalized and Mechanistic PV Module Performance Prediction From Computer Vision and Machine Learning on Electroluminescence Images,"Electroluminescence (EL) imaging of photovoltiac (PV) modules offers high-speed, high-resolution information about device performance, affording opportunities for greater insight and efficiency in module characterization across manufacturing, research and development, and power plant operations and management. Predicting module electrical properties from EL image features is a critical step toward these applications. In this article, we demonstrate quantification of both generalized and performance mechanism-specific EL image features, using pixel intensity-based and machine learning classification algorithms. From EL image features, we build predictive models for PV module power and series resistance, using time-series current-voltage (I-V) and EL data obtained stepwise on five brands of modules spanning three Si cell types through two accelerated exposures: damp heat (DH) (85°C/85% RH) and thermal cycling (TC) (IEC 61215). In total, 195 pairs of EL images and I-V characteristics were analyzed, yielding 11700 individual PV cell images. A convolutional neural network was built to classify cells by the severity of busbar corrosion with high accuracy (95%). Generalized power predictive models estimated the maximum power of PV modules from EL images with high confidence and an adjusted-R 2 of 0.88, across all module brands and cell types in extended DH and TC exposures. Mechanistic degradation prediction was demonstrated by quantification of busbar corrosion in EL images of three module brands in DH, and subsequent modeling of series resistance using these mechanism-specific EL image features. For modules exhibiting busbar corrosion, we demonstrated series resistance predictive models with adjusted-R 2 of up to 0.73."
9314379,DCGAN-Based Data Augmentation for Document Classification,"Document classification is a relevant task within every intelligent document processing system. With the advances in deep learning and computer vision techniques, this task has become a painless and straightforward process. However, the need for labelled data is always a hurdle to tackle before constructing, training, and validating classification models. The use of Generative modelling implies using a model to generate new samples that are similar to the training set but individually different from existing records. In this paper, we investigate using deep convolutional adversarial networks (DCGAN) to generate fake document images using existing scanned documents dataset. Moreover, we used a dataset created of generated images alongside original images to train an image classifier using a convolutional neural network (CNN) to classify documents, and we compared the overall accuracy with a model constructed on an original dataset of the same size. The constructed model performed as well as the model trained with authentic data (with an accuracy score of 90% and 91% respectively). This excellent performance can permit using DCGANs to augment existing datasets in case of a lack of labelled data while maintaining similar performance levels."
9489017,Realization of Intelligent Auxiliary Guiding System of Ball Sports with Animation Simulation and Computer Graphics,"Recently, the integration of the vision and guiding system is a emerging research hotspot. This paper studies the realization of intelligent auxiliary guiding system of ball sports with animation simulation and computer graphics. As the core factor of the proposed model, the motion analysis framework is essential. In this paper, the time dimension is sampled at exactly the same time interval, so the joint connection of the time dimension is regular and can be calculated according to the traditional convolution method, the FFT action is applied here to calculate the overall performance of the model. The simulation system is applied to show the overall performance. The intelligent graphic model is considered for the study of the system. The experiment is conducted which verifies the designed system can enhance the traditional guiding model."
9025374,Real-Time Physics-Based Removal of Shadows and Shading From Road Surfaces,"We present a real-time physics-based system for generating an illumination free representation of road surfaces that maintains the distinction between asphalt and painted road markings. Cast shadows on road surfaces can create false features and modify the color of road markings, potentially masking important information for vehicle vision systems. We demonstrate a novel method for identifying the relative spectral properties of the direct and ambient illumination conditions and for using that to create an illumination-free 2D chromaticity space in log RGB. We then show how that representation can be used to generate an illumination-free greyscale representation that distinguishes road, white paint, and yellow paint, making it suitable for further analysis and classification. The entire process runs faster than 30Hz on current automotive-grade embedded processing systems. We evaluate the system on a paint detection task, comparing two types of learned classifiers, random forests and convolutional neural networks. For each type, one classifier is trained on the original images, and the other is trained on the illumination-free greyscale output. The classifiers are of identical complexity and trained on the same size data set. For both types, the classifier trained on the illumination-free outputs performs better, even on images with no cast shadows. The gap in performance is indicative of the cost of forcing a classifier to learn a task in the presence of the confounding illumination signal."
9548561,Image Channel as an Input Method for Deep Learning Ensemble,"The application of computer vision (CV) in many fields is becoming an important factor in development, and the demand for high accuracy CV products is increasing. Therefore, a new method that ensembles three trained models with the same architecture but different inputs is proposed. Each of the different channels of the images were used as input. The first channel of the image was used in the dataset for the first model, the second channel was used for the second model, and the third channel was used for the third model. The classification probabilities were achieved after training three different models with three channels. The model with the highest accuracy was selected as the main model, and it was added to the classification probabilities of certain classes of the main model. Applying this method to the classification task resulted in 75.45% accuracy, while using the entire image for the model resulted in 70.9% accuracy. The advantage of using this ensemble method is that it can be used simultaneously with other ensemble methods and achieves better results. Dividing an image into three different channels than simply using the entire image helps the model learn the image better."
8822033,Human computer interaction based eye controlled mouse,"With advanced technologies in this digital era, there is always scope for development in the field of computing. Hands free computing is in demand as of today it addresses the needs of quadriplegics. This paper presents a Human computer interaction (HCI) system that is of great importance to amputees and those who have issues with using their hands. The system built is an eye based interface that acts as a computer mouse to translate eye movements such as blinking, gazing and squinting towards the mouse cursor actions. The system in discussion makes use of a simple webcam and its software requirements are Python(3.6), OpenCv, numpy and a few other packages which are necessary for face recognition. The face detector can be built using the HOG (Histogram of oriented Gradients) feature along with a linear classifier, and the sliding window technique. It is hands free and no external hardware or sensors are required."
9182668,Recognition of circular workpiece in complex environment,"With the development of science and technology, Using computer vision technology to detect and identify products has become a common application. As a common geometric figure in industrial production, circle often needs to be detected and recognized. In this paper, in order to solve the problem of the identification of the round workpiece in the complex environment, it is studied. Traditional Hough algorithm transforms image from original image space to parameter space, In the parameter space, we use some parameter form which most boundary points satisfy as the description of the curve in the image. industrial production environment is complex, Even if we try to eliminate noise or improve Hough algorithm, Still can't improve its recognition rate. For this reason, this paper proposes a method combining YOLOv3 deep learning system with Hough algorithm, which greatly improves the recognition rate of circular workpiece in complex industrial environment."
9707066,MobileStereoNet: Towards Lightweight Deep Networks for Stereo Matching,"Recent methods in stereo matching have continuously improved the accuracy using deep models. This gain, however, is attained with a high increase in computation cost, such that the network may not fit even on a moderate GPU. This issue raises problems when the model needs to be deployed on resource-limited devices. For this, we propose two light models for stereo vision with reduced complexity and without sacrificing accuracy. Depending on the dimension of cost volume, we design a 2D and a 3D model with encoder-decoders built from 2D and 3D convolutions, respectively. To this end, we leverage 2D MobileNet blocks and extend them to 3D for stereo vision application. Besides, a new cost volume is proposed to boost the accuracy of the 2D model, making it performing close to 3D networks. Experiments show that the proposed 2D/3D networks effectively reduce the computational expense (27%/95% and 72%/38% fewer parameters/operations in 2D and 3D models, respectively) while upholding the accuracy. Code: https://github.com/cogsys-tuebingen/mobilestereonet."
9667626,The Need and Status of Sea Turtle Conservation and Survey of Associated Computer Vision Advances,"For over hundreds of millions of years, sea turtles and their ancestors have swum in the vast expanses of the ocean. They have undergone a number of evolutionary changes, leading to speciation and sub-speciation. However, in the past few decades, some of the most notable forces driving the genetic variance and population decline have been global warming and anthropogenic impact ranging from large-scale poaching, collecting turtle eggs for food, besides dumping trash including plastic waste into the ocean. This leads to severe detrimental effects in the sea turtle population, driving them to extinction. This research focusses on the forces causing the decline in sea turtle population, the necessity for the global conservation efforts along with its successes and failures, followed by an in-depth analysis of the modern advances in detection and recognition of sea turtles, involving Machine Learning and Computer Vision systems, aiding the conservation efforts."
9578687,Bayesian Nested Neural Networks for Uncertainty Calibration and Adaptive Compression,"Nested networks or slimmable networks are neural networks whose architectures can be adjusted instantly during testing time, e.g., based on computational constraints. Recent studies have focused on a ""nested dropout"" layer, which is able to order the nodes of a layer by importance during training, thus generating a nested set of subnetworks that are optimal for different configurations of resources. However, the dropout rate is fixed as a hyperparameter over different layers during the whole training process. Therefore, when nodes are removed, the performance decays in a human-specified trajectory rather than in a trajectory learned from data. Another drawback is the generated sub-networks are deterministic networks without well-calibrated uncertainty. To address these two problems, we develop a Bayesian approach to nested neural networks. We propose a variational ordering unit that draws samples for nested dropout at a low cost, from a proposed Downhill distribution, which provides useful gradients to the parameters of nested dropout. Based on this approach, we design a Bayesian nested neural network that learns the order knowledge of the node distributions. In experiments, we show that the proposed approach outperforms the nested network in terms of accuracy, calibration, and out-of-domain detection in classification tasks. It also outperforms the related approach on uncertainty-critical tasks in computer vision."
9607742,Visual interpretability analysis of Deep CNNs using an Adaptive Threshold method on Diabetic Retinopathy images,"Deep neural networks have been dominating the field of computer vision, achieving exceptional performance on object detection and pattern recognition. However, despite the highly accurate predictions of these models, the continuous increase in depth and complexity comes at the cost of interpretability, making the task of explaining the reasoning behind these predictions very challenging. In this paper, an analysis of state-of-the-art approaches towards the direction of interpreting the networks’ representations, is carried out over two Diabetic Retinopathy image datasets, IDRiD and DDR. Furthermore, these techniques are compared in the task of image segmentation of the same datasets. This is to discover which method can produce the better attention maps that can solve the problem of segmentation without actually training the network for the specific task. To accomplish that we propose an adaptive threshold method that transforms the attention masks in a more suitable representation for segmentation. Experiments over multiple architectures were conducted to ensure the robustness of the results."
9577857,Domain-robust VQA with diverse datasets and methods but no target labels,"The observation that computer vision methods overfit to dataset specifics has inspired diverse attempts to make object recognition models robust to domain shifts. However, similar work on domain-robust visual question answering methods is very limited. Domain adaptation for VQA differs from adaptation for object recognition due to additional complexity: VQA models handle multimodal inputs, methods contain multiple steps with diverse modules resulting in complex optimization, and answer spaces in different datasets are vastly different. To tackle these challenges, we first quantify domain shifts between popular VQA datasets, in both visual and textual space. To disentangle shifts between datasets arising from different modalities, we also construct synthetic shifts in the image and question domains separately. Second, we test the robustness of different families of VQA methods (classic two-stream, transformer, and neuro-symbolic methods) to these shifts. Third, we test the applicability of existing domain adaptation methods and devise a new one to bridge VQA domain gaps, adjusted to specific VQA models. To emulate the setting of real-world generalization, we focus on unsupervised domain adaptation and the open-ended classification task formulation."
8826856,A Derivation of 3-D Error Propagation in Stereo Vision Tracking of Air Traffic used for the FAA Collision Risk Model,"This paper presents a novel study of deterministic error propagation in a stereo vision tracking system implemented by the Federal Aviation Administration (FAA) to produce accurate three-dimensional (3-D) flight tracks of aircraft during the visual approach segment of an instrument landing approach. The FAA is in the process of modernizing the National Airspace System (NAS) in the United States with the Next Generation Air Transport System (NextGen) project. This involves updating the means of navigation, detection of air traffic and airway assignment in all phases of flight. The Collision Risk Model (CRM) is the fundamental constituent that tracks Time and Space Position Information (TSPI) data to assess the probability of collision within a given airspace. The CRM is used to determine the separation standards of airborne aircrafts in order to minimize the risk probabilities. This paper presents the derivation of a risk model used by the FAA while it mathematically formulates and analyzes the gaussian error propagation of the underlying stereo vision tracking based on the placement coordinates of the optical sensors at airport terminals."
9377101,Automated Vehicle Numberplate Detection and Recognition,"With the emergence of information technology in the last decade, and the ever growing population, several approaches have been implemented and thought of regarding building an information system about Vehicle Number plate Detection. Among the 7.6 billion people living on this planet, approximately 1.4 billion own a vehicle that accounts for 18% of the world's total population, which calls for an intelligent information system that provides meaning out of the collected data and has useful applications in the domain of quotidian problems. The concept of Vehicle Number plate Detection and Recognition (VNDR) is an application of the computer vision domain entailing phases like detection, image processing and character recognition. This paper reviews the numerous methodologies which have been employed for better accuracy in recognition of vehicle number plates."
8697980,Hand Gesture Recognition and Implementation for Disables using CNN’S,"Hand Gestures are playing the major role in today's industry. The development of the gesture recognition or detection will help people in different ways. Development of many automated technologies like machine learning, deep learning, neural networks and computer vision will help in using futuristic methods like gesture controlling and recognition. These gestures can be used in such a way it can help with people who have difficulty in controlling or operation systems or devices. Here the development of the model is in such a way it can help us to recognition and implementation. For such developments these models will be working with Convolution Neural Networks and Back Propagation methodologies which can help for the easy working of these models."
9488174,A Survey on Quaternion Algebra and Geometric Algebra Applications in Engineering and Computer Science 1995–2020,"Geometric Algebra (GA) has proven to be an advanced language for mathematics, physics, computer science, and engineering. This review presents a comprehensive study of works on Quaternion Algebra and GA applications in computer science and engineering from 1995 to 2020. After a brief introduction of GA, the applications of GA are reviewed across many fields. We discuss the characteristics of the applications of GA to various problems of computer science and engineering. In addition, the challenges and prospects of various applications proposed by many researchers are analyzed. We analyze the developments using GA in image processing, computer vision, neurocomputing, quantum computing, robot modeling, control, and tracking, as well as improvement of computer hardware performance. We believe that up to now GA has proven to be a powerful geometric language for a variety of applications. Furthermore, there is evidence that this is the appropriate geometric language to tackle a variety of existing problems and that consequently, step-by-step GA-based algorithms should continue to be further developed. We also believe that this extensive review will guide and encourage researchers to continue the advancement of geometric computing for intelligent machines."
9577656,Back to Event Basics: Self-Supervised Learning of Image Reconstruction for Event Cameras via Photometric Constancy,"Event cameras are novel vision sensors that sample, in an asynchronous fashion, brightness increments with low latency and high temporal resolution. The resulting streams of events are of high value by themselves, especially for high speed motion estimation. However, a growing body of work has also focused on the reconstruction of intensity frames from the events, as this allows bridging the gap with the existing literature on appearance- and frame-based computer vision. Recent work has mostly approached this problem using neural networks trained with synthetic, ground-truth data. In this work we approach, for the first time, the intensity reconstruction problem from a self-supervised learning perspective. Our method, which leverages the knowledge of the inner workings of event cameras, combines estimated optical flow and the event-based photometric constancy to train neural networks without the need for any ground-truth or synthetic data. Results across multiple datasets show that the performance of the proposed self-supervised approach is in line with the state-of-the-art. Additionally, we propose a novel, lightweight neural network for optical flow estimation that achieves high speed inference with only a minor drop in performance."
8598852,A Scalable Off-the-Shelf Framework for Measuring Patterns of Attention in Young Children and Its Application in Autism Spectrum Disorder,"Autism spectrum disorder (ASD) is associated with deficits in the processing of social information and difficulties in social interaction, and individuals with ASD exhibit atypical attention and gaze. Traditionally, gaze studies have relied upon precise and constrained means of monitoring attention using expensive equipment in laboratories. In this work we develop a low-cost off-the-shelf alternative for measuring attention that can be used in natural settings. The head and iris positions of 104 16-31 months children, an age range appropriate for ASD screening and diagnosis, 22 of them diagnosed with ASD, were recorded using the front facing camera in an iPad while they watched on the device screen a movie displaying dynamic stimuli, social stimuli on the left and non-social stimuli on the right. The head and iris position were then automatically analyzed via computer vision algorithms to detect the direction of attention. We validate the proposed framework and computational tool showing that children in the ASD group paid less attention to the movie, showed less attention to the social as compared to the non-social stimuli, and often fixated their attention to one side of the screen. These results are expected from the ASD literature, here obtained with significantly simpler and less expensive attention tracking methods. The proposed method provides a low-cost means of monitoring attention to properly designed stimuli, demonstrating that the integration of stimuli design and automatic response analysis results in the opportunity to use off-the-shelf cameras to assess behavioral biomarkers."
9545919,Face Recognition Construction Site Access Control System Based on Open Source Computer Vision,"Face recognition is a kind of biometric identification technology based on the facial feature information of people. This article uses C51 single-chip microcomputer, OpenCV, etc. to complete the site access control system. By entering the face information, the system will open the access control if the system is recognized successfully, and will alarm if the recognition fails. This system completes the design requirements of the access control system and realizes the intelligent access control of the construction site."
9039044,Computer Vision Data Processing Algorithm in 3D Image Reconstruction System,"This article will discuss existing algorithms used in the process of 3D reconstruction using a stereo pair. The steps of constructing depth maps, algorithms for its construction and processing, as well as methods for finding points of interest of various images using detectors and descriptors for comparing images or searching for specific objects are considered."
8819804,A Novel Strategy for Gender Identification from Hand Dorsal Images using Computer Vision,"Criminal Investigations and Archaeological Studies often involve Gender Determination from different body parts when dismembered human remains are encountered. The body parts may include hair or hand in criminal investigations and hand bone length or hand to index finger length ratio in archaeological research. Often in crimes or even in accidents, Forensic Science comes into play. Forensic Science, by definition, is the scientific application of criminal and civil laws for collection of scientific evidences for prosecution or defense or both. So, in Forensic Research, Gender Determination is a very integral part especially from hair or hand, which becomes difficult for anyone other than Forensic Scientists, to conclude. This study aims at classification of the gender of an individual into male and female from the image of his or her dorsal side of hand. A Deep Learning-Machine Learning Hybrid Model is developed, yielding a Validation Accuracy of 100% and Recall of 1.0. Such an Automated System is highly essential in making Forensic Research, easy and hence swift analysis of scientific evidences."
9469700,A Piezoelectric Artificial Throat and Computer Vision Based on Smart Assistive System,"Amyotrophic lateral sclerosis (ALS) patients will gradually lost their motion abilities and have huge inconvenience in their daily life. In this article, an artificial throat and eye tracking based smart assistive system is presented. Through detecting eye motions and throat vibrations, the established neural network can interpret the patient's intention, and perform simple operations by a robotic arm (e.g. grasp an object). The experimental results demonstrate that the developed system can help patients obtain basic self-care ability."
8784858,Automatic Counting of Chili Ripeness on Computer Vision for Industri 4.0,"This study aims to determine and counting the number of ripe chili on a tree. The proposed system uses images taken in gardens with a distance of 40 cm and an image resolution of 576×864. This work implements RGB2HSV color transformation, image segmentation, threshold, and image morphology, and then implements a Blob Analysis method for detecting and counting the ripe chilies on the trees. The trial results of this study provide a boundary for the blob area that will be used. Blob area less than 400 will not be labeled for the bounding box of the object detected. The accuracy of the proposed method to detect and calculate the ripe chilies on tress is 89.7%."
9070912,Isolation of Prostate Gland in T1-Weighted Magnetic Resonance Images using Computer Vision,"In this work, segmentation of the region of prostate gland from T1-weighted magnetic resonant (MR) images is considered. There are many practical challenged that are encountered while segmentation such as the regions surrounding the prostate gland may be inconsistently defined, which can pose a problem for the algorithm to differentiate the pixels of the gland from the neighborhood pixels. The other problem may be the region around the gland may not be homogenous. Therefore, in this study, we have proposed a semi-automatic approach that can efficiently segment out the prostate gland in a computationally effective and robust manner as compared to the approaches available in the literature. Here, we have used image processing algorithms like anisotropic diffusion, Gaussian filtering, k-means clustering, and region-growing approach. The proposed method has been evaluated on the T1-weighted MR images of 30 subjects. The proposed method has given a Dice coefficient of 86.95% and a Jaccard index of nearly 80.56%."
8856547,Realtime Indoor Workout Analysis Using Machine Learning & Computer Vision,"These days there are thousands of workout videos available on the internet. Samsung Health 2 [1] provides a dedicated section called programs containing short workout videos for various exercises. The goal is to assist people perform these workouts independently on their own. A common observation is that even people who visit gym regularly find it difficult to perform all steps (body pose alignments) in a workout accurately. Continuously doing an exercise incorrectly may eventually cause severe long term injuries. To help solve this problem and provide assistance in form of a visual feedback while performing a workout, we propose a system to analyze user's body posture during a workout and compare it to a professional's reference workout. We represent human body as a collection of limbs and analyze angle between limb pairs to detect errors and provide corrective action to the user. Our system builds on the latest advancements using deep learning for human body pose estimation. We use techniques for time series data alignment like DTW [2] (Dynamic Time Warping) along with optical flow tracking to synchronize user/reference videos. We are able to detect and locate errors in user's activity (pose) very effectively based on some threshold deviation between the limb angles. The system in future can be extended to be used by physicians to monitor patient's recovery following an injury."
9303184,Using Computer Vision and Deep Learning for Nanoparticle Recognition on Scanning Probe Microscopy Images: Modified U-net Approach,"Particles characterization is a significant part of numerous studies in material sciences and engineering technologies. Microscopy images of materials containing particles are usually analyzed by operator with manual counting and measuring of particle sizing by a software ruler. Traditional automated image analyzing methods such as edge detection, segmentation, etc. are not universal, giving poor results on noisy pictures and need empirical fitted parameters. To realize automatic method of particles recognition on scanning tunneling microscopy (STM) data we used U-net and modified U-net neural networks, which was trained on ten STM images contained 1918 particles. Verification on 3 pictures with 695 particles showed mAP=0.12 for modified U-net neural network."
9080021,"Deep Learning and Computer Vision-based a Novel Framework for Himalayan Bear, Marco Polo Sheep and Snow Leopard Detection","Wildlife plays a vital role in balancing the environment. It also provides stability to different natural processes of nature. In recent year, there are many animals which are facing the danger of extinction. The reason for animal extinction is natural occurrences such as climatic heating, cooling, or changes in sea levels. In literate, many techniques are proposed to detect and classify animals, but each technique has a limitation. In this paper, we propose a novel framework using deep convolutional neural networks (D-CNN) and k Nearest Neighbors (kNN) to detect animals. The dataset contains four class snow leopard, Marco polo sheep, Himalayan bear, and other animals. Many D-CNN like AlexNet, ResNet-50, VGG-19, and inception v3 are used to extract features. The experimental results verify that inception v3 integrated with kNN outperforms other D-CNNs. It also has more accuracy of 98.3% with a classification error of 2%, which is quite negligible."
9073082,LPDDR4 SIPI Co-Simulation and Measurement Correlation for IOT Computer Vision Application,Simulation-measurement correlation presents unique challenges that require co-simulation between SI and PI to capture all sources of noise. A new hybrid methodology using peak distortion analysis and realistic worst case SSN was devised to enable full channel simulation including both SI and PI effects. The method was verified against lab measurements on two different designs.
8787723,An auxiliary container loading location algorithm based on computer vision,"Port container handling is an important part of terminal operations. When loading multiple containers, operators can only rely on human eyes and PLC information to obtain the location of the container. To avoid the serious consequences of miscalculation, a combination algorithm is proposed in this paper. We choose coastline, container ridgeline and container number as features to analyses and locate. coastline location algorithm based on color space and improved OTSU segmentation. Container ridgeline location based on Sobel and clustering algorithm. Container number location based on Maximally Stable Extremal Regions (MSER) algorithm. After testing 71 group operation videos of 4 bridge cranes in Ningbo port, the combination algorithm achieves better positioning accuracy. The average processing time of a single frame is less than 0.6s. The algorithm proposed in this paper provides an effective reference and solution for improving the efficiency of terminal operations."
8977698,Computer vision based method for identification of freshness in mushrooms,"Scientific testing of mushroom (Agaricus Bisporus) samples, and especially non-destructive can prove to be very beneficial in different fields of research including agriculture, food processing, health care, etc. This is mainly because, when these mushrooms are left exposed in the open atmosphere, these tend to react with surrounding and develop a brown coloured pigment. This phenomenon is commonly referred to as the ""enzymatic browning"". Here, this reaction is utilized to observe certain chemical changes over a course of time. This has led to a result which can be used for classification of mushroom samples. Hence, this classification can be used to prove the gradual change in pattern behaviour of mushrooms during different time intervals. SVM classifier is seen to give the result with an accuracy of 80%. Thus, this classification can also give an overlook of whether these mushroom samples are fresh for consumption or not."
9578311,ZeroScatter: Domain Transfer for Long Distance Imaging and Vision through Scattering Media,"Adverse weather conditions, including snow, rain, and fog, pose a major challenge for both human and computer vision. Handling these environmental conditions is essential for safe decision making, especially in autonomous vehicles, robotics, and drones. Most of today’s supervised imaging and vision approaches, however, rely on training data collected in the real world that is biased towards good weather conditions, with dense fog, snow, and heavy rain as outliers in these datasets. Without training data, let alone paired data, existing autonomous vehicles often limit themselves to good conditions and stop when dense fog or snow is detected. In this work, we tackle the lack of supervised training data by combining synthetic and indirect supervision. We present ZeroScatter, a domain transfer method for converting RGB-only captures taken in adverse weather into clear daytime scenes. ZeroScatter exploits model-based, temporal, multi-view, multi-modal, and adversarial cues in a joint fashion, allowing us to train on unpaired, biased data. We assess the proposed method on in-the-wild captures, and the proposed method outperforms existing monocular descattering approaches by 2.8 dB PSNR on controlled fog chamber measurements."
9118413,Packaging Defect Detection System Based on Machine Vision and Deep Learning,"Detecting packaging defection with high accuracy and efficiency is of great significance in product quality. We use OpenCV to preprocess images which come from damaged package according to characteristics of the image. The processed data is combined with deep learning and based on neural network model ResNet. Meanwhile the processed image data is sent to a convolutional neural network (CNN) for model training. We establish a detection system for product packaging. The detection system provides a solution for automatic detection of package defection, which realizes rapid and accurate detection of product packaging."
9660177,Bias and Fairness in Computer Vision Applications of the Criminal Justice System,"Discriminatory practices involving AI -driven police work have been the subject of much controversies in the past few years, with algorithms such as COMPAS, PredPol and ShotSpotter being accused of unfairly impacting minority groups. At the same time, the issues of fairness in machine learning, and in particular in computer vision, have been the subject of a growing number of academic works. In this paper, we examine how these area intersect. We provide information on how these practices have come to exist and the difficulties in alleviating them. We then examine three applications currently in development to understand what risks they pose to fairness and how those risks can be mitigated."
9176025,Real-time and High Quality Ultrasound Elastography Using Convolutional Neural Network by Incorporating Analytic Signal,"Convolutional Neural Networks (CNN) have been extensively used for many computer vision applications including optical flow estimation. Although CNNs have been very successful in optical flow problem, they have been rarely used for displacement estimation in Ultrasound Elastography (USE) due to vast differences between ultrasound data and computer vision images. In USE, a main goal is to obtain the strain image which is the derivative of the axial displacement in axial direction; therefore, a very accurate displacement estimation is required. Radio Frequency (RF) data is needed to obtain accurate displacement estimation. RF data contains high frequency contents which cannot be downsampled without significant loss of information, in contrast to computer vision images. We propose a novel technique to utilize LiteFlowNet for USE. For the first time, we incorporate analytic signal to improve the quality of the displacement estimation. We show that this network with the designed inputs is more suitable for USE compared to more complex networks such as FlowNet2. The network is adopted to our application and it is compared with FlowNet2 and a state-of-the-art elastography method (GLUE). The results show that this network performs well and comparable to GLUE. Furthermore, not only this network is faster and has lower memory footprint compared to FlowNet2, but also it obtains higher quality strain images which makes it suitable for portable and real-time elastography devices."
8697997,Smart Home Automation using Computer Vision and Segmented Image Processing,"The objective of this paper is to develop a smart IoT based light control system. Due to negligence and forgetfulness, many instances occur in establishments where the electrical appliances are left turned on even if there no human presence in a room. This is one of the most prominent cases of electricity wastage prevalent in society. Hence there is a need for an intelligent system that can ensure both efficiency and effectiveness. This project combines IoT, Artificial Intelligence and Image Processing which are powerful modern technologies. In this system, object detection methods are used which enable us to control appliances in a specific spatial region. It also uses image processing methods which are more efficient than conventional IR blaster-based home automation systems. conventional IR blasters inevitably come with a fallacy where any and all objects that obstruct the infrared ray trigger whatever response the system was programmed to achieve. These actions that are only meant for actual human beings can now be activated by any object. This produces an undesired result. This paper proposes a system which can efficiently utilize the lighting output and minimize the wastage of electricity by controlling the electrical appliances by detecting changes in the position of the humans in the room."
9155164,SterileAR: Exploration of Augmented Reality and Computer Vision Approaches for Real-Time Feedback in Sterile Compounding Training,"This paper presents preliminary work on the development of SterileAR, a software platform for providing real-time feedback to pharmacy students during training in sterile pharmaceutical compounding procedures. Simulation pedagogy requires that the learner receive immediate feedback to recognize procedural errors and internalize learning objectives. Instructors and task trainers are often utilized in tandem to allow deliberate practice for novices in specialized clinical procedures. In this work, we explore the possibility of augmenting sterile compounding procedure training with real-time feedback using augmented reality, machine learning, and computer vision technologies. We present our approaches to developing SterileAR in four iterations, including specific descriptions of successes and failures, and methodologies for capturing object spatial position, overlapping, and occlusion."
9457961,A Review of Convolutional Neural Networks and Gabor Filters in Object Recognition,"Convolutional neural networks (CNNs) have become a classic approach to solving challenging computer vision problems. Much of its success is due to its ability to discover optimal filters that capture non-trivial spatial relationships in data. Other vital components include advances in optimization, regularization, and overfitting prevention strategies. However, recently, researchers have observed closely the connections between what CNNs learn in the layers that capture low-level features and filter-banks such as Gabor filters. Gabor filters have been used in computer vision tasks long before CNNs were popularized with good performance. This paper presents a review of the literature concerning approaches that involve both Gabor filters and CNNs. We pay close attention to successes and opportunities for future research in the intersection of these two computer vision tools."
9663608,Object Recognition System for Visually Impaired People,"One of the biggest problems that visually Impaired (VI) individuals face in their daily lives is object detection and recognition. A model is created for an object detector that can detect items for VI persons and other important uses by recognizing them at a specific distance. Existing object detection algorithms necessitate a huge amount of training data, which takes longer time, more complicated, and it is a difficult process. As a result, a computer vision notion for converting an object to text was developed using the Caffemodel framework by importing a pretrained dataset model. The Mobilenet SSD method is then used to translate the texts into speech. On a single screen, this system can detect many objects. It aids visually challenged people in detecting objects in real time. This technology can also be put into any portable gadget to assist visually impaired people to recognize items at a certain distance."
8728438,Solar Powered Fire Extirpation Robot with Night Vision Camera,In this paper we have constructed a fire extirpation robot with night vision camera which is solar power driven. The agent is mounted with 4 sensors and 2 motors and has a capability of detecting the fire prone areas with the help of the smoke and temperature sensors. It surpasses any obstacles using ultrasonic sensor and also identifies any live body in the affected area using PIR sensor. It creates a buzzer alert and sprays water or co2 by pumping it from the tank or cylinder mounted on the chassis of the robot. Night vision camera can monitor all the activities of the robot even in dark
9638272,On How To Combine Image Segmentation Algorithms Using Entropy,"Image segmentation is one of the most frequently used computer vision techniques. Whether we talk about medical imaging or autonomous driving, image segmentation algorithms are required to obtain the desired result. Therefore, a variety of algorithms have been implemented so far. Being based on different approaches, each of these algorithms has its own advantages and disadvantages. No algorithm can perform the same regardless of input data, with algorithms yielding better or worse results depending on the characteristics of the image. Some may accurately preserve the borders between large regions while clustering small details together (under-segmentation), while others can correctly delimit details while at the same time splitting large regions in multiple clusters (over-segmentation). Moreover, some algorithms might have a natural tendency in over-segmentation or under-segmentation, independent of input. This paper proposes a voting method which combines the results of some notable segmentation algorithms. The aim of this method is to limit the downsides of the used algorithms and to obtain, if possible, more accurate results. The results show that, in most cases, the proposed method offers both improvements in the quality of the provided output and more overall confidence in its usage."
9170659,Comparison of Autonomy and Study of Deep Learning Tools for Object Detection in Autonomous Self Driving Vehicles,"Computer vision is a field of computer systems that can comprehend images and scenes and also identify them. Computer vision technology is very popular in recent times and it has numerous applications namely smart monitoring and surveillance, drones, medicine, sports, entertainment, industrial robotics and self driving cars. Image localization, classification and detection are the basis of the above technology applications. Current improvements in Convolutional Neural Networks i.e. CNNs also lead to exceptional feat in tasks related to visual identification. CNNs are largely applicable for image classification tasks, particularly, looking at an image and classifying various features in it. In this paper study is carried out on various computer vision algorithms and deep learning techniques and corresponding tools for object detection in autonomous self driving cars. Deep learning has revolutionized computer vision techniques namely for object detection. CNN for object detection include three main deep learning techniques; SSD (Single shot multibox detector), R-CNN i.e. Region based CNN, R-FCN i.e. Region based fully convolutional networks. In this paper, an effort is made to show how above deep learning techniques are effective in ameliorating pre-trained networks' performance using existing datasets under different weather and terrestrial conditions. A brief comparison is also done for human centered autonomy and full autonomy for self driving cars."
9156598,Private-kNN: Practical Differential Privacy for Computer Vision,"With increasing ethical and legal concerns on privacy for deep models in visual recognition, differential privacy has emerged as a mechanism to disguise membership of sensitive data in training datasets. Recent methods like Private Aggregation of Teacher Ensembles (PATE) leverage a large ensemble of teacher models trained on disjoint subsets of private data, to transfer knowledge to a student model with privacy guarantees. However, labeled vision data is often expensive and datasets, when split into many disjoint training sets, lead to significantly sub-optimal accuracy and thus hardly sustain good privacy bounds. We propose a practically data-efficient scheme based on private release of k-nearest neighbor (kNN) queries, which altogether avoids splitting the training dataset. Our approach allows the use of privacy-amplification by subsampling and iterative refinement of the kNN feature embedding. We rigorously analyze the theoretical properties of our method and demonstrate strong experimental performance on practical computer vision datasets for face attribute recognition and person reidentification. In particular, we achieve comparable or better accuracy than PATE while reducing more than 90% of the privacy loss, thereby providing the “most practical method to-date” for private deep learning in computer vision."
9390987,Research on cylindrical labels unfolding algorithm based on machine vision,"It is a difficult problem for imaging the cylindrical label, and this paper presents an algorithm for unfolding the cylindrical label. First, the cylindrical label is positioned by the monocular camera, and the 3D point sets of the cylindrical label are established. Second, the observation range of each camera is determined based on the poses of the cylinders. Then, the cylindrical distortion is corrected in each camera. Finally, two adjacent images are stitched together to obtain a continuous and normalized unfolding image of the cylindrical label. Real-world and synthetic data experiments results show that the algorithm can correct the cylindrical distortion, and it takes only about 160 ms to obtain the cylindrical label unfolding image. The algorithm execution process does not require complicated steps and additional auxiliary, and the algorithm execution speed meets the industrial requirements."
